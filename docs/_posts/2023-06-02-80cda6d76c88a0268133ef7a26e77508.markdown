--- 
layout: post 
title: "The Impact of Positional Encoding on Length Generalization in Transformers" 
date: 2023-06-02 15:36:55 -0400 
categories: jekyll update 
author: "A Kazemnejad, I Padhi, KN Ramamurthy, P Das - arXiv preprint arXiv , 2023" 
--- 
Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of  Cites: Super-naturalinstructions: Generalization via declarative