---
layout: post
title:  "Training Vision-Language Transformers from Captions Alone"
date:   2022-05-24 00:00:36 -0400
categories: jekyll update
author: "L Gui, Q Huang, A Hauptmann, Y Bisk, J Gao - arXiv preprint arXiv:2205.09256, 2022"
---
We show that Vision-Language Transformers can be learned without human labels (eg class labels, bounding boxes, etc). Existing work, whether explicitly utilizing bounding boxes or patches, assumes that the visual backbone must first be trained on ImageNet class prediction before being integrated into a multimodal linguistic pipeline. We show that this is not necessary and introduce a new model Vision-Language from Captions (VLC) built on top of Masked Auto-Encoders that does not  Cites: LXMERT: Learning Cross-Modality Encoder Representations from 