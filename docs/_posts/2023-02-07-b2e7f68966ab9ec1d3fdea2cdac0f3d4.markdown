--- 
layout: post 
title: "BBTv2: Towards a Gradient-Free Future with Large Language Models" 
date: 2023-02-07 01:43:12 -0400 
categories: jekyll update 
author: "T Sun, Z He, H Qian, Y Zhou, XJ Huang, X Qiu - of the 2022 Conference on Empirical , 2022" 
--- 
Most downstream adaptation methods tune all or part of the parameters of pre-trained models (PTMs) through gradient descent, where the tuning cost increases linearly with the growth of the model size. By contrast, gradient-free methods only require the forward computation of the PTM to tune the prompt, retaining the benefits of efficient tuning and deployment. Though, past work on gradient-free tuning often introduces gradient descent to seek a good initialization of prompt and lacks  Cites: Making Pre-trained Language Models Better Few-shot Learners