---
layout: post
title:  "Faith and Fate: Limits of Transformers on Compositionality"
date:   2023-06-02 15:36:55 -0400
categories: jekyll update
author: "N Dziri, X Lu, M Sclar, XL Li, L Jian, BY Lin, P West… - arXiv preprint arXiv …, 2023"
---
Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks--multi-digit multiplication …
Cites: ‪Scaling Laws for Generative Mixed-Modal Language Models‬