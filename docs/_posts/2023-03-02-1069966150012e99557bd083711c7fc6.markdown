--- 
layout: post 
title: "Fast Attention Requires Bounded Entries" 
date: 2023-03-02 06:18:50 -0400 
categories: jekyll update 
author: "J Alman, Z Song - arXiv preprint arXiv:2302.13214, 2023" 
--- 
In modern machine learning, inner product attention computation is a fundamental task for training large language models such as Transformer, GPT-1, BERT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as input three matrices $ Q, K, V\in [-B, B]^{n\times d} $, and the goal is to construct the matrix $\mathrm {Att}(Q, K, V):=\mathrm {diag}(A {\bf 1} _n)^{-1} AV\in\mathbb {R}^{n\times d} $, where $ A=\exp (QK^\top/d) $ is theattention matrix , and $\exp $ is applied entry-wise  Cites: Palm: Scaling language modeling with pathways