---
layout: post
title:  "Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction"
date:   2023-05-16 05:31:31 -0400
categories: jekyll update
author: "X Wang, Z Wang, W Hu - arXiv preprint arXiv:2305.06616, 2023"
---
Continual few-shot relation extraction (RE) aims to continuously train a model for new relations with few labeled training data, of which the major challenges are the catastrophic forgetting of old relations and the overfitting caused by data sparsity. In this paper, we propose a new model, namely SCKD, to accomplish the continual few-shot RE task. Specifically, we design serial knowledge distillation to preserve the prior knowledge from previous models and conduct contrastive learning with pseudo …
Cites: ‪Continual Relation Learning via Episodic Memory Activation and …‬