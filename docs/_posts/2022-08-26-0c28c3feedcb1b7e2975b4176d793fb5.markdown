--- 
layout: post 
title: "Treeformer: Dense Gradient Trees for Efficient Attention Computation" 
date: 2022-08-26 23:24:20 -0400 
categories: jekyll update 
author: "L Madaan, S Bhojanapalli, H Jain, P Jain - arXiv preprint arXiv:2208.09015, 2022" 
--- 
Standard inference and training with transformer based architectures scale quadratically with input sequence length. This is prohibitively large for a variety of applications especially in web-page translation, query-answering etc. Consequently, several approaches have been developed recently to speedup attention computation by enforcing different attention structures such as sparsity, low-rank, approximating attention using kernels. In this work, we view attention computation as that of nearest Cites: REALM: Retrieval-Augmented Language Model Pre-Training