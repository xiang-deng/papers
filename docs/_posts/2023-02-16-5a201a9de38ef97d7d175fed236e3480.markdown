--- 
layout: post 
title: "Stabilized In-Context Learning with Pre-trained Language Models for Few Shot Dialogue State Tracking" 
date: 2023-02-16 06:16:46 -0400 
categories: jekyll update 
author: "D Chen, K Qian, Z Yu - arXiv preprint arXiv:2302.05932, 2023" 
--- 
Prompt-based methods with large pre-trained language models (PLMs) have shown impressive unaided performance across many NLP tasks. These models improve even further with the addition of a few labeled in-context exemplars to guide output generation. However, for more complex tasks such as dialogue state tracking (DST), designing prompts that reliably convey the desired intent is nontrivial, leading to unstable results. Furthermore, building in-context exemplars for dialogue tasks is  Cites: A simple language model for task-oriented dialogue