--- 
layout: post 
title: "Unlimiformer: Long-Range Transformers with Unlimited Length Input" 
date: 2023-05-06 06:19:24 -0400 
categories: jekyll update 
author: "A Bertsch, U Alon, G Neubig, MR Gormley - arXiv preprint arXiv:2305.01625, 2023" 
--- 
Transformer-based models typically have a predefined bound to their input length, because of their need to potentially attend to every token in the input. In this work, we propose Unlimiformer: a general approach that can wrap any existing pretrained