--- 
layout: post 
title: "PILE: Pairwise Iterative Logits Ensemble for Multi-Teacher Labeled Distillation" 
date: 2022-11-17 00:57:01 -0400 
categories: jekyll update 
author: "L Cai, L Zhang, D Ma, J Fan, D Shi, Y Wu, Z Cheng - arXiv preprint arXiv , 2022" 
--- 
Pre-trained language models have become a crucial part of ranking systems and achieved very impressive effects recently. To maintain high performance while keeping efficient computations, knowledge distillation is widely used. In this paper, we focus on two key questions in knowledge distillation for ranking models: 1) how to ensemble knowledge from multi-teacher; 2) how to utilize the label information of data in the distillation process. We propose a unified algorithm called Pairwise Cites: Multi-stage document ranking with BERT