--- 
layout: post 
title: "One-Teacher and Multiple-Student Knowledge Distillation on Sentiment Classification" 
date: 2022-10-15 02:59:22 -0400 
categories: jekyll update 
author: "X Chang, SYM Lee, S Zhu, S Li, G Zhou - of the 29th International Conference on , 2022" 
--- 
Abstract Knowledge distillation is an effective method to transfer knowledge from a large pre-trained teacher model to a compacted student model. However, in previous studies, the distilled student models are still large and remain impractical in highly speed-sensitive systems (eg, an IR system). In this study, we aim to distill a deep pre-trained model into an extremely compacted shallow model like CNN. Specifically, we propose a novel one-teacher and multiple-student knowledge distillation approach to  Cites: Mobilebert: a compact task-agnostic bert for resource-limited devices