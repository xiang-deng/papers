--- 
layout: post 
title: "Parameter-Efficient Tuning Makes a Good Classification Head" 
date: 2022-11-03 01:42:13 -0400 
categories: jekyll update 
author: "Z Yang, M Ding, Y Guo, Q Lv, J Tang - arXiv preprint arXiv:2210.16771, 2022" 
--- 
In recent years, pretrained models revolutionized the paradigm of natural language understanding (NLU), where we append a randomly initialized classification head after the pretrained backbone, eg BERT, and finetune the whole model. As the pretrained backbone makes a major contribution to the improvement, we naturally expect a good pretrained classification head can also benefit the training. However, the final-layer output of the backbone, ie the input of the classification head, will Cites: Fine-tuning pretrained language models: Weight initializations