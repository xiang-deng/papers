--- 
layout: post 
title: "Parameter-efficient Modularised Bias Mitigation via AdapterFusion" 
date: 2023-02-16 06:16:46 -0400 
categories: jekyll update 
author: "D Kumar, O Lesota, G Zerveas, D Cohen, C Eickhoff - arXiv preprint arXiv , 2023" 
--- 
Large pre-trained language models contain societal biases and carry along these biases to downstream tasks. Current in-processing bias mitigation approaches (like adversarial training) impose debiasing by updating a model s parameters, effectively transferring the model to a new, irreversible debiased state. In this work, we propose a novel approach to develop stand-alone debiasing functionalities separate from the model, which can be integrated into the model on-demand, while keeping the core  Cites: AdapterFusion: Non-destructive task composition for transfer learning