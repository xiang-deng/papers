--- 
layout: post 
title: "Selective Annotation Makes Language Models Better Few-Shot Learners" 
date: 2022-09-10 00:05:49 -0400 
categories: jekyll update 
author: "H Su, J Kasai, CH Wu, W Shi, T Wang, J Xin, R Zhang - arXiv preprint arXiv , 2022" 
--- 
Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework Cites: Rethinking the Role of Demonstrations: What Makes In-Context