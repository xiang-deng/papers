--- 
layout: post 
title: "Faithfulness Tests for Natural Language Explanations" 
date: 2023-06-01 02:05:49 -0400 
categories: jekyll update 
author: "P Atanasova, OM Camburu, C Lioma, T Lukasiewicz - arXiv preprint arXiv , 2023" 
--- 
Explanations of neural models aim to reveal a model s decision-making process for its predictions. However, recent work shows that current methods giving explanations such as saliency maps or counterfactuals can be misleading, as they are prone to present reasons that are unfaithful to the model s inner workings. This work explores the challenging question of evaluating the faithfulness of natural language explanations (NLEs). To this end, we present two tests. First, we propose a Cites: Polyjuice: Generating Counterfactuals for Explaining, Evaluating