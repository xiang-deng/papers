--- 
layout: post 
title: "Training Scale-Invariant Neural Networks on the Sphere Can Happen in Three Regimes" 
date: 2022-09-12 23:50:28 -0400 
categories: jekyll update 
author: "M Kodryan, E Lobacheva, M Nakhodnov, D Vetrov - arXiv preprint arXiv:2209.03695, 2022" 
--- 
A fundamental property of deep learning normalization techniques, such as batch normalization, is making the pre-normalization parameters scale invariant. The intrinsic domain of such parameters is the unit sphere, and therefore their gradient optimization dynamics can be represented via spherical optimization with varying effective learning rate (ELR), which was studied previously. In this work, we investigate the properties of training scale-invariant neural networks directly on the  Cites: Catastrophic fisher explosion: Early phase fisher matrix impacts