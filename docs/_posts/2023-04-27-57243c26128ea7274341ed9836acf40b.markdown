--- 
layout: post 
title: "Exploring Challenges of Deploying BERT-based NLP Models in Resource-Constrained Embedded Devices" 
date: 2023-04-27 01:18:20 -0400 
categories: jekyll update 
author: "S Sarkar, MF Babar, MM Hassan, M Hasan, SKK Santu - arXiv preprint arXiv , 2023" 
--- 
BERT-based neural architectures have established themselves as popular state-of-the-art baselines for many downstream NLP tasks. However, these architectures are data-hungry and consume a lot of memory and energy, often hindering their deployment in many real-time, resource-constrained applications. Existing lighter versions of BERT (eg. DistilBERT and TinyBERT) often cannot perform well on complex NLP tasks. More importantly, from a designer s perspective, it is unclear  Cites: Are Sixteen Heads Really Better than One?