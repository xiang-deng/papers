--- 
layout: post 
title: "Modular Transformers: Compressing Transformers into Modularized Layers for Flexible Efficient Inference" 
date: 2023-06-08 03:52:18 -0400 
categories: jekyll update 
author: "W Zhou, RL Bras, Y Choi - arXiv preprint arXiv:2306.02379, 2023" 
--- 
Pre-trained Transformer models like T5 and BART have advanced the state of the art on a wide range of text generation tasks. Compressing these models into smaller ones has become critically important for practical use. Common neural network compression techniques such as knowledge distillation or quantization are limited to static compression where the compression ratio is fixed. In this paper, we introduce Modular Transformers, a modularized encoder-decoder framework for flexible Cites: Deep encoder, shallow decoder: Reevaluating non-autoregressive