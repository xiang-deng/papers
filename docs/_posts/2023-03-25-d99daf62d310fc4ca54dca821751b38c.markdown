---
layout: post
title:  "Heterogeneous-Branch Collaborative Learning for Dialogue Generation"
date:   2023-03-25 03:24:49 -0400
categories: jekyll update
author: "Y Li, S Feng, B Sun, K Li - arXiv preprint arXiv:2303.11621, 2023"
---
With the development of deep learning, advanced dialogue generation methods usually require a greater amount of computational resources. One promising approach to obtaining a high-performance and lightweight model is knowledge distillation, which relies heavily on the pre-trained powerful teacher. Collaborative learning, also known as online knowledge distillation, is an effective way to conduct one-stage group distillation in the absence of a well-trained large teacher model …
Cites: ‪DialogWAE: Multimodal response generation with conditional …‬