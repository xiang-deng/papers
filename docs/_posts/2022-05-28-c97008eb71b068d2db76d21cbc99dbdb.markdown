---
layout: post
title:  "GraB: Finding Provably Better Data Permutations than Random Reshuffling"
date:   2022-05-28 02:05:27 -0400
categories: jekyll update
author: "Y Lu, W Guo, C De Sa - arXiv preprint arXiv:2205.10733, 2022"
---
Random reshuffling, which randomly permutes the dataset each epoch, is widely adopted in model training because it yields faster convergence than with-replacement sampling. Recent studies indicate greedily chosen data orderings can further speed up convergence empirically, at the cost of using more computation and memory. However, greedy ordering lacks theoretical justification and has limited utility due to its non-trivial memory and computation overhead. In this paper, we first  Cites: Well-read students learn better: On the importance of pre-training 