--- 
layout: post 
title: "Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning" 
date: 2023-02-01 14:37:22 -0400 
categories: jekyll update 
author: "X Wang, W Zhu, WY Wang - arXiv preprint arXiv:2301.11916, 2023" 
--- 
In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning  Cites: Learning To Retrieve Prompts for In-Context Learning