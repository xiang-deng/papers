---
layout: post
title:  "In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models"
date:   2023-05-11 03:26:59 -0400
categories: jekyll update
author: "S Sia, K Duh - arXiv preprint arXiv:2305.03573, 2023"
---
The phenomena of in-context learning has typically been thought of as  learning from examples . In this work which focuses on Machine Translation, we present a perspective of in-context learning as the desired generation task maintaining coherency with its context, ie, the prompt examples. We first investigate randomly sampled prompts across 4 domains, and find that translation performance improves when shown in-domain prompts. Next, we investigate coherency for the in-domain …
Cites: ‪Demystifying prompts in language models via perplexity estimation‬