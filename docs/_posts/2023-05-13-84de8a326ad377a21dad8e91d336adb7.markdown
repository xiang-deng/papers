--- 
layout: post 
title: "Towards Building the Federated GPT: Federated Instruction Tuning" 
date: 2023-05-13 06:32:20 -0400 
categories: jekyll update 
author: "J Zhang, S Vahidian, M Kuo, C Li, R Zhang, G Wang - arXiv preprint arXiv , 2023" 
--- 
While``instruction-tuned generative large language models (LLMs) have demonstrated an impressive ability to generalize to new tasks, the training phases heavily rely on large amounts of diverse and high-quality instruction data (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data, especially when it comes to human-written data, can pose significant challenges both in terms of cost and accessibility. Moreover, concerns related to privacy can further limit access to Cites: Delta Tuning: A Comprehensive Study of Parameter Efficient