--- 
layout: post 
title: "Arbitrary Few Parameters are Good Enough for Adapting Large-scale Pre-trained Language Models" 
date: 2023-06-08 03:52:18 -0400 
categories: jekyll update 
author: "Y Su, CM Chan, J Cheng, Y Qin, Y Lin, S Hu, Z Yang - arXiv preprint arXiv , 2023" 
--- 
Parameter-efficient tuning (PET) methods can effectively drive extremely large pre-trained language models (PLMs) by only training minimal parameters. Different PET methods utilize different manually designed modules. In a small PLM, there are usually noticeable performance differences among PET methods. Nevertheless, when a PLM s scale grows up to tens of billions of parameters, all PET methods achieve almost the same performance and even perform on par with the full Cites: Sparse Structure Search for Delta Tuning