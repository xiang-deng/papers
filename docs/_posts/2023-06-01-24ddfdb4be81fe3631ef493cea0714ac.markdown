---
layout: post
title:  "Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models"
date:   2023-06-01 02:05:49 -0400
categories: jekyll update
author: "N Lawton, A Kumar, G Thattai, A Galstyan, GV Steeg - arXiv preprint arXiv …, 2023"
---
Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method …
Cites: ‪Unipelt: A unified framework for parameter-efficient language …‬