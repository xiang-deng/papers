---
layout: post
title:  "When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories"
date:   2022-12-23 23:45:02 -0400
categories: jekyll update
author: "A Mallen, A Asai, V Zhong, R Das, H Hajishirzi… - arXiv preprint arXiv …, 2022"
---
Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs  strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k …
Cites: ‪Unsupervised dense information retrieval with contrastive learning‬