--- 
layout: post 
title: "PVP: Pre-trained Visual Parameter-Efficient Tuning" 
date: 2023-04-29 05:44:29 -0400 
categories: jekyll update 
author: "Z Song, K Yang, N Guan, J Zhu, P Qiao, Q Hu - arXiv preprint arXiv:2304.13639, 2023" 
--- 
Large-scale pre-trained transformers have demonstrated remarkable success in various computer vision tasks. However, it is still highly challenging to fully fine-tune these models for downstream tasks due to their high computational and storage costs. Recently, Parameter-Efficient Tuning (PETuning) techniques, eg, Visual Prompt Tuning (VPT) and Low-Rank Adaptation (LoRA), have significantly reduced the computation and storage cost by inserting lightweight prompt modules into the Cites: Unipelt: A unified framework for parameter-efficient language