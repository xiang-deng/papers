---
layout: post
title:  "Language Models with Rationality"
date:   2023-05-27 10:00:59 -0400
categories: jekyll update
author: "N Kassner, O Tafjord, A Sabharwal, K Richardson… - arXiv preprint arXiv …, 2023"
---
While large language models (LLMs) are proficient at question-answering (QA), the dependencies between their answers and other  beliefs  they may have about the world are typically unstated, and may even be in conflict. Our goal is to uncover such dependencies and reduce inconsistencies among them, so that answers are supported by faithful, system-believed chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a  rational , self …
Cites: ‪Enhancing Self-Consistency and Performance of Pre-Trained …‬