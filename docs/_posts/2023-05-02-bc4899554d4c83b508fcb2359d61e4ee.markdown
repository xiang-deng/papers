---
layout: post
title:  "Do we need Label Regularization to Fine-tune Pre-trained Language Models?"
date:   2023-05-02 02:27:35 -0400
categories: jekyll update
author: "I Kobyzev, A Jafari, M Rezagholizadeh, T Li, A Do-Omri… - Proceedings of the 17th …, 2023"
---
Abstract Knowledge Distillation (KD) is a prominent neural model compression technique that heavily relies on teacher network predictions to guide the training of a student model. Considering the ever-growing size of pre-trained language models (PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is evident that in KD, deploying the teacher network during training adds to the memory and computational requirements of training. In the computer vision literature, the …
Cites: ‪Well-read students learn better: On the importance of pre-training …‬