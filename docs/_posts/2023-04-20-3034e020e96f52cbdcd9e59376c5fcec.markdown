--- 
layout: post 
title: "Constructing Effective In-Context Demonstration for Code Intelligence Tasks: An Empirical Study" 
date: 2023-04-20 07:45:04 -0400 
categories: jekyll update 
author: "S Gao, XC Wen, C Gao, W Wang, MR Lyu - arXiv preprint arXiv:2304.07575, 2023" 
--- 
Pre-trained models of code have gained widespread popularity in many code intelligence tasks. Recently, with the scaling of the model and corpus size, large language models have shown the ability of in-context learning. These models employ task instructions and a few demonstration examples as prompts to learn the semantics of the task and make predictions for test samples. This new learning paradigm is training-free and has shown impressive performance in various natural Cites: Binding language models in symbolic languages