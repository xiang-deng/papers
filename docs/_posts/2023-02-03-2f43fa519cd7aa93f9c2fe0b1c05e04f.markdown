--- 
layout: post 
title: "Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection" 
date: 2023-02-03 14:16:33 -0400 
categories: jekyll update 
author: "C Wang, Y Lu, Y Mu, Y Hu, T Xiao, J Zhu - arXiv preprint arXiv:2302.00444, 2023" 
--- 
Knowledge distillation addresses the problem of transferring knowledge from a teacher model to a student model. In this process, we typically have multiple types of knowledge extracted from the teacher model. The problem is to make full use of them to train the student model. Our preliminary study shows that:(1) not all of the knowledge is necessary for learning a good student model, and (2) knowledge distillation can benefit from certain knowledge at different training steps. In response  Cites: Well-Read Students Learn Better: On the Importance of Pre