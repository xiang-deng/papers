---
layout: post
title:  "Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models"
date:   2023-05-30 03:09:06 -0400
categories: jekyll update
author: "P Jiang, S Agarwal, B Jin, X Wang, J Sun, J Han - arXiv preprint arXiv:2305.15597, 2023"
---
The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM), exhibiting limited performance and requiring expensive efforts from experts. To this end, we propose TAGREAL that automatically generates quality query prompts and retrieves support …
Cites: ‪Do Pre-trained Models Benefit Knowledge Graph Completion? A …‬