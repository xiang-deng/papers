---
layout: post
title:  "ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval"
date:   2022-04-04 16:51:29 -0400
categories: jekyll update
author: "M Cheng, Y Sun, L Wang, X Zhu, K Yao, J Chen - arXiv preprint arXiv , 2022"
---
Visual appearance is considered to be the most important cue to understand images for cross-modal retrieval, while sometimes the scene text appearing in images can provide valuable information to understand the visual semantics. Most of existing cross-modal retrieval approaches ignore the usage of scene text information and directly adding this information may lead to performance degradation in scene text free scenarios. To address this issue, we propose a full transformer architecture to Cites: Vinvl: Revisiting visual representations in vision-language models