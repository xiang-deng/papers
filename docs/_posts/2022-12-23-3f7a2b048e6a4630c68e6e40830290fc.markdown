--- 
layout: post 
title: "Is Self-Attention Powerful to Learn Code Syntax and Semantics?" 
date: 2022-12-23 23:45:02 -0400 
categories: jekyll update 
author: "W Ma, M Zhao, X Xie, Q Hu, S Liu, J Zhang, W Wang - arXiv preprint arXiv , 2022" 
--- 
Pre-trained language models for programming languages have shown a powerful ability on processing many Software Engineering (SE) tasks, eg, program synthesis, code completion, and code search. However, it remains to be seen what is behind their success. Recent studies have examined how pre-trained models can effectively learn syntax information based on Abstract Syntax Trees. In this paper, we figure out what role the self-attention mechanism plays in understanding code syntax and  Cites: Codexglue: A machine learning benchmark dataset for code