---
layout: post
title:  "On the probability-quality paradox in language generation"
date:   2022-04-04 16:51:29 -0400
categories: jekyll update
author: "C Meister, G Wiher, T Pimentel, R Cotterell - arXiv preprint arXiv:2203.17217, 2022"
---
When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode- seeking decoding methods, ie, those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through Cites: Consistency of a recurrent language model with respect to