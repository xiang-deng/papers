--- 
layout: post 
title: "DCT-Former: Efficient Self-Attention with Discrete Cosine Transform" 
date: 2022-09-12 23:50:28 -0400 
categories: jekyll update 
author: "C Scribano, G Franchini, M Prato, M Bertogna - arXiv preprint arXiv:2203.01178, 2022" 
--- 
Since their introduction the Trasformer architectures emerged as the dominating architectures for both natural language processing and, more recently, computer vision applications. An intrinsic limitation of this family of fully-attentive architectures arises from the computation of the dot-product attention, which grows both in memory consumption and number of operations as O (n2) where n stands for the input sequence length, thus limiting the applications that require modeling very long Cites: Hard-Coded Gaussian Attention for Neural Machine Translation