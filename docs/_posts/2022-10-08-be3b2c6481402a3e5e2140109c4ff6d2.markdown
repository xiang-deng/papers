--- 
layout: post 
title: "Learning an Invertible Output Mapping Can Mitigate Simplicity Bias in Neural Networks" 
date: 2022-10-08 00:45:41 -0400 
categories: jekyll update 
author: "S Addepalli, A Nasery, RV Babu, P Netrapalli, P Jain - arXiv preprint arXiv , 2022" 
--- 
Deep Neural Networks are known to be brittle to even minor distribution shifts compared to the training distribution. While one line of work has demonstrated that Simplicity Bias (SB) of DNNs-bias towards learning only the simplest features-is a key reason for this brittleness, another recent line of work has surprisingly found that diverse/complex features are indeed learned by the backbone, and their brittleness is due to the linear classification head relying primarily on the simplest features. To Cites: Fine-tuning can distort pretrained features and underperform out-of