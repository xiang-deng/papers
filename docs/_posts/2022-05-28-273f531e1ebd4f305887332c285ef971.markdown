---
layout: post
title:  "Learning to Ignore Adversarial Attacks"
date:   2022-05-28 02:05:27 -0400
categories: jekyll update
author: "Y Zhang, Y Zhou, S Carton, C Tan - arXiv preprint arXiv:2205.11551, 2022"
---
Despite the strong performance of current NLP models, they can be brittle against adversarial attacks. To enable effective learning against adversarial inputs, we introduce the use of rationale models that can explicitly learn to ignore attack tokens. We find that the rationale models can successfully ignore over 90\% of attack tokens. This approach leads to consistent sizable improvements ($\sim $10\%) over baseline models in robustness on three datasets for both BERT and RoBERTa, and also … Cites: ‪When Can Models Learn From Explanations? A Formal …‬