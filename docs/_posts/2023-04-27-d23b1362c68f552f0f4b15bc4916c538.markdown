--- 
layout: post 
title: "Boosting Theory-of-Mind Performance in Large Language Models via Prompting" 
date: 2023-04-27 01:18:20 -0400 
categories: jekyll update 
author: "SR Moghaddam, CJ Honey - arXiv preprint arXiv:2304.11490, 2023" 
--- 
Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of  Cites: Neural theory-of-mind? on the limits of social intelligence in large lms