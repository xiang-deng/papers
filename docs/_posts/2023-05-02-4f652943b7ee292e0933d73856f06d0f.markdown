--- 
layout: post 
title: "Triple-Hybrid Energy-based Model Makes Better Calibrated Natural Language Understanding Models" 
date: 2023-05-02 02:27:35 -0400 
categories: jekyll update 
author: "H Xu, Y Zhang - Proceedings of the 17th Conference of the European , 2023" 
--- 
Though pre-trained language models achieve notable success in many applications, it s usually controversial for over-confident predictions. Specifically, the in-distribution (ID) miscalibration and out-of-distribution (OOD) detection are main concerns. Recently, some works based on energy-based models~(EBM) have shown great improvements on both ID calibration and OOD detection for images. However, it s rarely explored in natural language understanding tasks due to the non  Cites: Calibration of Pre-trained Transformers