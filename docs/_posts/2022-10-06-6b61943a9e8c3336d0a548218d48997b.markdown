--- 
layout: post 
title: "Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability" 
date: 2022-10-06 01:25:19 -0400 
categories: jekyll update 
author: "A Damian, E Nichani, JD Lee - arXiv preprint arXiv:2209.15594, 2022" 
--- 
Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness $ S (\theta) $, is bounded by $2/\eta $, training is stable and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen et al.(2021) observed two important phenomena. The first, dubbed progressive Cites: The break-even point on optimization trajectories of deep neural