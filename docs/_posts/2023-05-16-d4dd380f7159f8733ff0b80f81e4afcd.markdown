---
layout: post
title:  "Advancing Neural Encoding of Portuguese with Transformer Albertina PT"
date:   2023-05-16 05:31:31 -0400
categories: jekyll update
author: "J Rodrigues, L Gomes, J Silva, A Branco, R Santos… - arXiv preprint arXiv …, 2023"
---
To advance the neural encoding of Portuguese (PT), and a fortiori the technological preparation of this language for the digital age, we developed a Transformer-based foundation model that sets a new state of the art in this respect for two of its variants, namely European Portuguese from Portugal (PT-PT) and American Portuguese from Brazil (PT-BR). To develop this encoder, which we named Albertina PT-*, a strong model was used as a starting point, DeBERTa, and its pre-training was done over …
Cites: ‪To tune or not to tune? adapting pretrained representations to …‬