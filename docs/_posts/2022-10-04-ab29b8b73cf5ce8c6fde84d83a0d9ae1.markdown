--- 
layout: post 
title: "Bidirectional Language Models Are Also Few-shot Learners" 
date: 2022-10-04 00:49:37 -0400 
categories: jekyll update 
author: "A Patel, B Li, MS Rasooli, N Constant, C Raffel - arXiv preprint arXiv , 2022" 
--- 
Large language models such as GPT-3 (Brown et al., 2020) can perform arbitrary tasks without undergoing fine-tuning after being prompted with only a few labeled examples. An arbitrary task can be reformulated as a natural language prompt, and a language model can be asked to generate the completion, indirectly performing the task in a paradigm known as prompt-based learning. To date, emergent prompt-based learning capabilities have mainly been demonstrated for unidirectional  Cites: Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper