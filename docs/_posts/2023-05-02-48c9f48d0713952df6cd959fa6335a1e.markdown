--- 
layout: post 
title: "When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization" 
date: 2023-05-02 02:27:35 -0400 
categories: jekyll update 
author: "F Ladhak, E Durmus, M Suzgun, T Zhang, D Jurafsky - Proceedings of the 17th , 2023" 
--- 
Large language models (LLMs) are subject to sociocultural and other biases previously identified using intrinsic evaluations. However, when and how these intrinsic biases in pre-trained LM representations propagate to downstream, fine-tuned NLP tasks like summarization is not well understood. In this work, we investigate one type of biasname-nationality biasand trace it from the pre-training stage to a downstream summarization task across multiple summarization  Cites: Adapterhub: A framework for adapting transformers