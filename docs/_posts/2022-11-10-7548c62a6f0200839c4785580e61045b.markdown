--- 
layout: post 
title: "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers" 
date: 2022-11-10 01:14:02 -0400 
categories: jekyll update 
author: "M Hassid, H Peng, D Rotem, J Kasai, I Montero - arXiv preprint arXiv , 2022" 
--- 
The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as