---
layout: post
title:  "Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models"
date:   2023-05-19 23:52:25 -0400
categories: jekyll update
author: "B Cao, Q Tang, H Lin, X Han, J Chen, T Wang, L Sun - arXiv preprint arXiv …, 2023"
---
Memory is one of the most essential cognitive functions serving as a repository of world knowledge and episodes of activities. In recent years, large-scale pre-trained language models have shown remarkable memorizing ability. On the contrary, vanilla neural networks without pre-training have been long observed suffering from the catastrophic forgetting problem. To investigate such a retentive-forgetful contradiction and understand the memory mechanism of language models, we …
Cites: ‪Memorization without overfitting: Analyzing the training dynamics …‬