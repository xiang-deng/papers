---
layout: post
title:  "Plot Writing From Pre-Trained Language Models"
date:   2022-06-10 22:27:43 -0400
categories: jekyll update
author: "Y Jin, V Kadam, D Wanvarie - arXiv preprint arXiv:2206.03021, 2022"
---
Pre-trained language models (PLMs) fail to generate long-form narrative text because they do not consider global structure. As a result, the generated texts are often incohesive, repetitive, or lack content. Recent work in story generation reintroduced explicit content planning in the form of prompts, keywords, or semantic frames. Trained on large parallel corpora, these models can generate more logical event sequences and thus more contentful stories. However, these intermediate …
Cites: ‪Language models as fact checkers?‬  