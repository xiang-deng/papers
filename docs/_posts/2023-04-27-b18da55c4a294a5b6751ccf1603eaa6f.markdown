---
layout: post
title:  "Scaling Transformer to 1M tokens and beyond with RMT"
date:   2023-04-27 01:18:20 -0400
categories: jekyll update
author: "A Bulatov, Y Kuratov, MS Burtsev - arXiv preprint arXiv:2304.11062, 2023"
---
This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model s effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global …
Cites: ‪Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt …‬