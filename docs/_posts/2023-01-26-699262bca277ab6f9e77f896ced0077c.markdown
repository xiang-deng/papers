---
layout: post
title:  "Holistically Explainable Vision Transformers"
date:   2023-01-26 15:19:03 -0400
categories: jekyll update
author: "M Böhle, M Fritz, B Schiele - arXiv preprint arXiv:2301.08669, 2023"
---
Transformers increasingly dominate the machine learning landscape across many tasks and domains, which increases the importance for understanding their outputs. While their attention modules provide partial insight into their inner workings, the attention scores have been shown to be insufficient for explaining the models as a whole. To address this, we propose B-cos transformers, which inherently provide holistic explanations for their decisions. Specifically, we formulate each model …
Cites: ‪Is Attention Interpretable?‬