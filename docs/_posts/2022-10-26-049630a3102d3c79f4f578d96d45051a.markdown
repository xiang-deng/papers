--- 
layout: post 
title: "Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models" 
date: 2022-10-26 13:20:27 -0400 
categories: jekyll update 
author: "H Chen, Y Ji, D Evans - arXiv preprint arXiv:2210.11498, 2022" 
--- 
Traditional (fickle) adversarial examples involve finding a small perturbation that does not change an input s true label but confuses the classifier into outputting a different prediction. Conversely, obstinate adversarial examples occur when an adversary finds a small perturbation that preserves the classifier s prediction but changes the true label of an input. Adversarial training and certified robust training have shown some effectiveness in improving the robustness of machine learnt Cites: Pathologies of neural models make interpretations difficult