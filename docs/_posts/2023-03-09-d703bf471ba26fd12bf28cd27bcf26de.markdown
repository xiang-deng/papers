--- 
layout: post 
title: "Towards Zero-Shot Functional Compositionality of Language Models" 
date: 2023-03-09 05:52:34 -0400 
categories: jekyll update 
author: "H Yu, M Jeong, J Shin, H Moon, J Park, S Choi - arXiv preprint arXiv:2303.03103, 2023" 
--- 
Large Pre-trained Language Models (PLM) have become the most desirable starting point in the field of NLP, as they have become remarkably good at solving many individual tasks. Despite such success, in this paper, we argue that current paradigms of working with PLMs are neglecting a critical aspect of modeling human intelligence: functional compositionality. Functional compositionality-the ability to compose learned tasks-has been a long-standing challenge in the field of AI (and  Cites: Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper