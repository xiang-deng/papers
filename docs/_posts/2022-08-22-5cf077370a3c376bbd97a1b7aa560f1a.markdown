--- 
layout: post 
title: "Exploring and Exploiting Multi-Granularity Representations for Machine Reading Comprehension" 
date: 2022-08-22 23:37:16 -0400 
categories: jekyll update 
author: "N Chen, C You - arXiv preprint arXiv:2208.08750, 2022" 
--- 
Recently, the attention-enhanced multi-layer encoder, such as Transformer, has been extensively studied in Machine Reading Comprehension (MRC). To predict the answer, it is common practice to employ a predictor to draw information only from the final encoder layer which generates the coarse-grained representations of the source sequences, ie, passage and question. The analysis shows that the representation of source sequence becomes more coarse-grained from finegrained Cites: Bidirectional Attention Flow for Machine Comprehension