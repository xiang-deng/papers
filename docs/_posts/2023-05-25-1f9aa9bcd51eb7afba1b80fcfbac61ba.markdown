--- 
layout: post 
title: "Are Large Language Models Good Evaluators for Abstractive Summarization?" 
date: 2023-05-25 03:51:47 -0400 
categories: jekyll update 
author: "C Shen, L Cheng, Y You, L Bing - arXiv preprint arXiv:2305.13091, 2023" 
--- 
Human evaluations are often required for abstractive summary evaluations to give fairer judgments. However, they are often time-consuming, costly, inconsistent, and non-reproducible. To overcome these challenges, we explore the potential of using an out-of-the-box LLM (ie gpt-3.5-turbo ) for summarization evaluation without manually selecting demonstrations or complex prompt tuning. We compare different evaluation methods, including 2 methods for Likert-scale scoring and 1 method for  Cites: Summeval: Re-evaluating summarization evaluation