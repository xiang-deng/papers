---
layout: post
title:  "Language Contamination Explains the Cross-lingual Capabilities of English Pretrained Models"
date:   2022-04-23 07:54:44 -0400
categories: jekyll update
author: "T Blevins, L Zettlemoyer - arXiv preprint arXiv:2204.08110, 2022"
---
English pretrained language models, which make up the backbone of many modern NLP systems, require huge amounts of unlabeled training data. These models are generally presented as being trained only on English text but have been found to