--- 
layout: post 
title: "OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization" 
date: 2023-04-11 07:02:19 -0400 
categories: jekyll update 
author: "C Guo, J Tang, W Hu, J Leng, C Zhang, F Yang, Y Liu - Matrix, 2023" 
--- 
Transformer-based large language models (LLMs) have achieved great success with the growing model size. LLMs size grows by 240 every two years, which outpaces the hardware progress and makes model inference increasingly costly. Model quantization is a promising approach to mitigate the widening gap between LLM size and hardware capacity. However, the existence of outliers, values with significant magnitudes, in LLMs makes existing quantization methods less effective. Prior outlier  Cites: Documenting Large Webtext Corpora: A Case Study on the