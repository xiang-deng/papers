---
layout: post
title:  "BiT: Robustly Binarized Multi-distilled Transformer"
date:   2022-05-30 22:20:45 -0400
categories: jekyll update
author: "Z Liu, B Oguz, A Pappu, L Xiao, S Yih, M Li - arXiv preprint arXiv , 2022"
---
Modern pre-trained transformers have rapidly advanced the state-of-the-art in machine learning, but have also grown in parameters and computational complexity, making them increasingly difficult to deploy in resource-constrained environments