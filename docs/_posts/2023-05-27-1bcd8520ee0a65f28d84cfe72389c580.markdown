--- 
layout: post 
title: "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners" 
date: 2023-05-27 10:00:59 -0400 
categories: jekyll update 
author: "X Tang, Z Zheng, J Li, F Meng, SC Zhu, Y Liang - arXiv preprint arXiv , 2023" 
--- 
The emergent few-shot reasoning capabilities of Large Language Models (LLMs) have excited the natural language and machine learning community over recent years. Despite of numerous successful applications, the underlying mechanism of such in-context capabilities still remains unclear. In this work, we hypothesize that the learned\textit {semantics} of language tokens do the most heavy lifting during the reasoning process. Different from human s symbolic reasoning process, the semantic Cites: What learning algorithm is in-context learning? investigations with