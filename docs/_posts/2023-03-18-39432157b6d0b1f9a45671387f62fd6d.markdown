--- 
layout: post 
title: "A Comprehensive Study on Post-Training Quantization for Large Language Models" 
date: 2023-03-18 01:48:35 -0400 
categories: jekyll update 
author: "Z Yao, C Li, X Wu, S Youn, Y He - arXiv preprint arXiv:2303.08302, 2023" 
--- 
Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different\ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained Cites: Opt: Open pre-trained transformer language models