--- 
layout: post 
title: "Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study" 
date: 2022-12-23 23:45:02 -0400 
categories: jekyll update 
author: "D Wu, WU Ahmad, KW Chang - arXiv preprint arXiv:2212.10233, 2022" 
--- 
Neural models that do not rely on pre-training have excelled in the keyphrase generation task with large annotated datasets. Meanwhile, new approaches have incorporated pre-trained language models (PLMs) for their data efficiency. However, there lacks a systematic study of how the two types of approaches compare and how different design choices can affect the performance of PLM-based models. To fill in this knowledge gap and facilitate a more informed use of PLMs for keyphrase  Cites: Well-read students learn better: On the importance of pre-training