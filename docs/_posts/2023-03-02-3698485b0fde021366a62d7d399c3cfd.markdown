--- 
layout: post 
title: "Don t be fooled: label leakage in explanation methods and the importance of their quantitative evaluation" 
date: 2023-03-02 06:18:50 -0400 
categories: jekyll update 
author: "N Jethani, A Saporta, R Ranganath - arXiv preprint arXiv:2302.12893, 2023" 
--- 
Feature attribution methods identify which features of an input most influence a model s output. Most widely-used feature attribution methods (such as SHAP, LIME, and Grad-CAM) are class-dependent methods in that they generate a feature attribution vector as a function of class. In this work, we demonstrate that class-dependent methods can leak information about the selected class, making that class appear more likely than it is. Thus, an end user runs the risk of drawing false Cites: Why Should I Trust You? : Explaining the Predictions of Any