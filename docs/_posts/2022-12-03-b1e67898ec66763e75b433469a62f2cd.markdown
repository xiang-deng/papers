--- 
layout: post 
title: "Penalizing Confident Predictions on Largely Perturbed Inputs Does Not Improve Out-of-Distribution Generalization in Question Answering" 
date: 2022-12-03 01:42:11 -0400 
categories: jekyll update 
author: "K Shinoda, S Sugawara, A Aizawa - arXiv preprint arXiv:2211.16093, 2022" 
--- 
Question answering (QA) models are shown to be insensitive to large perturbations to inputs; that is, they make correct and confident predictions even when given largely perturbed inputs from which humans can not correctly derive answers. In addition, QA models fail to generalize to other domains and adversarial test sets, while humans maintain high accuracy. Based on these observations, we assume that QA models do not use intended features necessary for human reading but rely on Cites: MultiQA: An Empirical Investigation of Generalization and Transfer