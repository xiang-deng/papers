--- 
layout: post 
title: "Effective Cross-Task Transfer Learning for Explainable Natural Language Inference with T5" 
date: 2022-11-03 01:42:13 -0400 
categories: jekyll update 
author: "I Bigoulaeva, R Sachdeva, HT Madabushi - arXiv preprint arXiv , 2022" 
--- 
We compare sequential fine-tuning with a model for multi-task learning in the context where we are interested in boosting performance on two tasks, one of which depends on the other. We test these models on the FigLang2022 shared task which requires participants to predict language inference labels on figurative language along with corresponding textual explanations of the inference predictions. Our results show that while sequential multi-task learning can be tuned to be good at the Cites: AdapterFusion: Non-destructive task composition for transfer learning