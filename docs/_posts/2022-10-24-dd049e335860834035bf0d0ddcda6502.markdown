---
layout: post
title:  "Pre-training Language Models with Deterministic Factual Knowledge"
date:   2022-10-24 23:22:19 -0400
categories: jekyll update
author: "S Li, X Li, L Shang, C Sun, B Liu, Z Ji, X Jiang, Q Liu - arXiv preprint arXiv:2210.11165, 2022"
---
Previous works show that Pre-trained Language Models (PLMs) can capture factual knowledge. However, some analyses reveal that PLMs fail to perform it robustly, eg, being sensitive to the changes of prompts when extracting factual knowledge. To mitigate this issue, we propose to let PLMs learn the deterministic relationship between the remaining context and the masked content. The deterministic relationship ensures that the masked factual content can be deterministically …
Cites: ‪SpanBERT: Improving Pre-training by Representing and Predicting …‬