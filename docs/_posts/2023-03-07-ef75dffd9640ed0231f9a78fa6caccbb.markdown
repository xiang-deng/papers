---
layout: post
title:  "Safety without alignment"
date:   2023-03-07 06:19:37 -0400
categories: jekyll update
author: "A Kornai, M Bukatin, Z Zombori - arXiv preprint arXiv:2303.00752, 2023"
---
Currently, the dominant paradigm in AI safety is alignment with human values. Here we describe progress on developing an alternative approach to safety, based on ethical rationalism (Gewirth: 1978), and propose an inherently safe implementation path via hybrid theorem provers in a sandbox. As AGIs evolve, their alignment may fade, but their rationality can only increase (otherwise more rational ones will have a significant evolutionary advantage) so an approach that ties their ethics to their …
Cites: ‪Memory-Based Model Editing at Scale‬