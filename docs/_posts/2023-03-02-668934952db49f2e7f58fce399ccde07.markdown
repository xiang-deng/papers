---
layout: post
title:  "LLaMA: Open and Efficient Foundation Language Models"
date:   2023-03-02 06:18:50 -0400
categories: jekyll update
author: "H Touvron, T Lavril, G Izacard, X Martinet, MA Lachaux… - arXiv preprint arXiv …, 2023"
---
We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the …
Cites: ‪Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc Le …‬