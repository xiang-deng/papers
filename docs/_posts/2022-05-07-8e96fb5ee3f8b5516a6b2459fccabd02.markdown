---
layout: post
title:  "OmniKnight: Multilingual Neural Machine Translation with Language-Specific Self-Distillation"
date:   2022-05-07 02:52:45 -0400
categories: jekyll update
author: "Y Huang, X Feng, X Geng, B Qin - arXiv preprint arXiv:2205.01620, 2022"
---
Although all-in-one-model multilingual neural machine translation (MNMT) has achieved remarkable progress in recent years, its selected best overall checkpoint fails to achieve the best performance simultaneously in all language pairs. It is because that the best checkpoints for each individual language pair (ie, language- specific best checkpoints) scatter in different epochs. In this paper, we present a novel training strategy dubbed Language-Specific Self-Distillation (LSSD) for Cites: Distributionally Robust Multilingual Machine Translation