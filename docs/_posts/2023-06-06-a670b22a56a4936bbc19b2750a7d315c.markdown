--- 
layout: post 
title: "Birth of a Transformer: A Memory Viewpoint" 
date: 2023-06-06 05:46:58 -0400 
categories: jekyll update 
author: "A Bietti, V Cabannes, D Bouchacourt, H Jegou - arXiv preprint arXiv , 2023" 
--- 
Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic  Cites: Palm: Scaling language modeling with pathways