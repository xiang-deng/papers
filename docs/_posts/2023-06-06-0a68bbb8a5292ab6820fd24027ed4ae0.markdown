---
layout: post
title:  "Preference-grounded Token-level Guidance for Language Model Fine-tuning"
date:   2023-06-06 05:46:58 -0400
categories: jekyll update
author: "S Yang, S Zhang, C Xia, Y Feng, C Xiong, M Zhou - arXiv preprint arXiv:2306.00398, 2023"
---
Aligning language models (LMs) with preferences is an important problem in natural language generation. A key challenge is that preferences are typically provided at the sequence level while LM training and generation both occur at the token level. There is, therefore, a granularity mismatch between the preference and the LM training losses, which may complicate the learning problem. In this paper, we address this issue by developing an alternate training process, where we iterate …
Cites: ‪GrIPS: Gradient-free, Edit-based Instruction Search for Prompting …‬