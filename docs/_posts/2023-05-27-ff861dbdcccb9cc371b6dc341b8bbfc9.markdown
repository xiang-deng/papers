--- 
layout: post 
title: "BiasX: Thinking Slow in Toxic Content Moderation with Explanations of Implied Social Biases" 
date: 2023-05-27 10:00:59 -0400 
categories: jekyll update 
author: "Y Zhang, S Nanduri, L Jiang, T Wu, M Sap - arXiv preprint arXiv:2305.13589, 2023" 
--- 
Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit Cites: Does the Whole Exceed its Parts? The Effect of AI Explanations on