--- 
layout: post 
title: "A Survey of Knowledge-Enhanced Pre-trained Language Models" 
date: 2022-11-17 00:57:01 -0400 
categories: jekyll update 
author: "L Hu, Z Liu, Z Zhao, L Hou, L Nie, J Li - arXiv preprint arXiv:2211.05994, 2022" 
--- 
Pre-trained Language Models (PLMs) which are trained on large text corpus through the self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge Cites: Unifiedskg: Unifying and multi-tasking structured knowledge