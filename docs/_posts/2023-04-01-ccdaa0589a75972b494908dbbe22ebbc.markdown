--- 
layout: post 
title: "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention" 
date: 2023-04-01 04:48:36 -0400 
categories: jekyll update 
author: "R Zhang, J Han, A Zhou, X Hu, S Yan, P Lu, H Li - arXiv preprint arXiv , 2023" 
--- 
We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2 M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero Cites: Opt: Open pre-trained transformer language models