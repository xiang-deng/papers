--- 
layout: post 
title: "Why do Nearest Neighbor Language Models Work?" 
date: 2023-01-12 00:32:14 -0400 
categories: jekyll update 
author: "FF Xu, U Alon, G Neubig - arXiv preprint arXiv:2301.02828, 2023" 
--- 
Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric  Cites: The curious case of neural text degeneration