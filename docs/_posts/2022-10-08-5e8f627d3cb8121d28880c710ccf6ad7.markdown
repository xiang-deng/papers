--- 
layout: post 
title: "Complexity-Based Prompting for Multi-Step Reasoning" 
date: 2022-10-08 00:45:41 -0400 
categories: jekyll update 
author: "Y Fu, H Peng, A Sabharwal, P Clark, T Khot - arXiv preprint arXiv:2210.00720, 2022" 
--- 
We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a Cites: Selective Annotation Makes Language Models Better Few-Shot