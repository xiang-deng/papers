--- 
layout: post 
title: "Scale-invariant Bayesian Neural Networks with Connectivity Tangent Kernel" 
date: 2022-10-06 01:25:19 -0400 
categories: jekyll update 
author: "SY Kim, S Park, K Kim, E Yang - arXiv preprint arXiv:2209.15208, 2022" 
--- 
Explaining generalizations and preventing over-confident predictions are central goals of studies on the loss landscape of neural networks. Flatness, defined as loss invariability on perturbations of a pre-trained solution, is widely accepted as a predictor of generalization in this context. However, the problem that flatness and generalization bounds can be changed arbitrarily according to the scale of a parameter was pointed out, and previous studies partially solved the problem with Cites: Catastrophic fisher explosion: Early phase fisher matrix impacts