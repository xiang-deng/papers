--- 
layout: post 
title: "ContraBERT: Enhancing Code Pre-trained Models via Contrastive Learning" 
date: 2023-01-28 04:04:00 -0400 
categories: jekyll update 
author: "S Liu, B Wu, X Xie, G Meng, Y Liu - arXiv preprint arXiv:2301.09072, 2023" 
--- 
Large-scale pre-trained models such as CodeBERT, GraphCodeBERT have earned widespread attention from both academia and industry. Attributed to the superior ability in code representation, they have been further applied in multiple downstream tasks such as clone detection, code search and code translation. However, it is also observed that these state-of-the-art pre-trained models are susceptible to adversarial attacks. The performance of these pre-trained models drops significantly with simple Cites: VarCLR: Variable Semantic Representation Pre-training via