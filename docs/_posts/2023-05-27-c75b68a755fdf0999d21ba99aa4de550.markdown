--- 
layout: post 
title: "Selective Pre-training for Private Fine-tuning" 
date: 2023-05-27 10:00:59 -0400 
categories: jekyll update 
author: "D Yu, S Gopi, J Kulkarni, Z Lin, S Naik, TL Religa, J Yin - arXiv preprint arXiv , 2023" 
--- 
Suppose we want to train text prediction models in email clients or word processors. The models must preserve the privacy of user data and adhere to a specific fixed size to meet memory and inference time requirements. We introduce a generic framework to solve this problem. Specifically, we are given a public dataset $ D_\text {pub} $ and a private dataset $ D_\text {priv} $ corresponding to a downstream task $ T $. How should we pre-train a fixed-size model $ M $ on $ D_\text {pub} $ and fine-tune  Cites: Structured Pruning Learns Compact and Accurate Models