--- 
layout: post 
title: "Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach" 
date: 2022-03-22 03:39:25 -0400 
categories: jekyll update 
author: "B Wang, X Deng, H Sun - arXiv preprint arXiv:2203.08383, 2022" 
--- 
While Pre-trained Language Models (PLMs) internalize a great amount of world knowledge, they have been shown incapable of recalling these knowledge to solve tasks requiring complex & multi-step inference procedures. Similar to how humans develop a train of thought for these tasks, how can we equip PLMs with such abilities? In this work, we explore an iterative prompting framework, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step Cites: Reframing Instructional Prompts to GPTk s Language