--- 
layout: post 
title: "A Pretrainer s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity" 
date: 2023-05-25 03:51:47 -0400 
categories: jekyll update 
author: "S Longpre, G Yauney, E Reif, K Lee, A Roberts, B Zoph - arXiv preprint arXiv , 2023" 
--- 
Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28