--- 
layout: post 
title: "Training-free Lexical Backdoor Attacks on Language Models" 
date: 2023-02-11 02:41:58 -0400 
categories: jekyll update 
author: "Y Huang, TY Zhuo, Q Xu, H Hu, X Yuan, C Chen - arXiv preprint arXiv:2302.04116, 2023" 
--- 
Large-scale language models have achieved tremendous success across various natural language processing (NLP) applications. Nevertheless, language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors. Most existing backdoor attacks, such as data poisoning, require further (re) training or fine-tuning language models to learn the intended backdoor patterns. The additional training process however diminishes the Cites: Red alarm for pre-trained models: Universal vulnerability to neuron