---
layout: post
title:  "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval"
date:   2023-05-25 03:51:47 -0400
categories: jekyll update
author: "Z Li, Y Zhang, D Long, P Xie - arXiv preprint arXiv:2305.13197, 2023"
---
Recently, various studies have been directed towards exploring dense passage retrieval techniques employing pre-trained language models, among which the masked auto-encoder (MAE) pre-training architecture has emerged as the most promising. The conventional MAE framework relies on leveraging the passage reconstruction of decoder to bolster the text representation ability of encoder, thereby enhancing the performance of resulting dense retrieval systems. Within the context of …
Cites: ‪Unsupervised dense information retrieval with contrastive learning‬