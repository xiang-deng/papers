--- 
layout: post 
title: "SA-SGRU: Combining Improved Self-Attention and Skip-GRU for Text Classification" 
date: 2023-01-21 07:31:42 -0400 
categories: jekyll update 
author: "Y Huang, X Dai, J Yu, Z Huang - Applied Sciences, 2023" 
--- 
When reading texts for text classification tasks, a large number of words are irrelevant, and in text classification tasks, the traditional self-attention mechanism has the problem of weight distribution limitations. Therefore, a text classification model that combines an improved selfattention mechanism with a Skip-GRU (Skip-grate recurrent unit) network (SA-SGRU) is proposed in this paper. Firstly, Skip-GRU, the enhanced model of GRU (Grate Recurrent Unit), is used to skip the content that is not Cites: Learning structured text representations