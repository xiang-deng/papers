---
layout: post
title:  "TempLM: Distilling Language Models into Template-Based Generators"
date:   2022-05-28 02:05:27 -0400
categories: jekyll update
author: "T Zhang, M Lee, L Li, E Shen, TB Hashimoto - arXiv preprint arXiv:2205.11055, 2022"
---
While pretrained language models (PLMs) have greatly improved text generation, they have also been known to produce unfaithful or inappropriate content. In contrast, classic template-based systems provide strong guarantees of faithfulness at the cost of fluency. We propose TempLM, which achieves the best of both worlds by distilling a PLM into a template-based generator. On the E2E and SynthBio data-to-text datasets, we show that TempLM is more faithful than the original PLM and is … Cites: ‪Data-to-text generation with content selection and planning‬