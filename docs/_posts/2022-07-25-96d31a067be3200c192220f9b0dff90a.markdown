--- 
layout: post 
title: "Doge Tickets: Uncovering Domain-general Language Models by Playing Lottery Tickets" 
date: 2022-07-25 21:58:40 -0400 
categories: jekyll update 
author: "Y Yang, C Zhang, B Wang, D Song - arXiv preprint arXiv:2207.09638, 2022" 
--- 
Over-parameterized models, typically pre-trained language models (LMs), have shown an appealing expressive power due to their small learning bias. However, the huge learning capacity of LMs can also lead to large learning variance. In a pilot study, we find that, when faced with multiple domains, a critical portion of parameters behave unexpectedly in a domain-specific manner while others behave in a domain-general one. Motivated by this phenomenon, we for the first time posit that domain  Cites: Are Sixteen Heads Really Better than One?