--- 
layout: post 
title: "ReAugKD: Retrieval-augmented knowledge distillation for pre-trained language models" 
date: 2023-05-19 23:52:25 -0400 
categories: jekyll update 
author: "J Zhang, A Muhamed, A Anantharaman, G Wang - 2023" 
--- 
Abstract Knowledge Distillation (KD)(Hinton et al., 2015) is one of the most effective approaches for deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the largescale models to smaller student models. Previous KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non Cites: Retrieval-augmented generation for knowledge-intensive NLP tasks