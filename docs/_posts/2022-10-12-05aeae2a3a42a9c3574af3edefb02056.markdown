---
layout: post
title:  "Understanding Transformer Memorization Recall Through Idioms"
date:   2022-10-12 20:42:55 -0400
categories: jekyll update
author: "A Haviv, I Cohen, J Gidron, R Schuster, Y Goldberg… - arXiv preprint arXiv …, 2022"
---
To produce accurate predictions, language models (LMs) must balance between generalization and memorization. Yet, little is known about the mechanism by which transformer LMs employ their memorization capacity. When does a model decide to output a memorized phrase, and how is this phrase then retrieved from memory? In this work, we offer the first methodological framework for probing and characterizing recall of memorized sequences in transformer LMs. First, we lay out criteria for …
Cites: ‪Electra: Pre-training text encoders as discriminators rather than …‬