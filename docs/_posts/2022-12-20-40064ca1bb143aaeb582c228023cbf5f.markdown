--- 
layout: post 
title: "Transformers learn in-context by gradient descent" 
date: 2022-12-20 02:26:19 -0400 
categories: jekyll update 
author: "J von Oswald, E Niklasson, E Randazzo, J Sacramento - arXiv preprint arXiv , 2022" 
--- 
Transformers have become the state-of-the-art neural network architecture across numerous domains of machine learning. This is partly due to their celebrated ability to transfer and to learn in-context based on few examples. Nevertheless, the mechanisms by which Transformers become in-context learners are not well understood and remain mostly an intuition. Here, we argue that training Transformers on auto-regressive tasks can be closely related to well-known gradient-based meta  Cites: What learning algorithm is in-context learning? Investigations with