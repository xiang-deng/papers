--- 
layout: post 
title: "Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE" 
date: 2022-12-08 02:33:21 -0400 
categories: jekyll update 
author: "Q Zhong, L Ding, Y Zhan, Y Qiao, Y Wen, L Shen, J Liu - arXiv preprint arXiv , 2022" 
--- 
This technical report briefly describes our JDExplore d-team s Vega v2 submission on the SuperGLUE leaderboard. SuperGLUE is more challenging than the widely used general language understanding evaluation (GLUE) benchmark, containing eight difficult language understanding tasks, including question answering, natural language inference, word sense disambiguation, coreference resolution, and reasoning.[Method] Instead of arbitrarily increasing the size of a pretrained language Cites: SpanBERT: Improving Pre-training by Representing and Predicting