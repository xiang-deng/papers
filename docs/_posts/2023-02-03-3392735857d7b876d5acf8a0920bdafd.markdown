---
layout: post
title:  "Does Vision Accelerate Hierarchical Generalization of Neural Language Learners?"
date:   2023-02-03 14:16:33 -0400
categories: jekyll update
author: "T Kuribayashi - arXiv preprint arXiv:2302.00667, 2023"
---
Neural language models (LMs) are arguably less data-efficient than humans--why does this gap occur? In this study, we hypothesize that this gap stems from the learners  accessibility to modalities other than text, specifically, vision. We conducted two complementary experiments (using noisy, realistic data and a simplified, artificial one) toward the advantage of vision in the syntactic generalization of LMs. Our results showed that vision accelerated a proper linguistic generalization in the …
Cites: ‪Transformer feed-forward layers are key-value memories‬