---
layout: post
title:  "LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers"
date:   2023-06-02 15:36:55 -0400
categories: jekyll update
author: "X Liu, Z Liu - arXiv preprint arXiv:2305.18396, 2023"
---
Prior works have attempted to build private inference frameworks for transformer-based large language models (LLMs) in a server-client setting, where the server holds the model parameters and the client inputs the private data for inference. However, these frameworks impose significant overhead when the private inputs are forward propagated through the original LLMs. In this paper, we show that substituting the computation-and communication-heavy operators in the transformer …
Cites: ‪Well-read students learn better: The impact of student initialization …‬