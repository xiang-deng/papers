--- 
layout: post 
title: "Improving Knowledge Graph Embedding via Iterative Self-Semantic Knowledge Distillation" 
date: 2022-06-10 22:27:43 -0400 
categories: jekyll update 
author: "Z Zhou, D Chen, C Wang, Y Feng, C Chen - arXiv preprint arXiv:2206.02963, 2022" 
--- 
Knowledge graph embedding (KGE) has been intensively investigated for link prediction by projecting entities and relations into continuous vector spaces. Current popular high-dimensional KGE methods obtain quite slight performance gains while require enormous computation and memory costs. In contrast to high-dimensional KGE models, training low-dimensional models is more efficient and worthwhile for better deployments to practical intelligent systems. However, the model Cites: Observed versus latent features for knowledge base and text