--- 
layout: post 
title: "TranCIM: Full-Digital Bitline-Transpose CIM-based Sparse Transformer Accelerator With Pipeline/Parallel Reconfigurable Modes" 
date: 2022-11-01 03:49:43 -0400 
categories: jekyll update 
author: "F Tu, Z Wu, Y Wang, L Liang, L Liu, Y Ding, L Liu - IEEE Journal of Solid-State , 2022" 
--- 
Transformer models achieve excellent results in the fields like natural language processing, computer vision, and bioinformatics. Their large numbers of matrix multiplications (MMs) lead to substantial data movement and computation. Although computing-in-memory (CIM) has proven to be an efficient architecture for MM computation, transformer s attention mechanism raises new challenges in memory access and computation aspects: the dynamic MM in attention layers causes Cites: Natural questions: a benchmark for question answering research