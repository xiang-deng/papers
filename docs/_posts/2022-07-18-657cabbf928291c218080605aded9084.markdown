---
layout: post
title:  "Adversarial Self-Attention for Language Understanding"
date:   2022-07-18 23:00:30 -0400
categories: jekyll update
author: "H Wu, H Zhao - arXiv preprint arXiv:2206.12608, 2022"
---
An ultimate language system aims at the high generalization and robustness when adapting to diverse scenarios. Unfortunately, the recent white hope pre-trained language models (PrLMs) barely escape from stacking excessive parameters to the over-parameterized Transformer architecture to achieve higher performances. This paper thus proposes\textit {Adversarial Self-Attention} mechanism (ASA), which adversarially reconstructs the Transformer attentions and facilitates model training …
Cites: ‪Better fine-tuning by reducing representational collapse‬  