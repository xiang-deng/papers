---
layout: post
title:  "SAdaBoundNc: an adaptive subgradient online learning algorithm with logarithmic regret bounds"
date:   2022-12-13 08:01:52 -0400
categories: jekyll update
author: "L Wang, X Wang, T Li, R Zheng, J Zhu, M Zhang - Neural Computing and Applications, 2022"
---
Adaptive (sub) gradient methods have received wide applications such as the training of deep networks. The square-root regret bounds are achieved in convex settings. However, how to exploit strong convexity for improving convergence rate and improve the generalization performance of adaptive (sub) gradient methods remain an open problem. For this reason, we devise an adaptive subgradient online learning algorithm called SAdaBoundNc in strong convexity settings. Moreover, we …
Cites: ‪Improving generalization performance by switching from adam to sgd‬