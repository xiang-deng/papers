--- 
layout: post 
title: "Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging" 
date: 2022-12-14 16:04:21 -0400 
categories: jekyll update 
author: "P Lu, I Kobyzev, M Rezagholizadeh, A Rashid - arXiv preprint arXiv , 2022" 
--- 
Knowledge Distillation (KD) is a commonly used technique for improving the generalization of compact Pre-trained Language Models (PLMs) on downstream tasks. However, such methods impose the additional burden of training a separate teacher model for every new dataset. Alternatively, one may directly work on the improvement of the optimization procedure of the compact model toward better generalization. Recent works observe that the flatness of the local minimum  Cites: Don t give me the details, just the summary! topic-aware