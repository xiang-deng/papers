---
layout: post
title:  "AUTOMATIC ROMANIAN TEXT GENERATION USING GPT-2"
date:   2022-12-20 02:26:19 -0400
categories: jekyll update
author: "MC BUZEA, Ș TRĂUȘAN, T REBEDEA"
---
One of the most significant tasks in natural language processing (NLG) is text generation, which beneficiate from the recent architectures that use large pretrained transformer models, such as the Generative Pre-trained Transformer-2 (GPT-2) or GPT-3 developed by OpenAI, and Google s Bidirectional Encoder Representations from Transformers (BERT). The paper presents a NLG model based on the GPT-2 architecture that generates Romanian instances, using manually annotated texts. A …
Cites: ‪Progen: Language modeling for protein generation‬