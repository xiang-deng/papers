--- 
layout: post 
title: "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention" 
date: 2023-06-08 03:52:18 -0400 
categories: jekyll update 
author: "M Pagliardini, D Paliotta, M Jaggi, F Fleuret - arXiv preprint arXiv:2306.01160, 2023" 
--- 
Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention--which is the only component scaling quadratically wrt the sequence length--becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up  Cites: Structured Pruning Learns Compact and Accurate Models