--- 
layout: post 
title: "Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations" 
date: 2022-11-17 00:57:01 -0400 
categories: jekyll update 
author: "S Saha, P Hase, N Rajani, M Bansal - arXiv preprint arXiv:2211.07517, 2022" 
--- 
Recent work on explainable NLP has shown that few-shot prompting can enable large pretrained language models (LLMs) to generate grammatical and factual natural language explanations for data labels. In this work, we study the connection between explainability and sample hardness by investigating the following research question- Are LLMs and humans equally good at explaining data labels for both easy and hard samples? We answer this question by first collecting human-written Cites: Palm: Scaling language modeling with pathways