--- 
layout: post 
title: "Massive Language Models Can Be Accurately Pruned in One-Shot" 
date: 2023-01-04 14:44:31 -0400 
categories: jekyll update 
author: "E Frantar, D Alistarh - arXiv preprint arXiv:2301.00774, 2023" 
--- 
We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. When executing SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, we can reach 60% sparsity with Cites: The case for 4-bit precision: k-bit Inference Scaling Laws