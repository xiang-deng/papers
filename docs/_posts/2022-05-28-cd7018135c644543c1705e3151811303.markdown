---
layout: post
title:  "Active Learning Through a Covering Lens"
date:   2022-05-28 02:05:27 -0400
categories: jekyll update
author: "O Yehuda, A Dekel, G Hacohen, D Weinshall - arXiv preprint arXiv:2205.11320, 2022"
---
Deep active learning aims to reduce the annotation cost for deep neural networks, which are notoriously data-hungry. Until recently, deep active learning methods struggled in the low-budget regime, where only a small amount of samples are annotated. The situation has been alleviated by recent advances in self-supervised representation learning methods, which impart the geometry of the data representation with rich information about the points. Taking advantage of this  Cites: Cold-start active learning through self-supervised language 