---
layout: post
title:  "Dynamic Scheduled Sampling with Imitation Loss for Neural Text Generation"
date:   2023-02-03 14:16:33 -0400
categories: jekyll update
author: "X Lin, P Jwalapuram, S Joty - arXiv preprint arXiv:2301.13753, 2023"
---
State-of-the-art neural text generation models are typically trained to maximize the likelihood of each token in the ground-truth sequence conditioned on the previous target tokens. However, during inference, the model needs to make a prediction conditioned on the tokens generated by itself. This train-test discrepancy is referred to as exposure bias. Scheduled sampling is a curriculum learning strategy that gradually exposes the model to its own predictions during training to mitigate this …
Cites: ‪MTNT: A Testbed for Machine Translation of Noisy Text‬