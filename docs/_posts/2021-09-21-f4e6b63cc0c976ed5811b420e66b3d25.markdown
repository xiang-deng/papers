---
layout: post
title:  "Language Models are Few-shot Multilingual Learners"
date:   2021-09-21 13:29:05 -0400
categories: jekyll update
author: "GI Winata, A Madotto, Z Lin, R Liu, J Yosinski, P Fung - arXiv preprint arXiv , 2021"
---
General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre Cites: Prefix-tuning: Optimizing continuous prompts for generation