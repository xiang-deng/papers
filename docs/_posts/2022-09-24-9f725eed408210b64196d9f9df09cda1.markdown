--- 
layout: post 
title: "Enhance the Visual Representation via Discrete Adversarial Training" 
date: 2022-09-24 00:16:11 -0400 
categories: jekyll update 
author: "X Mao, Y Chen, R Duan, Y Zhu, G Qi, S Ye, X Li - arXiv preprint arXiv , 2022" 
--- 
Adversarial Training (AT), which is commonly accepted as one of the most effective approaches defending against adversarial examples, can largely harm the standard performance, thus has limited usefulness on industrial-scale production and applications. Surprisingly, this phenomenon is totally opposite in Natural Language Processing (NLP) task, where AT can even benefit for generalization. We notice the merit of AT in NLP tasks could derive from the discrete and symbolic input space. For Cites: Achieving Model Robustness through Discrete Adversarial Training