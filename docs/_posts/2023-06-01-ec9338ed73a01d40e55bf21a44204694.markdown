--- 
layout: post 
title: "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models" 
date: 2023-06-01 02:05:49 -0400 
categories: jekyll update 
author: "Z Liu, B Oguz, C Zhao, E Chang, P Stock, Y Mehdad - arXiv preprint arXiv , 2023" 
--- 
Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing Cites: Bit: Robustly binarized multi-distilled transformer