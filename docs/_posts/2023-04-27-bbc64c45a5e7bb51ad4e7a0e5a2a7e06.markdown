--- 
layout: post 
title: "Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation" 
date: 2023-04-27 01:18:20 -0400 
categories: jekyll update 
author: "F Huang, P Ke, M Huang - arXiv preprint arXiv:2304.11791, 2023" 
--- 
Non-AutoRegressive (NAR) text generation models have drawn much attention because of their significantly faster decoding speed and good generation quality in machine translation. However, in a wider range of text generation tasks, existing NAR models lack proper pre-training, making them still far behind the pre-trained autoregressive models. In this paper, we propose Pre-trained Directed Acyclic Transformer (PreDAT) and a novel pre-training task to promote prediction  Cites: Is GPT-3 text indistinguishable from human text? scarecrow: A