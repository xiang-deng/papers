--- 
layout: post 
title: "Boosting source code suggestion with self-supervised Transformer Gated Highway" 
date: 2022-11-08 00:47:36 -0400 
categories: jekyll update 
author: "Y Hussain, Z Huang, Y Zhou, S Wang - Journal of Systems and Software, 2022" 
--- 
Attention-based transformer language models have shown significant performance gains in various natural language tasks. In this work, we explore the impact of transformer language models on the task of source code suggestion. The core intention of this work is to boost the modeling performance for the source code suggestion task and to explore how the training procedures and model architectures impact modeling performance. Additionally, we propose a transformer-based self Cites: Graphcodebert: Pre-training code representations with data flow