--- 
layout: post 
title: "On Second Thought, Let s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning" 
date: 2022-12-20 02:26:19 -0400 
categories: jekyll update 
author: "O Shaikh, H Zhang, W Held, M Bernstein, D Yang - arXiv preprint arXiv:2212.08061, 2022" 
--- 
Generating a chain of thought (CoT) can increase large language model (LLM) performance on a wide range of tasks. Zero-shot CoT evaluations, however, have been conducted primarily on logical tasks (eg arithmetic, commonsense QA). In this paper, we perform a controlled evaluation of zero-shot CoT across two sensitive domains: harmful questions and stereotype benchmarks. We find that using zero-shot CoT reasoning in a prompt can significantly increase a model s likelihood to Cites: Rethinking the Role of Demonstrations: What Makes In-Context