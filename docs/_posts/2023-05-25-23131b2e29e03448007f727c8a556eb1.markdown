--- 
layout: post 
title: "Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance" 
date: 2023-05-25 03:51:47 -0400 
categories: jekyll update 
author: "Y Zhang, L Cui, D Cai, X Huang, T Fang, W Bi - arXiv preprint arXiv:2305.13225, 2023" 
--- 
ChatGPT and GPT-4 have attracted substantial interest from both academic and industrial circles, owing to their remarkable few-shot (or even zero-shot) ability to handle various tasks. Recent work shows that, after being fine-tuned with a few sets of instruction-driven data, the recently proposed LLM, LLaMa, exhibits an impressive capability to address a broad range of tasks. However, the zero-shot performance of LLMs does not consistently outperform that of models fined-tuned for specific  Cites: EditEval: An Instruction-Based Benchmark for Text Improvements