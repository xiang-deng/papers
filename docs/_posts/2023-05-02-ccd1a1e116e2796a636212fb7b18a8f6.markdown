--- 
layout: post 
title: "Robustness Challenges in Model Distillation and Pruning for Natural Language Understanding" 
date: 2023-05-02 02:27:35 -0400 
categories: jekyll update 
author: "M Du, S Mukherjee, Y Cheng, M Shokouhi, X Hu - Proceedings of the 17th , 2023" 
--- 
Recent work has focused on compressing pre-trained language models (PLMs) like BERT where the major focus has been to improve the in-distribution performance for downstream tasks. However, very few of these studies have analyzed the impact of compression on the generalizability and robustness of compressed models for out-of-distribution (OOD) data. Towards this end, we study two popular model compression techniques including knowledge distillation and pruning and show that the  Cites: Avoiding the hypothesis-only bias in natural language inference