---
layout: post
title:  "Demystifying BERT: System Design Implications"
date:   2022-12-17 01:50:56 -0400
categories: jekyll update
author: "S Pati, S Aga, N Jayasena, MD Sinclair - 2022 IEEE International Symposium on …, 2022"
---
Transfer learning in natural language processing (NLP) uses increasingly large models that tackle challenging problems. Consequently, these applications are driving the requirements of future systems. To this end, we study the computationally and time-intensive training phase of NLP models and identify how its algorithmic behavior can guide future accelerator design. We focus on BERT (Bi-directional Encoder Representations from Transformer), one of the most popular Transformer …
Cites: ‪Well-Read Students Learn Better: On the Importance of Pre …‬