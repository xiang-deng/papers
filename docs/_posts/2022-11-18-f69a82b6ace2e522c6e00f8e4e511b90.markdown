--- 
layout: post 
title: "RetroMAE v2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models" 
date: 2022-11-18 16:55:42 -0400 
categories: jekyll update 
author: "S Xiao, Z Liu - arXiv preprint arXiv:2211.08769, 2022" 
--- 
To better support retrieval applications such as web search and question answering, growing effort is made to develop retrieval-oriented language models. Most of the existing works focus on improving the semantic representation capability for the contextualized embedding of [CLS] token. However, recent study shows that the ordinary tokens besides [CLS] may provide extra information, which helps to produce a better representation effect. As such, it s necessary to extend the current methods Cites: Sparse, dense, and attentional representations for text retrieval