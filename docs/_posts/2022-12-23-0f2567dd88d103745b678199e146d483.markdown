--- 
layout: post 
title: "Careful Data Curation Stabilizes In-context Learning" 
date: 2022-12-23 23:45:02 -0400 
categories: jekyll update 
author: "TY Chang, R Jia - arXiv preprint arXiv:2212.10378, 2022" 
--- 
In-context learning (ICL) enables large language models (LLMs) to perform new tasks by prompting them with a sequence of training examples. However, ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance. In this paper, we show that curating a carefully chosen subset of training data greatly stabilizes ICL performance. We propose two methods to choose training subsets, both of which score training  Cites: BoolQ: Exploring the Surprising Difficulty of Natural Yes/No