---
layout: post
title:  "On effects of Knowledge Distillation on Transfer Learning"
date:   2022-10-22 02:20:44 -0400
categories: jekyll update
author: "S Thapa - arXiv preprint arXiv:2210.09668, 2022"
---
Knowledge distillation is a popular machine learning technique that aims to transfer knowledge from a large teacher network to a smaller student network and improve the student s performance by training it to emulate the teacher. In recent years, there has been significant progress in novel distillation techniques that push performance frontiers across multiple problems and benchmarks. Most of the reported work focuses on achieving state-of-the-art results on the specific problem. However, there …
Cites: ‪Anchors: High-Precision Model-Agnostic Explanations‬