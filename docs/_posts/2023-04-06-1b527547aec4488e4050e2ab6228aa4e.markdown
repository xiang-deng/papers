--- 
layout: post 
title: "Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation" 
date: 2023-04-06 06:45:39 -0400 
categories: jekyll update 
author: "M Liu, Y Bao, C Zhao, S Huang - arXiv preprint arXiv:2303.17910, 2023" 
--- 
Benefiting from the sequence-level knowledge distillation, the Non-Autoregressive Transformer (NAT) achieves great success in neural machine translation tasks. However, existing knowledge distillation has side effects, such as propagating errors from the teacher to NAT students, which may limit further improvements of NAT models and are rarely discussed in existing research. In this paper, we introduce selective knowledge distillation by introducing an NAT evaluator to select NAT  Cites: Deep encoder, shallow decoder: Reevaluating non-autoregressive