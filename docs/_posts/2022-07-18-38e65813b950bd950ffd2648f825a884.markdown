---
layout: post
title:  "N-Grammer: Augmenting Transformers with latent n-grams"
date:   2022-07-18 23:00:30 -0400
categories: jekyll update
author: "A Roy, R Anil, G Lai, B Lee, J Zhao, S Zhang, S Wang… - arXiv preprint arXiv …, 2022"
---
Transformer models have recently emerged as one of the foundational models in natural language processing, and as a byproduct, there is significant recent interest and investment in scaling these models. However, the training and inference costs of these large Transformer language models are prohibitive, thus necessitating more research in identifying more efficient variants. In this work, we propose a simple yet effective modification to the Transformer architecture inspired by the literature in …
Cites: ‪Revisiting Simple Neural Probabilistic Language Models‬  