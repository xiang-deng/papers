--- 
layout: post 
title: "Expediting Distributed DNN Training with Device Topology-Aware Graph Deployment" 
date: 2023-02-11 02:41:58 -0400 
categories: jekyll update 
author: "S Zhang, X Yi, L Diao, C Wu, S Wang, W Lin - IEEE Transactions on Parallel and , 2023" 
--- 
This paper presents TAG, an automatic system to derive optimized DNN training graph and its deployment onto any device topology, for expedited training in device-and topology-heterogeneous ML clusters. We novelly combine both the DNN computation graph and the device topology graph as input to a graph neural network (GNN), and join the GNN with a search-based method to quickly identify optimized distributed training strategies. To reduce communication in a heterogeneous cluster  Cites: Palm: Scaling language modeling with pathways