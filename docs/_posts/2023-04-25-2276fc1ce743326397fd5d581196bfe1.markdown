---
layout: post
title:  "A Theory on Adam Instability in Large-Scale Machine Learning"
date:   2023-04-25 03:38:40 -0400
categories: jekyll update
author: "I Molybog, P Albert, M Chen, Z DeVito, D Esiobu… - arXiv preprint arXiv …, 2023"
---
We present a theory for the previously unexplained divergent behavior noticed in the training of large language models. We argue that the phenomenon is an artifact of the dominant optimization algorithm used for training, called Adam. We observe that Adam can enter a state in which the parameter update vector has a relatively large norm and is essentially uncorrelated with the direction of descent on the training loss landscape, leading to divergence. This artifact is more likely to be observed in the …
Cites: ‪PaLM: Scaling language modeling with pathways‬