--- 
layout: post 
title: "To Adapt or to Fine-tune: A Case Study on Abstractive Summarization" 
date: 2022-09-05 21:46:44 -0400 
categories: jekyll update 
author: "Z Zhao, P Chen - arXiv preprint arXiv:2208.14559, 2022" 
--- 
Recent advances in the field of abstractive summarization leverage pre-trained language models rather than train a model from scratch. However, such models are sluggish to train and accompanied by a massive overhead. Researchers have proposed a few lightweight alternatives such as smaller adapters to mitigate the drawbacks. Nonetheless, it remains uncertain whether using adapters benefits the task of summarization, in terms of improved efficiency without an unpleasant sacrifice  Cites: Text summarization with pretrained encoders