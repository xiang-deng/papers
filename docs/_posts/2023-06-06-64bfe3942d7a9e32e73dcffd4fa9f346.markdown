---
layout: post
title:  "High-Efficiency Device-Cloud Collaborative Transformer Model"
date:   2023-06-06 05:46:58 -0400
categories: jekyll update
author: "P Jiang, K Xin, C Li, Y Zhou - Proceedings of the IEEE/CVF Conference on Computer …, 2023"
---
Abstract Natural Language Processing (NLP) experts have had significant success with unsupervised language pre-training techniques. However, compared to typical NLP models, modern self-attention models require far more computational and memory resources than conventional NLP models, making pre-training or even fine-tuning them quite costly. It drastically restricts their success and uses in a variety of fields. To improve the efficiency, we propose Device-Cloud Collaborative …
Cites: ‪Deep contextualized word representations‬