---
layout: post
title:  "Binarized Neural Machine Translation"
date:   2023-02-16 06:16:46 -0400
categories: jekyll update
author: "Y Zhang, A Garg, Y Cao, Ł Lew, B Ghorbani, Z Zhang… - arXiv preprint arXiv …, 2023"
---
The rapid scaling of language models is motivating research using low-bitwidth quantization. In this work, we propose a novel binarization technique for Transformers applied to machine translation (BMT), the first of its kind. We identify and address the problem of inflated dot-product variance when using one-bit weights and activations. Specifically, BMT leverages additional LayerNorms and residual connections to improve binarization quality. Experiments on the WMT dataset show …
Cites: ‪PaLM: Scaling language modeling with pathways‬