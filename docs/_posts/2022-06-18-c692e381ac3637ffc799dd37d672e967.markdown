---
layout: post
title:  "Fine-tuning Pre-trained Language Models with Noise Stability Regularization"
date:   2022-06-18 03:19:09 -0400
categories: jekyll update
author: "H Hua, X Li, D Dou, CZ Xu, J Luo - arXiv preprint arXiv:2206.05658, 2022"
---
The advent of large-scale pre-trained language models has contributed greatly to the recent progress in natural language processing. Many state-of-the-art language models are first trained on a large text corpus and then fine-tuned on downstream tasks. Despite its recent success and wide adoption, fine-tuning a pre-trained language model often suffers from overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples 
Cites: MRQA 2019 Shared Task: Evaluating Generalization in Reading