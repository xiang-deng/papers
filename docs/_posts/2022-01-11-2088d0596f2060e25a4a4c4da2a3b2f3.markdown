--- 
layout: post 
title: "SPT-Code: Sequence-to-Sequence Pre-Training for Learning the Representation of Source Code" 
date: 2022-01-11 11:24:28 -0400 
categories: jekyll update 
author: "C Niu, C Li, V Ng, J Ge, L Huang, B Luo - arXiv preprint arXiv:2201.01549, 2022" 
--- 
Recent years have seen the successful application of large pre-trained models to code representation learning, resulting in substantial improvements on many code- related downstream tasks. But there are issues surrounding their application to SE tasks. First, the majority of the pre-trained models focus on pre-training only the encoder of the Transformer. For generation tasks that are addressed using models with the encoder-decoder architecture, however, there is no reason why the decoder Cites: Graphcodebert: Pre-training code representations with data flow