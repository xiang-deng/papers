---
layout: post
title:  "High-throughput Generative Inference of Large Language Models with a Single GPU"
date:   2023-03-16 06:48:33 -0400
categories: jekyll update
author: "Y Sheng, L Zheng, B Yuan, Z Li, M Ryabinin, DY Fu… - arXiv preprint arXiv …, 2023"
---
The high computational and memory requirements of large language model (LLM) inference traditionally make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen …
Cites: ‪Don t give me the details, just the summary! topic-aware …‬