---
layout: post
title:  "Zero-Shot Dynamic Quantization for Transformer Inference"
date:   2022-11-22 02:23:19 -0400
categories: jekyll update
author: "Y El-Kurdi, J Quinn, A Sil - arXiv preprint arXiv:2211.09744, 2022"
---
We introduce a novel run-time method for significantly reducing the accuracy loss associated with quantizing BERT-like models to 8-bit integers. Existing methods for quantizing models either modify the training procedure, or they require an additional calibration step to adjust parameters that also requires a selected held-out dataset. Our method permits taking advantage of quantization without the need for these adjustments. We present results on several NLP tasks demonstrating the usefulness …
Cites: ‪Natural questions: a benchmark for question answering research‬