---
layout: post
title:  "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta Optimizers"
date:   2022-12-23 23:45:02 -0400
categories: jekyll update
author: "D Dai, Y Sun, L Dong, Y Hao, Z Sui, F Wei - arXiv preprint arXiv:2212.10559, 2022"
---
Large pretrained language models have shown surprising In-Context Learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without additional parameter updates. Despite the great success in performance, the working mechanism of ICL still remains an open problem. In order to better understand how ICL works, this paper explains language models as meta optimizers and understands ICL as a kind of implicit finetuning. Theoretically, we …
Cites: ‪What learning algorithm is in-context learning? Investigations with …‬