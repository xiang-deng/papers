--- 
layout: post 
title: "LERT: A Linguistically-motivated Pre-trained Language Model" 
date: 2022-11-15 00:38:37 -0400 
categories: jekyll update 
author: "Y Cui, W Che, S Wang, T Liu - arXiv preprint arXiv:2211.05344, 2022" 
--- 
Pre-trained Language Model (PLM) has become a representative foundation model in the natural language processing field. Most PLMs are trained with linguistic-agnostic pre-training tasks on the surface form of the text, such as the masked language model (MLM). To further empower the PLMs with richer linguistic features, in this paper, we aim to propose a simple but effective way to learn linguistic features for pre-trained language models. We propose LERT, a pre-trained language model  Cites: Linguistic Knowledge and Transferability of Contextual