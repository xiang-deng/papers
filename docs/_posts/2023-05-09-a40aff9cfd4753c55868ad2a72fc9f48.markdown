---
layout: post
title:  "Heuristic Masking for Text Representation Pretraining"
date:   2023-05-09 11:33:00 -0400
categories: jekyll update
author: "Y Zhuang - ICASSP 2023-2023 IEEE International Conference on …, 2023"
---
Masked language model pretraining provides a standardized way to learn contextualized semantic representations, which reconstructs corrupted text sequences by estimating the conditional probabilities of randomly masked tokens given the context. We attempt to exploit language knowledge from the model itself to boost its pretraining in a lightweight and on-the-fly fashion. In this paper, a heuristic token masking scheme is studied, in which those tokens that deep networks and …
Cites: ‪Recent advances in natural language processing via large pre …‬