--- 
layout: post 
title: "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback" 
date: 2023-03-14 05:28:18 -0400 
categories: jekyll update 
author: "HR Kirk, B Vidgen, P Rttger, SA Hale - arXiv preprint arXiv:2303.05453, 2023" 
--- 
Large language models (LLMs) are used to generate content for a wide range of tasks, and are set to reach a growing audience in coming years due to integration in product interfaces like ChatGPT or search engines like Bing. This intensifies the need to ensure that models are aligned with human preferences and do not produce unsafe, inaccurate or toxic outputs. While alignment techniques like reinforcement learning with human feedback (RLHF) and red-teaming can mitigate some safety Cites: Dynabench: Rethinking benchmarking in NLP