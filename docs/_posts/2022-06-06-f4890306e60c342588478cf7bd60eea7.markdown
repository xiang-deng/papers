---
layout: post
title:  "An Empirical Study on the Fairness of Pre-trained Word Embeddings"
date:   2022-06-06 21:51:57 -0400
categories: jekyll update
author: "E Sesari, M Hort, F Sarro - 2022"
---
Pre-trained word embedding models are easily distributed and applied, as they alleviate users from the effort to train models themselves. With widely distributed models, it is important to ensure that they do not exhibit undesired behaviour, such as biases against population groups. For this purpose, we carry out an empirical study on evaluating the bias of 15 publicly available, pre-trained word embeddings model based on three training algorithms (GloVe, word2vec, and fastText) with 
Cites: Learning sentiment-specific word embedding for twitter sentiment