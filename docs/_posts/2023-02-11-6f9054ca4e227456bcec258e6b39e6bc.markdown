--- 
layout: post 
title: "Exploring the Benefits of Training Expert Language Models over Instruction Tuning" 
date: 2023-02-11 02:41:58 -0400 
categories: jekyll update 
author: "J Jang, S Kim, S Ye, D Kim, L Logeswaran, M Lee - arXiv preprint arXiv , 2023" 
--- 
Recently, Language Models (LMs) instruction-tuned on multiple tasks, also known as multitask-prompted fine-tuning (MT), have shown the capability to generalize to unseen tasks. Previous work has shown that scaling the number of training tasks is the key component in making stronger MT LMs. In this work, we report an unexpected finding that an expert LM fine-tuned on just a single task can outperform an MT LM trained with 300+ different tasks on 11 different unseen datasets and on Cites: Super-naturalinstructions: Generalization via declarative