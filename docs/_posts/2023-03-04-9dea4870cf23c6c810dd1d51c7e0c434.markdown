---
layout: post
title:  "AccelTran: A Sparsity-Aware Accelerator for Dynamic Inference with Transformers"
date:   2023-03-04 02:48:03 -0400
categories: jekyll update
author: "S Tuli, NK Jha - arXiv preprint arXiv:2302.14705, 2023"
---
Self-attention-based transformer models have achieved tremendous success in the domain of natural language processing. Despite their efficacy, accelerating the transformer is challenging due to its quadratic computational complexity and large activation sizes. Existing transformer accelerators attempt to prune its tokens to reduce memory access, albeit with high compute overheads. Moreover, previous works directly operate on large matrices involved in the attention operation, which …
Cites: ‪Well-read students learn better: The impact of student initialization …‬