--- 
layout: post 
title: "Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models" 
date: 2022-10-22 02:20:44 -0400 
categories: jekyll update 
author: "Z Zhang, L Lyu, X Ma, C Wang, X Sun - arXiv preprint arXiv:2210.09545, 2022" 
--- 
Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks. In Natural Language Processing (NLP), DNNs are often backdoored during the fine-tuning process of a large-scale Pre-trained Language Model (PLM) with poisoned samples. Although the clean weights of PLMs are readily available, existing methods have ignored this information in defending NLP models against backdoor attacks. In this work, we take the first step to exploit the pre-trained (unfine-tuned) weights to Cites: Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic