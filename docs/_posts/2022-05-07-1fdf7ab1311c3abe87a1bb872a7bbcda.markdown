--- 
layout: post 
title: "MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning" 
date: 2022-05-07 02:52:45 -0400 
categories: jekyll update 
author: "E Karpas, O Abend, Y Belinkov, B Lenz, O Lieber - arXiv preprint arXiv , 2022" 
--- 
Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural Cites: Bert: Pre-training of deep bidirectional transformers for language