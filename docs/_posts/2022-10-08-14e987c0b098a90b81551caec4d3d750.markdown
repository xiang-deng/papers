---
layout: post
title:  "Memory in humans and deep language models: Linking hypotheses for model augmentation"
date:   2022-10-08 00:45:41 -0400
categories: jekyll update
author: "O Raccah, P Chen, TL Willke, D Poeppel, VA Vo - arXiv preprint arXiv:2210.01869, 2022"
---
The computational complexity of the self-attention mechanism in Transformer models significantly limits their ability to generalize over long temporal durations. Memory-augmentation, or the explicit storing of past information in external memory for subsequent predictions, has become a constructive avenue for mitigating this limitation. We argue that memory-augmented Transformers can benefit substantially from considering insights from the memory literature in humans. We detail an …
Cites: ‪Retrieval-augmented generation for knowledge-intensive NLP tasks‬