--- 
layout: post 
title: "Theoretical Characterization of How Neural Network Pruning Affects its Generalization" 
date: 2023-01-04 14:44:31 -0400 
categories: jekyll update 
author: "H Yang, Y Liang, X Guo, L Wu, Z Wang - arXiv preprint arXiv:2301.00335, 2023" 
--- 
It has been observed in practice that applying pruning-at-initialization methods to neural networks and training the sparsified networks can not only retain the testing performance of the original dense models, but also sometimes even slightly boost the generalization performance. Theoretical understanding for such experimental observations are yet to be developed. This work makes the first attempt to study how different pruning fractions affect the model s gradient descent dynamics and Cites: Good subnetworks provably exist: Pruning via greedy forward