---
layout: post
title:  "Learning Language Representations with Logical Inductive Bias"
date:   2023-02-23 04:09:00 -0400
categories: jekyll update
author: "J Chen - arXiv preprint arXiv:2302.09458, 2023"
---
Transformer architectures have achieved great success in solving natural language tasks, which learn strong language representations from large-scale unlabeled texts. In this paper, we seek to go further beyond and explore a new logical inductive bias for better language representation learning. Logic reasoning is known as a formal methodology to reach answers from given knowledge and facts. Inspired by such a view, we develop a novel neural architecture named FOLNet (First-Order Logic …
Cites: ‪Unifying language learning paradigms‬