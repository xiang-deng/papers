--- 
layout: post 
title: "LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction" 
date: 2023-06-02 15:36:55 -0400 
categories: jekyll update 
author: "J Milbauer, A Louis, MJ Hosseini, A Fabrikant - arXiv preprint arXiv , 2023" 
--- 
Transformer encoders contextualize token representations by attending to all other tokens at each layer, leading to quadratic increase in compute effort with the input length. In practice, however, the input text of many NLP tasks can be seen as a sequence of related segments (eg, the sequence of sentences within a passage, or the hypothesis and premise in NLI). While attending across these segments is highly beneficial for many tasks, we hypothesize that this interaction can be delayed until Cites: ContractNLI: A dataset for document-level natural language