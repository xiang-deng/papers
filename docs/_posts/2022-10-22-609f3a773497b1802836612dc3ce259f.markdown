--- 
layout: post 
title: "Soft-Labeled Contrastive Pre-training for Function-level Code Representation" 
date: 2022-10-22 02:20:44 -0400 
categories: jekyll update 
author: "X Li, D Guo, Y Gong, Y Lin, Y Shen, X Qiu, D Jiang - arXiv preprint arXiv , 2022" 
--- 
Code contrastive pre-training has recently achieved significant progress on code-related tasks. In this paper, we present\textbf {SCodeR}, a\textbf {S} oft-labeled contrastive pre-training framework with two positive sample construction methods to learn functional-level\textbf {Code}\textbf {R} epresentation. Considering the relevance between codes in a large-scale code corpus, the soft-labeled contrastive pre-training can obtain fine-grained soft-labels through an iterative adversarial  Cites: Latent retrieval for weakly supervised open domain question