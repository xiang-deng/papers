---
layout: post
title:  "Training Recipe for N: M Structured Sparsity with Decaying Pruning Mask"
date:   2022-09-24 00:16:11 -0400
categories: jekyll update
author: "SC Kao, A Yazdanbakhsh, S Subramanian, S Agrawal… - arXiv preprint arXiv …, 2022"
---
Sparsity has become one of the promising methods to compress and accelerate Deep Neural Networks (DNNs). Among different categories of sparsity, structured sparsity has gained more attention due to its efficient execution on modern accelerators. Particularly, N: M sparsity is attractive because there are already hardware accelerator architectures that can leverage certain forms of N: M structured sparsity to yield higher compute-efficiency. In this work, we focus on N: M sparsity …
Cites: ‪Mobilebert: a compact task-agnostic bert for resource-limited devices‬