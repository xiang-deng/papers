---
layout: post
title:  "Using Focal Loss to Fight Shallow Heuristics: An Empirical Analysis of Modulated Cross-Entropy in Natural Language Inference"
date:   2022-12-01 07:00:03 -0400
categories: jekyll update
author: "F Rajič, I Stresec, A Marmet, T Poštuvan - arXiv preprint arXiv:2211.13331, 2022"
---
There is no such thing as a perfect dataset. In some datasets, deep neural networks discover underlying heuristics that allow them to take shortcuts in the learning process, resulting in poor generalization capability. Instead of using standard cross-entropy, we explore whether a modulated version of cross-entropy called focal loss can constrain the model so as not to use heuristics and improve generalization performance. Our experiments in natural language inference show that focal loss has …
Cites: ‪Behavior analysis of NLI models: Uncovering the influence of three …‬