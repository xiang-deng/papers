---
layout: post
title:  "Layer-wise Shared Attention Network on Dynamical System Perspective"
date:   2022-11-03 01:42:13 -0400
categories: jekyll update
author: "Z Huang, S Liang, M Liang, W He, L Lin - arXiv preprint arXiv:2210.16101, 2022"
---
Attention networks have successfully boosted accuracy in various vision problems. Previous works lay emphasis on designing a new self-attention module and follow the traditional paradigm that individually plugs the modules into each layer of a network. However, such a paradigm inevitably increases the extra parameter cost with the growth of the number of layers. From the dynamical system perspective of the residual neural network, we find that the feature maps from the layers of the same …
Cites: ‪Long short-term memory-networks for machine reading‬