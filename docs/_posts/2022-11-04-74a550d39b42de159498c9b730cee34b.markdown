--- 
layout: post 
title: "VarMAE: Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding" 
date: 2022-11-04 15:58:33 -0400 
categories: jekyll update 
author: "D Hu, X Hou, X Du, M Zhou, L Jiang, Y Mo, X Shi - arXiv preprint arXiv:2211.00430, 2022" 
--- 
Pre-trained language models have achieved promising performance on general benchmarks, but underperform when migrated to a specific domain. Recent works perform pre-training from scratch or continual pre-training on domain corpora. However, in many specific domains, the limited corpus can hardly support obtaining precise representations. To address this issue, we propose a novel Transformer-based language model named VarMAE for domain-adaptive language Cites: Don t stop pretraining: adapt language models to domains and tasks