---
layout: post
title:  "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training"
date:   2023-05-06 06:19:24 -0400
categories: jekyll update
author: "N Calderon, S Mukherjee, R Reichart, A Kantor - arXiv preprint arXiv:2305.02031, 2023"
---
Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to …
Cites: ‪Genie: A leaderboard for human-in-the-loop evaluation of text …‬