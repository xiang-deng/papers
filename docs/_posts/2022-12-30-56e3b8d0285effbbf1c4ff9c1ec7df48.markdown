--- 
layout: post 
title: "Length-Based Curriculum Learning for Efficient Pre-training of Language Models" 
date: 2022-12-30 14:30:10 -0400 
categories: jekyll update 
author: "K Nagatsuka, C Broni-Bediako, M Atsumi - New Generation Computing, 2022" 
--- 
Recently, pre-trained language models (PLMs) have become core components in a wide range of natural language processing applications. However, PLMs like BERT and RoBERTa are typically trained with a large amount of unlabeled text corpora which requires extremely high computational cost. Curriculum learning (CL) is a learning strategy for training a model from easy samples to hard ones that has potential to alleviate this problem. Nevertheless, how to determine the difficulty Cites: Deep contextualized word representations