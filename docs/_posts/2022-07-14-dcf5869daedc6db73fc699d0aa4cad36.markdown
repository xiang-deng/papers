---
layout: post
title:  "Controlling Conditional Language Models without Catastrophic Forgetting"
date:   2022-07-14 01:37:31 -0400
categories: jekyll update
author: "T Korbak, H Elsahar, G Kruszewski, M Dymetman - International Conference on …, 2022"
---
Abstract Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (eg, hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained …
Cites: ‪Multi-Reward Reinforced Summarization with Saliency and …‬  