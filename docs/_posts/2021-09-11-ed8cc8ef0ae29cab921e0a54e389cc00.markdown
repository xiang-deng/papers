--- 
layout: post 
title: "Interpretable Deep Learning: Beyond Feature-Importance with Concept-based Explanations" 
date: 2021-09-11 11:24:16 -0400 
categories: jekyll update 
author: "B Dimanov - 2021" 
--- 
Deep Neural Network (DNN) models are challenging to interpret because of their highly complex and non-linear nature. This lack of interpretability (1) inhibits adoption within safety critical applications,(2) makes it challenging to debug existing models, and (3) prevents us from extracting valuable knowledge. Explainable AI (XAI) research aims to increase the transparency of DNN model behaviour to improve interpretability. Feature importance explanations are the most popular Cites: Concept bottleneck models