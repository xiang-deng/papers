---
layout: post
title:  "Team  UFAL at CMCL 2022 Shared Task: Figuring out the correct recipe for predicting Eye-Tracking features using Pretrained Language Models"
date:   2022-04-16 01:25:48 -0400
categories: jekyll update
author: "S Bhattacharya, R Kumar, O Bojar - arXiv preprint arXiv:2204.04998, 2022"
---
Eye-Tracking data is a very useful source of information to study cognition and especially language comprehension in humans. In this paper, we describe our systems for the CMCL 2022 shared task on predicting eye-tracking information. We describe our experiments with pretrained models like BERT and XLM and the different ways in which we used those representations to predict four eye-tracking features. Along with analysing the effect of using two different kinds of pretrained Cites: What does BERT look at? An analysis of BERT s attention