--- 
layout: post 
title: "Elixir: Train a Large Language Model on a Small GPU Cluster" 
date: 2022-12-14 16:04:21 -0400 
categories: jekyll update 
author: "H Huang, J Fang, H Liu, S Li, Y You - arXiv preprint arXiv:2212.05339, 2022" 
--- 
In recent years, the number of parameters of one deep learning (DL) model has been growing much faster than the growth of GPU memory space. People who are inaccessible to a large number of GPUs resort to heterogeneous training systems for storing model parameters in CPU memory. Existing heterogeneous systems are based on parallelization plans in the scope of the whole model. They apply a consistent parallel training method for all the operators in the computation. Therefore  Cites: Opt: Open pre-trained transformer language models