---
layout: post
title:  "Unsupervised Context Aware Sentence Representation Pretraining for Multi-lingual Dense Retrieval"
date:   2022-06-10 22:27:43 -0400
categories: jekyll update
author: "N Wu, Y Liang, H Ren, L Shou, N Duan, M Gong… - arXiv preprint arXiv …, 2022"
---
Recent research demonstrates the effectiveness of using pretrained language models (PLM) to improve dense retrieval and multilingual dense retrieval. In this work, we present a simple but effective monolingual pretraining task called contrastive context prediction~(CCP) to learn sentence representation by modeling sentence level contextual relation. By pushing the embedding of sentences in a local context closer and pushing random negative samples away, different languages …
Cites: ‪XOR QA: Cross-lingual open-retrieval question answering‬  