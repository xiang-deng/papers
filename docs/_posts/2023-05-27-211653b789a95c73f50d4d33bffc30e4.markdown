---
layout: post
title:  "Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator"
date:   2023-05-27 10:00:59 -0400
categories: jekyll update
author: "Z He, M Yang, M Feng, J Yin, X Wang, J Leng, Z Lin - arXiv preprint arXiv:2305.15099, 2023"
---
The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformer s inefficiency has been …
Cites: ‪An Efficient Memory-Augmented Transformer for Knowledge …‬