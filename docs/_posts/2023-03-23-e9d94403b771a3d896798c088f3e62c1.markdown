--- 
layout: post 
title: "Conceptual Attention in StyleGAN" 
date: 2023-03-23 03:27:25 -0400 
categories: jekyll update 
author: "K Suekane, S Haji, H Sano, T Takagi - 2023 IEEE 17th International Conference on , 2023" 
--- 
Attention is an important function in human cognition. Since the human brain can only process a limited amount of information at a time, attention selects and focuses on just the important information among all the perceived information. Recent years have seen the advent of methods that imitate the human attention mechanism by means of deep learning, one of which is the multi-head attention proposed for Transformer. Humans can consider a single object from multiple perspectives, so Cites: Palm: Scaling language modeling with pathways