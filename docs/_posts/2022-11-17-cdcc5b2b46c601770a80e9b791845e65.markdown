---
layout: post
title:  "Metaphors We Learn By"
date:   2022-11-17 00:57:01 -0400
categories: jekyll update
author: "R Memisevic - arXiv preprint arXiv:2211.06441, 2022"
---
Gradient based learning using error back-propagation (``backprop  ) is a well-known contributor to much of the recent progress in AI. A less obvious, but arguably equally important, ingredient is parameter sharing-most well-known in the context of convolutional networks. In this essay we relate parameter sharing (``weight sharing  ) to analogy making and the school of thought of cognitive metaphor. We discuss how recurrent and auto-regressive models can be thought of as extending analogy …
Cites: ‪Palm: Scaling language modeling with pathways‬