--- 
layout: post 
title: "Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence" 
date: 2022-05-14 04:38:21 -0400 
categories: jekyll update 
author: "M Jang, F Mtumbuka, T Lukasiewicz - arXiv preprint arXiv:2205.03815, 2022" 
--- 
The logical negation property (LNP), which implies generating different predictions for semantically opposite inputs, is an important property that a trustworthy language model must satisfy. However, much recent evidence shows that large-size pre- trained language models (PLMs) do not satisfy this property. In this paper, we perform experiments using probing tasks to assess PLM s LNP understanding. Unlike previous studies that only examined negation expressions, we expand the Cites: Neural text generation with unlikelihood training