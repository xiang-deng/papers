--- 
layout: post 
title: "You Are My Type! Type Embeddings for Pre-trained Language Models" 
date: 2022-11-03 01:42:13 -0400 
categories: jekyll update 
author: "M SAEED, P PAPOTTI" 
--- 
One reason for the positive impact of Pretrained Language Models (PLMs) in NLP tasks is their ability to encode semantic types, such as European City or Woman . While previous work has analyzed such information in the context of interpretability, it is not clear how to use types to steer the PLM output. For example, in a cloze statement, it is desirable to steer the model to generate a token that satisfies a user-specified type, eg, predict a date rather than a location. In this work, we introduce Cites: Do NLP Models Know Numbers? Probing Numeracy in Embeddings