--- 
layout: post 
title: "The CRINGE Loss: Learning what language not to model" 
date: 2022-11-17 00:57:01 -0400 
categories: jekyll update 
author: "L Adolphs, T Gao, J Xu, K Shuster, S Sukhbaatar - arXiv preprint arXiv , 2022" 
--- 
Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data--examples of what the model should not do. In this work, we propose a novel procedure to train with such data called the CRINGE loss (ContRastive Iterative  Cites: Gedi: Generative discriminator guided sequence generation