--- 
layout: post 
title: "What Makes Pre-trained Language Models Better Zero/Few-shot Learners?" 
date: 2022-10-06 01:25:19 -0400 
categories: jekyll update 
author: "J Lu, R Zhao, B Mac Namee, D Zhu, W Han, F Tan - arXiv preprint arXiv:2209.15206, 2022" 
--- 
In this paper, we propose a theoretical framework to explain the efficacy of prompt learning in zero/few-shot scenarios. First, we prove that conventional pre-training and fine-tuning paradigm fails in few-shot scenarios due to overfitting the unrepresentative labelled data. We then detail the assumption that prompt learning is more effective because it empowers pre-trained language model that is built upon massive text corpora, as well as domain-related human knowledge to participate  Cites: Unifiedqa: Crossing format boundaries with a single qa system