--- 
layout: post 
title: "Saisiyat Is Where It Is At! Insights Into Backdoors And Debiasing Of Cross Lingual Transformers For Named Entity Recognition" 
date: 2023-01-31 01:49:57 -0400 
categories: jekyll update 
author: "RA Calix, J Ben-Joseph, N Lopatina, R Ashley - 2022 IEEE International , 2022" 
--- 
Deep learning and, in particular, Transformer-based models are revolutionizing natural language processing (NLP). As a result, NLP models can now be pre-trained and fine-tuned by anyone with sufficient resources, and subsequently shared with the world at large. This is an unprecedented approach that helps level the AI playing field and improves productivity. However, this new AI sharing approach presents novel and largely unaddressed challenges involving bias and backdoors. This study  Cites: Effects of parameter norm growth during transformer training