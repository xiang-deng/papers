--- 
layout: post 
title: "Task-Specific Embeddings for Ante-Hoc Explainable Text Classification" 
date: 2022-12-06 02:51:26 -0400 
categories: jekyll update 
author: "K Halder, J Krapac, A Akbik, A Brew, M Lyra - arXiv preprint arXiv:2212.00086, 2022" 
--- 
Current state-of-the-art approaches to text classification typically leverage BERT-style Transformer models with a softmax classifier, jointly fine-tuned to predict class labels of a target task. In this paper, we instead propose an alternative training objective in which we learn task-specific embeddings of text: our proposed objective learns embeddings such that all texts that share the same target class label should be close together in the embedding space, while all others should be far apart. This Cites: Interpreting neural networks with nearest neighbors