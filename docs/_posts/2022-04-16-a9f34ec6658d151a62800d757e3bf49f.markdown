--- 
layout: post 
title: "What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured" 
date: 2022-04-16 01:25:48 -0400 
categories: jekyll update 
author: "A Henlein, A Mehler - arXiv preprint arXiv:2204.05673, 2022" 
--- 
Transformer-based models are now predominant in NLP. They outperform approaches based on static models in many respects. This success has in turn prompted research that reveals a number of biases in the language models generated by transformers. In this paper we utilize this research on biases to investigate to what extent transformer-based language models allow for extracting knowledge about object relations (X occurs in Y; X consists of Z; action A involves Cites: How Context Affects Language Models Factual Predictions