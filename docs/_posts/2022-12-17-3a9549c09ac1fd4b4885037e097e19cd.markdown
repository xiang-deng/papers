--- 
layout: post 
title: "Pre-trained Language Models can be Fully Zero-Shot Learners" 
date: 2022-12-17 01:50:56 -0400 
categories: jekyll update 
author: "X Zhao, S Ouyang, Z Yu, M Wu, L Li - arXiv preprint arXiv:2212.06950, 2022" 
--- 
How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods  Cites: Making Pre-trained Language Models Better Few-shot Learners