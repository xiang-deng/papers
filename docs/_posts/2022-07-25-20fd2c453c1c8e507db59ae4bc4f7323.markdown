--- 
layout: post 
title: "Revealing Secrets From Pre-trained Models" 
date: 2022-07-25 21:58:40 -0400 
categories: jekyll update 
author: "MA Rafi, Y Feng, H Jeon - arXiv preprint arXiv:2207.09539, 2022" 
--- 
With the growing burden of training deep learning models with large data sets, transfer-learning has been widely adopted in many emerging deep learning algorithms. Transformer models such as BERT are the main player in natural language processing and use transfer-learning as a de facto standard training method. A few big data companies release pre-trained models that are trained with a few popular datasets with which end users and researchers fine-tune the model with  Cites: Thieves on Sesame Street! Model Extraction of BERT-based APIs