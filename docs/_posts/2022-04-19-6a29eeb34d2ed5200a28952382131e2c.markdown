---
layout: post
title:  "Multilingual Language Model Adaptive Fine-Tuning: A Study on African Languages"
date:   2022-04-19 07:59:02 -0400
categories: jekyll update
author: "JO Alabi, DI Adelani, M Mosbach, D Klakow - arXiv preprint arXiv:2204.06487, 2022"
---
Multilingual pre-trained language models (PLMs) have demonstrated impressive performance on several downstream tasks on both high resourced and low- resourced languages. However, there is still a large performance drop for languages unseen during pre-training, especially African languages. One of the most effective approaches to adapt to a new language is language adaptive fine-tuning (LAFT)-- fine-tuning a multilingual PLM on monolingual texts of a language using the same Cites: Mobilebert: a compact task-agnostic bert for resource-limited devices