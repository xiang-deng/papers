--- 
layout: post 
title: "Directional Privacy for Deep Learning" 
date: 2022-11-11 23:39:32 -0400 
categories: jekyll update 
author: "P Faustini, N Fernandes, A McIver, M Dras - arXiv preprint arXiv:2211.04686, 2022" 
--- 
Differentially Private Stochastic Gradient Descent (DP-SGD) is a key method for applying privacy in the training of deep learning models. This applies isotropic Gaussian noise to gradients during training, which can perturb these gradients in any direction, damaging utility. Metric DP, however, can provide alternative mechanisms based on arbitrary metrics that might be more suitable. In this paper we apply\textit {directional privacy}, via a mechanism based on the von Mises-Fisher (VMF) Cites: Large language models can be strong differentially private learners