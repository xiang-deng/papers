---
layout: post
title:  "LMPriors: Pre-Trained Language Models as Task-Specific Priors"
date:   2022-10-26 13:20:27 -0400
categories: jekyll update
author: "K Choi, C Cundy, S Srivastava, S Ermon - arXiv preprint arXiv:2210.12530, 2022"
---
Particularly in low-data regimes, an outstanding challenge in machine learning is developing principled techniques for augmenting our models with suitable priors. This is to encourage them to learn in ways that are compatible with our understanding of the world. But in contrast to generic priors such as shrinkage or sparsity, we draw inspiration from the recent successes of large-scale language models (LMs) to construct task-specific priors distilled from the rich knowledge of …
Cites: ‪On the opportunities and risks of foundation models‬