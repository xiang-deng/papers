--- 
layout: post 
title: "Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation" 
date: 2023-05-11 03:26:59 -0400 
categories: jekyll update 
author: "TC Chi, TH Fan, AI Rudnicky, PJ Ramadge - arXiv preprint arXiv:2305.03796, 2023" 
--- 
Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test Cites: Train short, test long: Attention with linear biases enables input