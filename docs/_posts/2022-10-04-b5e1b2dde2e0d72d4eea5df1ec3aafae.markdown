--- 
layout: post 
title: "General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings" 
date: 2022-10-04 00:49:37 -0400 
categories: jekyll update 
author: "L Galke, I Cuber, C Meyer, HF Nlscher - 2022 International Joint , 2022" 
--- 
Large pretrained language models (PreLMs) are rev-olutionizing natural language processing across all benchmarks. However, their sheer size is prohibitive for small laboratories or for deployment on mobile devices. Approaches like pruning and distillation reduce the model size but typically retain the same model architecture. In contrast, we explore distilling PreLMs into a different, more efficient architecture, Continual Multiplication of Words (CMOW), which embeds each word as a matrix and Cites: Well-read students learn better: The impact of student initialization