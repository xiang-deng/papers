--- 
layout: post 
title: "R^ 2: Range Regularization for Model Compression and Quantization" 
date: 2023-03-18 01:48:35 -0400 
categories: jekyll update 
author: "A Kundu, C Yoo, S Mishra, M Cho, S Adya - arXiv preprint arXiv:2303.08253, 2023" 
--- 
Model parameter regularization is a widely used technique to improve generalization, but also can be used to shape the weight distributions for various purposes. In this work, we shed light on how weight regularization can assist model quantization and compression techniques, and then propose range regularization (R^ 2) to further boost the quality of model optimization by focusing on the outlier prevention. By effectively regulating the minimum and maximum weight values from Cites: Mobilebert: a compact task-agnostic bert for resource-limited devices