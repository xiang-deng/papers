--- 
layout: post 
title: "A Scalable and Modular Framework for Training Provably Robust Large Transformer Models via Neural Network Duality" 
date: 2023-04-14 18:18:10 -0400 
categories: jekyll update 
author: "AJ Wang, Y Li, W Cao, Y Yue" 
--- 
We propose a comprehensive modular framework for enhancing and verifying the robustness of deep neural networks against norm-bounded adversarial attacks by leveraging dualized network representations. We provide rigorous mathematical proof for the derivation of dual problem and provide dual layer formulation for common network layers such as linear, residual, ReLU and attention layers. We calculate dual layer upper bounds for various layers in Transformer models such as Cites: Hard-Coded Gaussian Attention for Neural Machine Translation