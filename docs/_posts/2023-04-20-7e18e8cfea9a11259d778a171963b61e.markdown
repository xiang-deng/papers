--- 
layout: post 
title: "Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales" 
date: 2023-04-20 07:45:04 -0400 
categories: jekyll update 
author: "Y Yao, Y Wang - arXiv preprint arXiv:2304.06875, 2023" 
--- 
As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue Cites: Unifying language learning paradigms