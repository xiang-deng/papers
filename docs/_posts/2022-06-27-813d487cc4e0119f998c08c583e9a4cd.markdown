---
layout: post
title:  "Novel multi-domain attention for abstractive summarisation"
date:   2022-06-27 23:23:24 -0400
categories: jekyll update
author: "C Qu, L Lu, A Wang, W Yang, Y Chen - CAAI Transactions on Intelligence , 2022"
---
The existing abstractive text summarisation models only consider the word sequence correlations between the source document and the reference summary, and the summary generated by models lacks the cover of the subject of source document due to models  small perspective. In order to make up these disadvantages, a multi-domain attention pointer (MDA-Pointer) abstractive summarisation model is proposed in this work. First, the model uses bidirectional long short-term memory to 
Cites: Learning phrase representations using RNN encoder-decoder for