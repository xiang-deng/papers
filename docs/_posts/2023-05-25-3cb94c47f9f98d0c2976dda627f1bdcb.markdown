--- 
layout: post 
title: "Lifting the Curse of Capacity Gap in Distilling Language Models" 
date: 2023-05-25 03:51:47 -0400 
categories: jekyll update 
author: "C Zhang, Y Yang, J Liu, J Wang, Y Xian, B Wang - arXiv preprint arXiv , 2023" 
--- 
Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is  Cites: Length-adaptive transformer: Train once with length drop, use