--- 
layout: post 
title: "Language Models of Code are Few-Shot Commonsense Learners" 
date: 2022-10-18 02:49:27 -0400 
categories: jekyll update 
author: "A Madaan, S Zhou, U Alon, Y Yang, G Neubig - arXiv preprint arXiv:2210.07128, 2022" 
--- 
We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event--or a reasoning-graph. To employ large language models (LMs) for this task, existing approaches``serialize the output graph as a flat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show Cites: proscript: Partially ordered scripts generation via pre-trained