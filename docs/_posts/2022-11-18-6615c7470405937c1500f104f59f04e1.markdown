--- 
layout: post 
title: "Evaluating How Fine-tuning on Bimodal Data Effects Code Generation" 
date: 2022-11-18 16:55:42 -0400 
categories: jekyll update 
author: "G Orlanski, S Yang, M Healy - arXiv preprint arXiv:2211.07842, 2022" 
--- 
Despite the increase in popularity of language models for code generation, it is still unknown how training on bimodal coding forums affects a model s code generation performance and reliability. We, therefore, collect a dataset of over 2.2 M StackOverflow questions with answers for finetuning. These fine-tuned models have average $ pass@ k $ improvements of 54.64% and 85.35% on the HumanEval (Chen et al., 2021) and Mostly Basic Program Problems (Austin et al., 2021) tasks Cites: Incoder: A generative model for code infilling and synthesis