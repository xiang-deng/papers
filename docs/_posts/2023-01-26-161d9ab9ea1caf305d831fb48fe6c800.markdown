---
layout: post
title:  "Batch Prompting: Efficient Inference with Large Language Model APIs"
date:   2023-01-26 15:19:03 -0400
categories: jekyll update
author: "Z Cheng, J Kasai, T Yu - arXiv preprint arXiv:2301.08721, 2023"
---
Performing inference on hundreds of thousands of samples with large language models (LLMs) can be computationally and financially costly. We propose batch prompting, a simple alternative prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs …
Cites: ‪Chain of thought prompting elicits reasoning in large language …‬