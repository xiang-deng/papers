--- 
layout: post 
title: "Robustness of Explanation Methods for NLP Models" 
date: 2022-07-25 21:58:40 -0400 
categories: jekyll update 
author: "S Atmakuri, T Chheda, D Kandula, N Yadav, T Lee - arXiv preprint arXiv , 2022" 
--- 
Explanation methods have emerged as an important tool to highlight the features responsible for the predictions of neural networks. There is mounting evidence that many explanation methods are rather unreliable and susceptible to malicious manipulations. In this paper, we particularly aim to understand the robustness of explanation methods in the context of text modality. We provide initial insights and results towards devising a successful adversarial attack against text explanations. To  Cites: Do feature attribution methods correctly attribute features?