--- 
layout: post 
title: "Representation Deficiency in Masked Language Modeling" 
date: 2023-02-09 01:30:47 -0400 
categories: jekyll update 
author: "Y Meng, J Krishnan, S Wang, Q Wang, Y Mao, H Fang - arXiv preprint arXiv , 2023" 
--- 
Masked Language Modeling (MLM) has been one of the most prominent approaches for pretraining bidirectional text encoders due to its simplicity and effectiveness. One notable concern about MLM is that the special $\texttt {[MASK]} $ symbol causes a discrepancy between pretraining data and downstream data as it is present only in pretraining but not in fine-tuning. In this work, we offer a new perspective on the consequence of such a discrepancy: We demonstrate empirically and theoretically  Cites: Should you mask 15% in masked language modeling?