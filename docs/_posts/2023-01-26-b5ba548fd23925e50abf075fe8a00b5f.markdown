--- 
layout: post 
title: "AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation" 
date: 2023-01-26 15:19:03 -0400 
categories: jekyll update 
author: "M Deb, B Deiseroth, S Weinbach, P Schramowski - arXiv preprint arXiv , 2023" 
--- 
Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present  Cites: Why Should I Trust You? : Explaining the Predictions of Any