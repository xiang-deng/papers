--- 
layout: post 
title: "Ensemble Compressed Language Model Based on Knowledge Distillation and Multi-Task Learning" 
date: 2022-06-15 15:55:00 -0400 
categories: jekyll update 
author: "K Xiang, A Fujii - 2022 7th International Conference on Business and , 2022" 
--- 
The success of pre-trained language representation models such as BERT benefits from their overparameterized nature, resulting in training time consuming, high computational complexity and superior requirement of devices. Among the variety of model compression and acceleration techniques, Knowledge Distillation (KD) has attracted extensive attention for compressing pre-trained language models. However, the major two challenges for KD are:(i) Transfer more knowledge from the Cites: BAM! Born-Again Multi-Task Networks for Natural Language