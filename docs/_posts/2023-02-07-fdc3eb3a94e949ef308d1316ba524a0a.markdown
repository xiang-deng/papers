--- 
layout: post 
title: "Rethinking Positional Encoding in Tree Transformer for Code Representation" 
date: 2023-02-07 01:43:12 -0400 
categories: jekyll update 
author: "H Peng, G Li, Y Zhao, Z Jin - Proceedings of the 2022 Conference on Empirical , 2022" 
--- 
Transformers are now widely used in code representation, and several recent works further develop tree Transformers to capture the syntactic structure in source code. Specifically, novel tree positional encodings have been proposed to incorporate inductive bias into Transformer. In this work, we propose a novel tree Transformer encoding node positions based on our new description method for tree structures. Technically, local and global soft bias shown in previous works is both introduced as Cites: Tree-structured attention with hierarchical accumulation