---
layout: post
title:  "Coneheads: Hierarchy Aware Attention"
date:   2023-06-06 05:46:58 -0400
categories: jekyll update
author: "A Tseng, T Yu, TJB Liu, C De Sa - arXiv preprint arXiv:2306.00392, 2023"
---
Attention networks such as transformers have achieved state-of-the-art performance in many domains. These networks rely heavily on the dot product attention operator, which computes the similarity between two points by taking their inner product. However, the inner product does not explicitly model the complex structural properties of real world datasets, such as hierarchies between data points. To remedy this, we introduce cone attention, a drop-in replacement for dot product …
Cites: ‪Liangke Gui, Graham Neubig, Jonathan May, and Luke …‬