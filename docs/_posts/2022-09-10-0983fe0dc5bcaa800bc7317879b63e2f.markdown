--- 
layout: post 
title: " " 
date: 2022-09-10 00:05:49 -0400 
categories: jekyll update 
author: " , " 
--- 
The transformer architecture has led to breakthroughs in the field of NLP, achieving state of the art performance in various tasks and consistently improving in the computer vision domain as well. However, the sheer size of some of the latest models along with the standard technique of fine tuning the same model separately for each task make the ef fective use of such architectures very demanding in terms of both memory and computing power. In this work we propose a technique for  Cites: Adapterhub: A framework for adapting transformers