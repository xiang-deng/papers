---
layout: post
title:  "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers"
date:   2021-09-16 19:19:30 -0400
categories: jekyll update
author: "B Kim, HS Kim, SW Lee, G Lee, D Kwak, DH Jeon - arXiv preprint arXiv , 2021"
---
GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens Cites: Prefix-tuning: Optimizing continuous prompts for generation