---
layout: post
title:  "Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks"
date:   2022-04-08 14:57:15 -0400
categories: jekyll update
author: "T Thrush, K Tirumala, A Gupta, M Bartolo, P Rodriguez - arXiv preprint arXiv , 2022"
---
We introduce Dynatask: an open source system for setting up custom NLP tasks that aims to greatly lower the technical knowledge and effort required for hosting and evaluating state-of-the-art NLP models, as well as for conducting model in the loop data collection with crowdworkers. Dynatask is integrated with Dynabench, a research platform for rethinking benchmarking in AI that facilitates human and model in the loop data collection and evaluation. To create a task, users only need to write a Cites: Models in the Loop: Aiding Crowdworkers with Generative