--- 
layout: post 
title: "Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations" 
date: 2022-12-23 23:45:02 -0400 
categories: jekyll update 
author: "J Chen, Y Zhang, L Liu, R Dong, X Chen, P Ng - arXiv preprint arXiv , 2022" 
--- 
There has been great progress in unifying various table-to-text tasks using a single encoder-decoder model trained via multi-task learning (Xie et al., 2022). However, existing methods typically encode task information with a simple dataset name as a prefix to the encoder. This not only limits the effectiveness of multi-task learning, but also hinders the model s ability to generalize to new domains or tasks that were not seen during training, which is crucial for real-world applications. In this paper, we  Cites: TaBERT: Pretraining for joint understanding of textual and tabular