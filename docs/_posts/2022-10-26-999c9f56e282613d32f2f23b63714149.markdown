--- 
layout: post 
title: "Tail Batch Sampling: Approximating Global Contrastive Losses as Optimization over Batch Assignments" 
date: 2022-10-26 13:20:27 -0400 
categories: jekyll update 
author: "V Sachidananda, Z Yang, C Zhu - arXiv preprint arXiv:2210.12874, 2022" 
--- 
Contrastive Learning has recently achieved state-of-the-art performance in a wide range of tasks. Many contrastive learning approaches use mined hard negatives to make batches more informative during training but these approaches are inefficient as they increase epoch length proportional to the number of mined negatives and require frequent updates of nearest neighbor indices or mining from recent batches. In this work, we provide an alternative to hard negative mining in supervised Cites: Graphcodebert: Pre-training code representations with data flow