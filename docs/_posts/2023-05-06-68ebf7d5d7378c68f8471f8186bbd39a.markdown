--- 
layout: post 
title: "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes" 
date: 2023-05-06 06:19:24 -0400 
categories: jekyll update 
author: "CY Hsieh, CL Li, CK Yeh, H Nakhost, Y Fujii, A Ratner - arXiv preprint arXiv , 2023" 
--- 
Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that Cites: Evaluating Explanations: How much do explanations from the