--- 
layout: post 
title: "ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts" 
date: 2022-10-26 13:20:27 -0400 
categories: jekyll update 
author: "A Asai, M Salehi, ME Peters, H Hajishirzi" 
--- 
This work introduces a new multi-task, parameter-efficient language model (LM) tuning method that learns to transfer knowledge across different tasks via a mixture of soft promptssmall prefix embedding vectors pretrained for different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt Tuning), obtains source prompts as encodings of large-scale source tasks into a small number of parameters and trains an attention module to interpolate the source prompts and a newly  Cites: Single-dataset Experts for Multi-dataset Question Answering