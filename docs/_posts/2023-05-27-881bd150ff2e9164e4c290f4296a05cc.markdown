--- 
layout: post 
title: "Modeling rapid language learning by distilling Bayesian priors into artificial neural networks" 
date: 2023-05-27 10:00:59 -0400 
categories: jekyll update 
author: "RT McCoy, TL Griffiths - arXiv preprint arXiv:2305.14701, 2023" 
--- 
Humans can learn languages from remarkably little experience. Developing computational models that explain this ability has been a major challenge in cognitive science. Bayesian models that build in strong inductive biases-factors that guide generalization-have been successful at explaining how humans might generalize from few examples in controlled settings but are usually too restrictive to be tractably applied to more naturalistic data. By contrast, neural networks have Cites: Meta-Learning to Compositionally Generalize