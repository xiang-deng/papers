--- 
layout: post 
title: "Understanding Gradient Regularization in Deep Learning: Efficient Finite-Difference Computation and Implicit Bias" 
date: 2022-10-10 14:05:52 -0400 
categories: jekyll update 
author: "R Karakida, T Takase, T Hayase, K Osawa - arXiv preprint arXiv:2210.02720, 2022" 
--- 
Gradient regularization (GR) is a method that penalizes the gradient norm of the training loss during training. Although some studies have reported that GR improves generalization performance in deep learning, little attention has been paid to it from the algorithmic perspective, that is, the algorithms of GR that efficiently improve performance. In this study, we first reveal that a specific finite-difference computation, composed of both gradient ascent and descent steps, reduces the computational Cites: Catastrophic fisher explosion: Early phase fisher matrix impacts