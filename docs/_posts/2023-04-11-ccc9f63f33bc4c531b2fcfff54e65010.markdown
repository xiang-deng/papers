--- 
layout: post 
title: "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster" 
date: 2023-04-11 07:02:19 -0400 
categories: jekyll update 
author: "N Dey, G Gosal, H Khachane, W Marshall, R Pathria - arXiv preprint arXiv , 2023" 
--- 
We study recent research advances that improve large language models through efficient pre-training and scaling, and open datasets and tools. We combine these advances to introduce Cerebras-GPT, a family of open compute-optimal language models scaled from 111M to 13B parameters. We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget). We characterize the Cites: Train short, test long: Attention with linear biases enables input