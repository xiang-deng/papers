---
layout: post
title:  "Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich Document Understanding"
date:   2022-07-02 02:42:16 -0400
categories: jekyll update
author: "C Luo, G Tang, Q Zheng, C Yao, L Jin, C Li, Y Xue, L Si - arXiv preprint arXiv …, 2022"
---
Multi-modal document pre-trained models have proven to be very effective in a variety of visually-rich document understanding (VrDU) tasks. Though existing document pre-trained models have achieved excellent performance on standard benchmarks for VrDU, the way they model and exploit the interactions between vision and language on documents has hindered them from better generalization ability and higher accuracy. In this work, we investigate the problem of vision …
Cites: ‪Scaling up visual and vision-language representation learning with …‬  