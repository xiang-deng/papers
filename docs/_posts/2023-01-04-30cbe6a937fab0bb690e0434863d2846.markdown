--- 
layout: post 
title: "Hungry Hungry Hippos: Towards Language Modeling with State Space Models" 
date: 2023-01-04 14:44:31 -0400 
categories: jekyll update 
author: "T Dao, DY Fu, KK Saab, AW Thomas, A Rudra, C R - arXiv preprint arXiv , 2022" 
--- 
State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between  Cites: Opt: Open pre-trained transformer language models