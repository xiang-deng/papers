---
layout: post
title:  "BatteryBERT: A Pretrained Language Model for Battery Database Enhancement"
date:   2022-05-14 04:38:21 -0400
categories: jekyll update
author: "S Huang, JM Cole - Journal of Chemical Information and Modeling, 2022"
---
A great number of scientific papers are published every year in the field of battery research, which forms a huge textual data source. However, it is difficult to explore and retrieve useful information efficiently from these large unstructured sets of text. The Bidirectional Encoder Representations from Transformers (BERT) model, trained on a large data set in an unsupervised way, provides a route to process the scientific text automatically with minimal human effort. To this end, we realized six battery Cites: BERT: Pre-training of Deep Bidirectional Transformers for