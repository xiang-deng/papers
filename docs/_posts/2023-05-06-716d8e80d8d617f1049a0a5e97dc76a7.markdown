---
layout: post
title:  "Making the Most of What You Have: Adapting Pre-trained Visual Language Models in the Low-data Regime"
date:   2023-05-06 06:19:24 -0400
categories: jekyll update
author: "C Zhang, A Miech, J Shen, JB Alayrac, P Luc - arXiv preprint arXiv:2305.02297, 2023"
---
Large-scale visual language models are widely used as pre-trained models and then adapted for various downstream tasks. While humans are known to efficiently learn new tasks from a few examples, deep learning models struggle with adaptation from few examples. In this work, we look into task adaptation in the low-data regime, and provide a thorough study of the existing adaptation methods for generative Visual Language Models. And we show important benefits of self-labelling, ie using the …
Cites: ‪Merlot reserve: Neural script knowledge through vision and …‬