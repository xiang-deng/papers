--- 
layout: post 
title: "Impact of Morphological Segmentation on Pre-trained Language Models" 
date: 2022-11-22 02:23:19 -0400 
categories: jekyll update 
author: "M Westhelle, L Bencke, VP Moreira - Brazilian Conference on Intelligent Systems, 2022" 
--- 
Abstract Pre-trained Language Models are the current state-of-the-art in many natural language processing tasks. These models rely on subword-based tokenization to solve the problem of out-of-vocabulary words. However, commonly used subword segmentation methods have no linguistic foundation. In this paper, we investigate the hypothesis that the study of internal word structure (ie, morphology) can offer informed priors to these models, such that they perform better in common  Cites: Byte Pair Encoding is Suboptimal for Language Model Pretraining