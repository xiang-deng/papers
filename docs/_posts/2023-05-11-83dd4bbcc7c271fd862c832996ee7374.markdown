--- 
layout: post 
title: "Prompt-Distiller: Few-Shot Knowledge Distillation for Prompt-Based Language Learners with Dual Contrastive Learning" 
date: 2023-05-11 03:26:59 -0400 
categories: jekyll update 
author: "B Hou, C Wang, X Chen, M Qiu, L Feng, J Huang - ICASSP 2023-2023 IEEE , 2023" 
--- 
Prompt-based learning has improved the few-shot learning performance of large-scale Pre-trained Language Models (PLMs). Yet, it is challenging to deploy large-scale PLMs in resource-constrained environments for online applications. Knowledge Distillation (KD) is a promising approach for PLM compression. However, distilling prompt-tuned PLMs in the few-shot learning setting is a non-trivial problem due to the lack of task-specific training data and KD techniques for the new  Cites: Well-read students learn better: The impact of student initialization