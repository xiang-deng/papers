--- 
layout: post 
title: "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers" 
date: 2022-11-03 01:42:13 -0400 
categories: jekyll update 
author: "E Frantar, S Ashkboos, T Hoefler, D Alistarh - arXiv preprint arXiv:2210.17323, 2022" 
--- 
Generative Pre-trained Transformer (GPT) models set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs to execute, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the Cites: LLM. int8 (): 8-bit Matrix Multiplication for Transformers at Scale