--- 
layout: post 
title: "Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples" 
date: 2022-09-10 00:05:49 -0400 
categories: jekyll update 
author: "HJ Branch, JR Cefalu, J McHugh, L Hujer, A Bahl - arXiv preprint arXiv , 2022" 
--- 
Recent advances in the development of large language models have resulted in public access to state-of-the-art pre-trained language models (PLMs), including Generative Pre-trained Transformer 3 (GPT-3) and Bidirectional Encoder Representations from Transformers (BERT). However, evaluations of PLMs, in practice, have shown their susceptibility to adversarial attacks during the training and fine-tuning stages of development. Such attacks can result in erroneous outputs  Cites: Trick me if you can: Human-in-the-loop generation of adversarial