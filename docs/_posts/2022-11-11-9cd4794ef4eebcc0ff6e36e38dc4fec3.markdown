--- 
layout: post 
title: "Near-Negative Distinction: Giving a Second Life to Human Evaluation Datasets" 
date: 2022-11-11 23:39:32 -0400 
categories: jekyll update 
author: "PLCSW Wenhao, LC Xiong" 
--- 
Precisely assessing the progress in natural language generation (NLG) tasks is challenging, and human evaluation to establish a preference in a model s output over another is often necessary. However, human evaluation is usually costly, difficult to reproduce, and non-reusable. In this paper, we propose a new and simple automatic evaluation method for NLG called Near-Negative Distinction (NND) that repurposes prior human annotations into NND tests. In an NND test, an NLG model  Cites: Summeval: Re-evaluating summarization evaluation