--- 
layout: post 
title: "Modelling Commonsense Properties using Pre-Trained Bi-Encoders" 
date: 2022-10-10 14:05:52 -0400 
categories: jekyll update 
author: "A Gajbhiye, L Espinosa-Anke, S Schockaert - arXiv preprint arXiv:2210.02771, 2022" 
--- 
Grasping the commonsense properties of everyday concepts is an important prerequisite to language understanding. While contextualised language models are reportedly capable of predicting such commonsense properties with human-level accuracy, we argue that such results have been inflated because of the high similarity between training and test concepts. This means that models which capture concept similarity can perform well, even if they do not capture any knowledge of the Cites: Symbolic knowledge distillation: from general language models to