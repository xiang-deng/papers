--- 
layout: post 
title: "Decoupled Model Schedule for Deep Learning Training" 
date: 2023-02-20 23:17:05 -0400 
categories: jekyll update 
author: "H Chen, CH Yu, S Zheng, Z Zhang, Z Zhang, Y Wang - arXiv preprint arXiv , 2023" 
--- 
Recent years have seen an increase in the development of large deep learning (DL) models, which makes training efficiency crucial. Common practice is struggling with the trade-off between usability and performance. On one hand, DL frameworks such as PyTorch use dynamic graphs to facilitate model developers at a price of sub-optimal model training performance. On the other hand, practitioners propose various approaches to improving the training efficiency by sacrificing some of the  Cites: PaLM: Scaling language modeling with pathways