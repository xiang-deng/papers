--- 
layout: post 
title: "On the validity of pre-trained transformers for natural language processing in the software engineering domain" 
date: 2021-09-16 19:19:30 -0400 
categories: jekyll update 
author: "J von der Mosel, A Trautsch, S Herbold - arXiv preprint arXiv:2109.04738, 2021" 
--- 
Transformers are the current state-of-the-art of natural language processing in many domains and are using traction within software engineering research as well. Such models are pre-trained on large amounts of data, usually from the general domain. However, we only have a limited understanding regarding the validity of transformers within the software engineering domain, ie, how good such models are at understanding words and sentences within a software engineering context and how Cites: Codebert: A pre-trained model for programming and natural