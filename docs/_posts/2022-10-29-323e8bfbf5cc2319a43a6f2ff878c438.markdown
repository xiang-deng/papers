--- 
layout: post 
title: "Autoregressive Structured Prediction with Language Models" 
date: 2022-10-29 01:49:44 -0400 
categories: jekyll update 
author: "T Liu, Y Jiang, N Monath, R Cotterell, M Sachan - arXiv preprint arXiv:2210.14698, 2022" 
--- 
Recent years have seen a paradigm shift in NLP towards using pretrained language models ({PLM}) for a wide range of tasks. However, there are many difficult design decisions to represent structures (eg tagged text, coreference chains) in a way such that they can be captured by PLMs. Prior work on structured prediction with PLMs typically flattens the structured output into a sequence, which limits the quality of structural information being learned and leads to inferior performance compared to Cites: A Joint Model for Entity Analysis: Coreference, Typing, and Linking