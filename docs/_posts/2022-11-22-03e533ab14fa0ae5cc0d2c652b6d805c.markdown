---
layout: post
title:  "Efficient Transformers with Dynamic Token Pooling"
date:   2022-11-22 02:23:19 -0400
categories: jekyll update
author: "P Nawrot, J Chorowski, A Łańcucki, EM Ponti - arXiv preprint arXiv:2211.09761, 2022"
---
Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or phrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive …
Cites: ‪Emergent abilities of large language models‬