---
layout: post
title:  "A New Feature Engineering-Driven Pre-Trained Language Model for Enabling Semantically Enriched Natural Language Processing Tasks"
date:   2023-03-09 05:52:34 -0400
categories: jekyll update
author: "K Ameri - 2022"
---
Natural language processing (NLP) techniques had significantly improved by introducing pre-trained language models (PLM). The pre-training method uses unannotated data for self-supervised training and can be applied to downstream tasks through fine-tuning and few-shot training. Even though PLMs can recognize useful linguistic information in unlabeled texts, factual knowledge is generally not well represented.
Cites: ‪Mobilebert: a compact task-agnostic bert for resource-limited devices‬