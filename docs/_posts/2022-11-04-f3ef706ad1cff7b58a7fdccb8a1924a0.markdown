---
layout: post
title:  "Characterizing Intrinsic Compositionality In Transformers With Tree Projections"
date:   2022-11-04 15:58:33 -0400
categories: jekyll update
author: "S Murty, P Sharma, J Andreas, CD Manning - arXiv preprint arXiv:2211.01288, 2022"
---
When trained on language data, do transformers learn some arbitrary computation that utilizes the full capacity of the architecture or do they learn a simpler, tree-like computation, hypothesized to underlie compositional meaning systems like human languages? There is an apparent tension between compositional accounts of human language understanding, which are based on a restricted bottom-up computational process, and the enormous success of neural models like transformers, which can …
Cites: ‪Beyond Accuracy: Behavioral Testing of NLP Models with CheckList‬