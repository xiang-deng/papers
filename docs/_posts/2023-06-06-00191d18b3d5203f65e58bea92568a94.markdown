---
layout: post
title:  "Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning"
date:   2023-06-06 05:46:58 -0400
categories: jekyll update
author: "B Liao, S Tan, C Monz - arXiv preprint arXiv:2306.00477, 2023"
---
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to …
Cites: ‪LST: Ladder Side-Tuning for Parameter and Memory Efficient …‬