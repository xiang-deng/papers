--- 
layout: post 
title: "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond" 
date: 2023-05-27 10:00:59 -0400 
categories: jekyll update 
author: "P Laban, W Kryciski, D Agarwal, AR Fabbri, C Xiong - arXiv preprint arXiv , 2023" 
--- 
With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals that  Cites: Understanding Factual Errors in Summarization: Errors