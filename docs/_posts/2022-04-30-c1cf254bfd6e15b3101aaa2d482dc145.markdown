--- 
layout: post 
title: "TRACE: A Fast Transformer-based General-Purpose Lossless Compressor" 
date: 2022-04-30 03:01:01 -0400 
categories: jekyll update 
author: "Y Mao, Y Cui, TW Kuo, CJ Xue - Proceedings of the ACM Web Conference 2022, 2022" 
--- 
Deep-learning-based compressor has received interests recently due to much improved compression ratio. However, modern approaches suffer from long execution time. To ease this problem, this paper targets on cutting down the execution time of deep-learning-based compressors. Building history-dependencies sequentially (eg, recurrent neural networks) is responsible for long inference latency. Instead, we introduce transformer into deep learning compressors to build history Cites: BERT: Pre-training of deep bidirectional transformers for language