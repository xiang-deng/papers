---
layout: post
title:  "Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models"
date:   2022-12-23 23:45:02 -0400
categories: jekyll update
author: "S Wang, J Wei, A Sabne, A Davis, B Ilbeyi, B Hechtman… - Proceedings of the 28th …, 2022"
---
Large deep learning models have shown great potential with state-of-the-art results in many tasks. However, running these large models is quite challenging on an accelerator (GPU or TPU) because the on-device memory is too limited for the size of these models. Intra-layer model parallelism is an approach to address the issues by partitioning individual layers or operators across multiple devices in a distributed accelerator cluster. But, the data communications generated by intra-layer model …
Cites: ‪Bigssl: Exploring the frontier of large-scale semi-supervised …‬