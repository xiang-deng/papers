--- 
layout: post 
title: "A Joint Domain-Specific Pre-Training Method Based on Data Enhancement" 
date: 2023-03-28 04:46:22 -0400 
categories: jekyll update 
author: "Y Gan, G Lu, Z Su, L Wang, J Zhou, J Jiang, D Chen - Applied Sciences, 2023" 
--- 
State-of-the-art performances for natural language processing tasks are achieved by supervised learning, specifically, by fine-tuning pre-trained language models such as BERT (Bidirectional Encoder Representation from Transformers). With increasingly accurate models, the size of the finetuned pre-training corpus is becoming larger and larger. However, very few studies have explored the selection of pre-training corpus. Therefore, this paper proposes a data enhancement-based domain pre-training  Cites: K-adapter: Infusing knowledge into pre-trained models with adapters