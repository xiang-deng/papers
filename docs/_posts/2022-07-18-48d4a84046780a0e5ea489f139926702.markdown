---
layout: post
title:  "Recurrent Memory Transformer"
date:   2022-07-18 23:00:30 -0400
categories: jekyll update
author: "A Bulatov, Y Kuratov, MS Burtsev - arXiv preprint arXiv:2207.06881, 2022"
---
Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent …
Cites: ‪Gmat: Global memory augmentation for transformers‬  