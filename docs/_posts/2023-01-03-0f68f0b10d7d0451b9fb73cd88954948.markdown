--- 
layout: post 
title: "Online Knowledge Distillation for Multi-Task Learning" 
date: 2023-01-03 01:29:15 -0400 
categories: jekyll update 
author: "GM Jacob, V Agarwal, B Stenger - Proceedings of the IEEE/CVF Winter Conference , 2023" 
--- 
Multi-task learning (MTL) has found wide application in computer vision tasks. It uses a common backbone network allowing shared feature computation for different tasks such as semantic segmentation, depth-and normal estimation. In many cases negative transfer, ie impaired performance in the target domain, causes the MTL accuracy to be lower than simply training the corresponding single-task networks. To mitigate this issue, we propose an online knowledge distillation method for MTL  Cites: BAM! Born-Again Multi-Task Networks for Natural Language