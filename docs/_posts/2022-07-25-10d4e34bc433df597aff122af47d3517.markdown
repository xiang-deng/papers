--- 
layout: post 
title: "Learning Sequence Representations by Non-local Recurrent Neural Memory" 
date: 2022-07-25 21:58:40 -0400 
categories: jekyll update 
author: "W Pei, X Feng, C Fu, Q Cao, G Lu, YW Tai - arXiv preprint arXiv:2207.09710, 2022" 
--- 
The key challenge of sequence representation learning is to capture the long-range temporal dependencies. Typical methods for supervised sequence representation learning are built upon recurrent neural networks to capture temporal dependencies. One potential limitation of these methods is that they only model one-order information interactions explicitly between adjacent time steps in a sequence, hence the high-order interactions between nonadjacent time steps are not fully exploited. It  Cites: Ask me anything: Dynamic memory networks for natural language