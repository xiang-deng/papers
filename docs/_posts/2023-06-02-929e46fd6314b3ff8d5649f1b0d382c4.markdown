--- 
layout: post 
title: "Dissecting Chain-of-Thought: A Study on Compositional In-Context Learning of MLPs" 
date: 2023-06-02 15:36:55 -0400 
categories: jekyll update 
author: "Y Li, K Sreenivasan, A Giannou, D Papailiopoulos - arXiv preprint arXiv , 2023" 
--- 
Chain-of-thought (CoT) is a method that enables language models to handle complex reasoning tasks by decomposing them into simpler steps. Despite its success, the underlying mechanics of CoT are not yet fully understood. In an attempt to shed light on this, our study investigates the impact of CoT on the ability of transformers to in-context learn a simple to study, yet general family of compositional functions: multi-layer perceptrons (MLPs). In this setting, we reveal that the success  Cites: What can transformers learn in-context? a case study of simple