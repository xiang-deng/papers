--- 
layout: post 
title: "InfoNCE Loss Provably Learns Cluster-Preserving Representations" 
date: 2023-02-20 23:17:05 -0400 
categories: jekyll update 
author: "A Parulekar, L Collins, K Shanmugam, A Mokhtari - arXiv preprint arXiv , 2023" 
--- 
The goal of contrasting learning is to learn a representation that preserves underlying clusters by keeping samples with similar content, eg the``dogness of a dog, close to each other in the space generated by the representation. A common and successful approach for tackling this unsupervised learning problem is minimizing the InfoNCE loss associated with the training samples, where each sample is associated with their augmentations (positive samples such as rotation  Cites: Connect, not collapse: Explaining contrastive learning for