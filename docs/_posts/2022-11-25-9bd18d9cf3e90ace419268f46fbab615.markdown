---
layout: post
title:  "Recent Advances in Pre-trained Language Models: Why Do They Work and How Do They Work"
date:   2022-11-25 23:42:34 -0400
categories: jekyll update
author: "CH Chiang, YS Chuang, HY Lee - Proceedings of the 2nd Conference of the Asia …, 2022"
---
Pre-trained language models (PLMs) are language models that are pre-trained on large-scaled corpora in a self-supervised fashion. These PLMs have fundamentally changed the natural language processing community in the past few years. In this tutorial, we aim to provide a broad and comprehensive introduction from two perspectives: why those PLMs work, and how to use them in NLP tasks. The first part of the tutorial shows some insightful analysis on PLMs that partially explain their …
Cites: ‪Rethinking the Role of Demonstrations: What Makes In-Context …‬