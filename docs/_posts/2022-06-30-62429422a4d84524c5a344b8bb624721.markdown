---
layout: post
title:  "COMPRESSING BERT 25 TIMES BY RNN IN NAMED ENTITY RECOGNITION TASK"
date:   2022-06-30 03:02:10 -0400
categories: jekyll update
author: "I Malakhov"
---
In named entity recognition task state-of-the-art models usually have a lot of parameters. It may become an obstacle to use these models on devices with limited performance due to big size and long inference time. In this work we used a pre-trained BERT-based model to train several RNN models with significantly less number of parameters using knowledge distillation technique. It was shown that such approach allows to achieve much better quality than usual training on a labeled task …
Cites: ‪Well-read students learn better: The impact of student initialization …‬  