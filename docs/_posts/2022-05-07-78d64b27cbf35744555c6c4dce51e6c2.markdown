---
layout: post
title:  "OPT: Open Pre-trained Transformer Language Models"
date:   2022-05-07 02:52:45 -0400
categories: jekyll update
author: "S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen - arXiv preprint arXiv , 2022"
---
Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero-and few-shot learning. Given their computational cost, these models are difficult to replicate without