--- 
layout: post 
title: "Evaluate Confidence Instead of Perplexity for Zero-shot Commonsense Reasoning" 
date: 2022-08-26 23:24:20 -0400 
categories: jekyll update 
author: "L Peng, Z Li, H Zhao - arXiv preprint arXiv:2208.11007, 2022" 
--- 
Commonsense reasoning is an appealing topic in natural language processing (NLP) as it plays a fundamental role in supporting the human-like actions of NLP systems. With large-scale language models as the backbone, unsupervised pre-training on numerous corpora shows the potential to capture commonsense knowledge. Current pre-trained language model (PLM)-based reasoning follows the traditional practice using perplexity metric. However, commonsense reasoning is Cites: Commonsenseqa 2.0: Exposing the limits of ai through gamification