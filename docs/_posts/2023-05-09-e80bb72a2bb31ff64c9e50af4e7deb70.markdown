--- 
layout: post 
title: "PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer" 
date: 2023-05-09 11:33:00 -0400 
categories: jekyll update 
author: "L Chen, H Huang, M Cheng - arXiv preprint arXiv:2305.02423, 2023" 
--- 
Recent studies show that prompt tuning can better leverage the power of large language models than fine-tuning on downstream natural language understanding tasks. However, the existing prompt tuning methods have training instability issues, as the variance of scores under different random seeds is quite large. To address this critical problem, we first investigate and find that the loss landscape of vanilla prompt tuning is precipitous when it is visualized, where a slight change of input data can  Cites: BoolQ: Exploring the surprising difficulty of natural yes/no questions