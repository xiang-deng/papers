--- 
layout: post 
title: "FAST: Improving Controllability for Text Generation with Feedback Aware Self-Training" 
date: 2022-10-12 20:42:55 -0400 
categories: jekyll update 
author: "J Chai, R Pryzant, VY Dong, K Golobokov, C Zhu, Y Liu - arXiv preprint arXiv , 2022" 
--- 
Controllable text generation systems often leverage control codes to direct various properties of the output like style and length. Inspired by recent work on causal inference for NLP, this paper reveals a previously overlooked flaw in these control code-based conditional text generation algorithms. Spurious correlations in the training data can lead models to incorrectly rely on parts of the input other than the control code for attribute selection, significantly undermining downstream generation  Cites: Prefix-tuning: Optimizing continuous prompts for generation