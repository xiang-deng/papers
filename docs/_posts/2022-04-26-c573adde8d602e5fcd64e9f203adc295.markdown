---
layout: post
title:  "Spinning Language Models: Risks of Propaganda-as-a-Service and Countermeasures"
date:   2022-04-26 05:34:18 -0400
categories: jekyll update
author: "E Bagdasaryan, V Shmatikov - 2022 IEEE Symposium on Security and Privacy (SP), 2022"
---
We investigate a new threat to neural sequence-to-sequence (seq2seq) models: training-time attacks that cause models to  spin  their outputs so as to support an adversary-chosen sentiment or point of view--but only when the input contains adversary-chosen trigger words. For example, a spinned summarization model outputs positive summaries of any text that mentions the name of some individual or organization. Model spinning introduces a  meta-backdoor  into a model. Whereas Cites: Summeval: Re-evaluating summarization evaluation