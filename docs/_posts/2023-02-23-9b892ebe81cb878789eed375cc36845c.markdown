--- 
layout: post 
title: "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT" 
date: 2023-02-23 04:09:00 -0400 
categories: jekyll update 
author: "C Zhou, Q Li, C Li, J Yu, Y Liu, G Wang, K Zhang, C Ji - arXiv preprint arXiv , 2023" 
--- 
The Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A pretrained foundation model, such as BERT, GPT-3, MAE, DALLE-E, and ChatGPT, is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. The idea of pretraining behind PFMs plays an important role in the application of large models. Different from previous methods that apply  Cites: Deep contextualized word representations