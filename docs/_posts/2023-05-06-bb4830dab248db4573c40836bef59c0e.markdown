--- 
layout: post 
title: "Can Large Language Models Be an Alternative to Human Evaluations?" 
date: 2023-05-06 06:19:24 -0400 
categories: jekyll update 
author: "CH Chiang, H Lee - arXiv preprint arXiv:2305.01937, 2023" 
--- 
Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are Cites: The Perils of Using Mechanical Turk to Evaluate Open-Ended Text