---
layout: post
title:  "An Extensive Study on Pre-trained Models for Program Understanding and Generation"
date:   2022-06-18 03:19:09 -0400
categories: jekyll update
author: "Z Zeng, H Tan, H Zhang, J Li, Y Zhang, L Zhang - 2022"
---
Automatic program understanding and generation techniques could significantly advance the productivity of programmers and have been widely studied by academia and industry. Recently, the advent of pre-trained paradigm enlightens researchers to develop general-purpose pre-trained models which can be applied for a broad range of program understanding and generation tasks. Such pre-trained models, derived by self-supervised objectives on large unlabelled corpora, can be fine-tuned in …
Cites: ‪Beyond Accuracy: Behavioral Testing of NLP Models with CheckList‬  