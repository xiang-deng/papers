--- 
layout: post 
title: "Probing for Constituency Structure in Neural Language Models" 
date: 2022-04-19 07:59:02 -0400 
categories: jekyll update 
author: "D Arps, Y Samih, L Kallmeyer, H Sajjad - arXiv preprint arXiv:2204.06201, 2022" 
--- 
In this paper, we investigate to which extent contextual neural language models (LMs) implicitly learn syntactic structure. More concretely, we focus on constituent structure as represented in the Penn Treebank (PTB). Using standard probing techniques based on diagnostic classifiers, we assess the accuracy of representing constituents of different categories within the neuron activations of a LM such as RoBERTa. In order to make sure that our probe focuses on syntactic knowledge and Cites: Dissecting contextual word embeddings: Architecture and