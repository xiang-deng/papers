--- 
layout: post 
title: "RPTQ: Reorder-based Post-training Quantization for Large Language Models" 
date: 2023-04-06 06:45:39 -0400 
categories: jekyll update 
author: "Z Yuan, L Niu, J Liu, W Liu, X Wang, Y Shang, G Sun - arXiv preprint arXiv , 2023" 
--- 
Large-scale language models (LLMs) have demonstrated outstanding performance on various tasks, but their deployment poses challenges due to their enormous model size. In this paper, we identify that the main challenge in quantizing LLMs stems from the different activation ranges between the channels, rather than just the issue of outliers. We propose a novel reorder-based quantization approach, RPTQ, that addresses the issue of quantizing the activations of LLMs. RPTQ rearranges the Cites: High-throughput Generative Inference of Large Language Models