--- 
layout: post 
title: "The Trade-off between Universality and Label Efficiency of Representations from Contrastive Learning" 
date: 2023-03-04 02:48:03 -0400 
categories: jekyll update 
author: "Z Shi, J Chen, K Li, J Raghuram, X Wu, Y Liang, S Jha - arXiv preprint arXiv , 2023" 
--- 
Pre-training representations (aka foundation models) has recently become a prevalent learning paradigm, where one first pre-trains a representation using large-scale unlabeled data, and then learns simple predictors on top of the representation using small labeled data from the downstream tasks. There are two key desiderata for the representation: label efficiency (the ability to learn an accurate classifier on top of the representation with a small amount of labeled data) and universality  Cites: Palm: Scaling language modeling with pathways