---
layout: post
title:  "Robust fine-tuning of zero-shot models"
date:   2021-09-11 11:24:16 -0400
categories: jekyll update
author: "M Wortsman, G Ilharco, M Li, JW Kim, H Hajishirzi - arXiv preprint arXiv , 2021"
---
Large pre-trained models such as CLIP offer consistent accuracy across a range of data distributions when performing zero-shot inference (ie, without fine-tuning on a specific dataset). Although existing fine-tuning approaches substantially improve accuracy in-distribution, they also reduce out-of-distribution robustness. We address this tension by introducing a simple and effective method for improving robustness: ensembling the weights of the zero-shot and fine-tuned models. Compared to Cites: Wilds: A benchmark of in-the-wild distribution shifts