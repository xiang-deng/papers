--- 
layout: post 
title: "Needle in a Haystack: An Analysis of Finding Qualified Workers on MTurk for Summarization" 
date: 2022-12-23 23:45:02 -0400 
categories: jekyll update 
author: "L Zhang, J Sedoc, S Mille, Y Hou, S Gehrmann - arXiv preprint arXiv , 2022" 
--- 
The acquisition of high-quality human annotations through crowdsourcing platforms like Amazon Mechanical Turk (MTurk) is more challenging than expected. The annotation quality might be affected by various aspects like annotation instructions, Human Intelligence Task (HIT) design, and wages paid to annotators, etc. To avoid potentially low-quality annotations which could mislead the evaluation of automatic summarization system outputs, we investigate the recruitment of high-quality MTurk  Cites: The Perils of Using Mechanical Turk to Evaluate Open-Ended Text