---
layout: post
title:  "Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning"
date:   2022-05-10 03:22:04 -0400
categories: jekyll update
author: "X Chen, L Li, N Zhang, C Tan, F Huang, L Si, H Chen - arXiv preprint arXiv , 2022"
---
Pre-trained language models have contributed significantly to relation extraction by demonstrating remarkable few-shot learning abilities. However, prompt tuning methods for relation extraction may still fail to generalize to those rare or hard patterns. Note that the previous parametric learning paradigm can be viewed as memorization regarding training data as a book and inference as the close-book test. Those long-tailed or hard patterns can hardly be memorized in parameters given few Cites: Efficient Nearest Neighbor Language Models