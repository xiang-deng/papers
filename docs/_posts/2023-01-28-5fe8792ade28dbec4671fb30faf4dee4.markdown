--- 
layout: post 
title: "Tighter Bounds on the Expressivity of Transformer Encoders" 
date: 2023-01-28 04:04:00 -0400 
categories: jekyll update 
author: "D Chiang, P Cholak, A Pillay - arXiv preprint arXiv:2301.10743, 2023" 
--- 
Characterizing neural networks in terms of better-understood formal systems has the potential to yield new insights into the power and limitations of these networks. Doing so for transformers remains an active area of research. Bhattamishra and others have shown that transformer encoders are at least as expressive as a certain kind of counter machine, while Merrill and Sabharwal have shown that fixed-precision transformer encoders recognize only languages in uniform $ TC^ 0$. We connect Cites: Saturated transformers are constant-depth threshold circuits