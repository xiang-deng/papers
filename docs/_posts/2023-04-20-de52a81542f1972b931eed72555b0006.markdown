--- 
layout: post 
title: "Sabi\ a: Portuguese Large Language Models" 
date: 2023-04-20 07:45:04 -0400 
categories: jekyll update 
author: "R Pires, H Abonizio, T Rogrio, R Nogueira - arXiv preprint arXiv:2304.07880, 2023" 
--- 
As the capabilities of language models continue to advance, it is conceivable that one-size-fits-all model will remain as the main paradigm. For instance, given the vast number of languages worldwide, many of which are low-resource, the prevalent practice is to pretrain a single model on multiple languages. In this paper, we add to the growing body of evidence that challenges this practice, demonstrating that monolingual pretraining on the target language significantly improves models  Cites: Beyond the imitation game: Quantifying and extrapolating the