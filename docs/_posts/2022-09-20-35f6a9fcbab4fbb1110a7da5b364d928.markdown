--- 
layout: post 
title: "Graph-to-Text Generation with Dynamic Structure Pruning" 
date: 2022-09-20 01:42:47 -0400 
categories: jekyll update 
author: "L Li, R Geng, B Li, C Ma, Y Yue, B Li, Y Li - arXiv preprint arXiv:2209.07258, 2022" 
--- 
Most graph-to-text works are built on the encoder-decoder framework with cross-attention mechanism. Recent studies have shown that explicitly modeling the input graph structure can significantly improve the performance. However, the vanilla structural encoder cannot capture all specialized information in a single forward pass for all decoding steps, resulting in inaccurate semantic representations. Meanwhile, the input graph is flatted as an unordered sequence in the cross attention, ignoring Cites: Handling divergent reference texts when evaluating table-to-text