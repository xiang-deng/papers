---
layout: post
title:  "MuCoT: Multilingual Contrastive Training for Question-Answering in Low-resource Languages"
date:   2022-04-16 01:25:48 -0400
categories: jekyll update
author: "GK Kumar, AS Gehlot, SS Mullappilly, K Nandakumar - arXiv preprint arXiv , 2022"
---
Accuracy of English-language Question Answering (QA) systems has improved significantly in recent years with the advent of Transformer-based models (eg, BERT). These models are pre-trained in a self-supervised fashion with a large English text corpus and further fine-tuned with a massive English QA dataset (eg, SQuAD). However, QA datasets on such a scale are not available for most of the other languages. Multi-lingual BERT-based models (mBERT) are often used to Cites: Contrastive learning of medical visual representations from paired