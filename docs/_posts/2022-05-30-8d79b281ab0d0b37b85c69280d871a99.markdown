---
layout: post
title:  "Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT"
date:   2022-05-30 22:20:45 -0400
categories: jekyll update
author: "J Lee-Thorp, J Ainslie - arXiv preprint arXiv:2205.12399, 2022"
---
We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the speed and stability of linear, mixing transformations to design the Sparse Mixer encoder model. The Sparse Mixer slightly outperforms (< 1%) BERT on GLUE and SuperGLUE, but more importantly trains 65% faster and runs inference 61% faster. We also present a faster variant, prosaically named Fast Sparse Mixer, that marginally underperforms (< 0.2%) BERT on SuperGLUE, but trains and runs nearly  Cites: Electra: Pre-training text encoders as discriminators rather than 