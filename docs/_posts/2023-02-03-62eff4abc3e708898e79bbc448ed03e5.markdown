---
layout: post
title:  "An Empirical Study on the Transferability of Transformer Modules in Parameter-Efficient Fine-Tuning"
date:   2023-02-03 14:16:33 -0400
categories: jekyll update
author: "M AkbarTajari, S Rajaee, MT Pilehvar - arXiv preprint arXiv:2302.00378, 2023"
---
Parameter-efficient fine-tuning approaches have recently garnered a lot of attention. Having considerably lower number of trainable weights, these methods can bring about scalability and computational effectiveness. In this paper, we look for optimal sub-networks and investigate the capability of different transformer modules in transferring knowledge from a pre-trained model to a downstream task. Our empirical results suggest that every transformer module in BERT can act as a winning ticket …
Cites: ‪Pruning redundant mappings in transformer models via spectral …‬