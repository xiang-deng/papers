---
layout: post
title:  "Materialized Knowledge Bases from Commonsense Transformers"
date:   2022-01-08 08:13:01 -0400
categories: jekyll update
author: "TP Nguyen, S Razniewski - arXiv preprint arXiv:2112.14815, 2021"
---
Starting from the COMET methodology by Bosselut et al.(2019), generating commonsense knowledge directly from pre-trained language models has recently received significant attention. Surprisingly, up to now no materialized resource of commonsense knowledge generated this way is publicly available. This paper fills this gap, and uses the materialized resources to perform a detailed analysis of the potential of this approach in terms of precision and recall. Furthermore, we identify Cites: How Context Affects Language Models  Factual Predictions