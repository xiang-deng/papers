--- 
layout: post 
title: "Structured Prompting: Scaling In-Context Learning to 1,000 Examples" 
date: 2022-12-17 01:50:56 -0400 
categories: jekyll update 
author: "Y Hao, Y Sun, L Dong, Z Han, Y Gu, F Wei - arXiv preprint arXiv:2212.06713, 2022" 
--- 
Large language models have exhibited intriguing in-context learning capability, achieving promising zero-and few-shot performance without updating the parameters. However, conventional in-context learning is usually restricted by length constraints, rendering it ineffective to absorb supervision from a large number of examples. In order to go beyond few shots, we introduce structured prompting that breaks the length limit and scales in-context learning to thousands of examples Cites: Piqa: Reasoning about physical commonsense in natural language