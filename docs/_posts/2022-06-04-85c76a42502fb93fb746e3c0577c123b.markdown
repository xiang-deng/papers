--- 
layout: post 
title: "Understanding Long Programming Languages with Structure-Aware Sparse Attention" 
date: 2022-06-04 01:43:25 -0400 
categories: jekyll update 
author: "T Liu, C Wang, C Chen, M Gao, A Zhou - arXiv preprint arXiv:2205.13730, 2022" 
--- 
Programming-based Pre-trained Language Models (PPLMs) such as CodeBERT have achieved great success in many downstream code-related tasks. Since the memory and computational complexity of self-attention in the Transformer grow quadratically with the sequence length, PPLMs typically limit the code length to 512. However, codes in real-world applications are generally long, such as code searches, which cannot be processed efficiently by existing PPLMs. To solve this Cites: Dawn Drain