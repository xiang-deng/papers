--- 
layout: post 
title: "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence" 
date: 2022-10-29 01:49:44 -0400 
categories: jekyll update 
author: "HT Chen, MJQ Zhang, E Choi - arXiv preprint arXiv:2210.13701, 2022" 
--- 
Question answering models can use rich knowledge sources--up to one hundred retrieved passages and parametric knowledge in the large-scale language model (LM). Prior work assumes information in such knowledge sources is consistent with each other, paying little attention to how models blend information stored in their LM parameters with that from retrieved evidence documents. In this paper, we simulate knowledge conflicts (ie, where parametric knowledge suggests one answer and Cites: Beat the AI: Investigating adversarial human annotation for reading