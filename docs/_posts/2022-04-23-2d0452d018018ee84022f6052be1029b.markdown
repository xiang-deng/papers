---
layout: post
title:  "Impact of Tokenization on Language Models: An Analysis for Turkish"
date:   2022-04-23 07:54:44 -0400
categories: jekyll update
author: "C Toraman, EH Yilmaz, F ahinu, O Ozcelik - arXiv preprint arXiv:2204.08832, 2022"
---
Tokenization is an important text preprocessing step to prepare input tokens for deep language models. WordPiece and BPE are de facto methods employed by important models, such as BERT and GPT. However, the impact of tokenization can be different for morphologically rich languages, such as Turkic languages, where many words can be generated by adding prefixes and suffixes. We compare five tokenizers at different granularity levels, ie their outputs vary from smallest pieces of characters to Cites: Charformer: Fast character transformers via gradient-based