--- 
layout: post 
title: "FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement" 
date: 2023-04-12 17:14:48 -0400 
categories: jekyll update 
author: "X Nie, X Miao, Z Wang, Z Yang, J Xue, L Ma, G Cao - arXiv preprint arXiv , 2023" 
--- 
With the increasing data volume, there is a trend of using large-scale pre-trained models to store the knowledge into an enormous number of model parameters. The training of these models is composed of lots of dense algebras, requiring a huge amount of hardware resources. Recently, sparsely-gated Mixture-of-Experts (MoEs) are becoming more popular and have demonstrated impressive pretraining scalability in various downstream tasks. However, such a sparse conditional Cites: BERT: Pre-training of Deep Bidirectional Transformers for