--- 
layout: post 
title: "Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation" 
date: 2022-11-08 00:47:36 -0400 
categories: jekyll update 
author: "Y Li, J Zhao, MR Lyu, L Wang - arXiv preprint arXiv:2211.01587, 2022" 
--- 
Recent advances in large-scale pre-training provide large models with the potential to learn knowledge from the raw text. It is thus natural to ask whether it is possible to leverage these large models as knowledge bases for downstream tasks. In this work, we answer the aforementioned question in unsupervised knowledge-grounded conversation. We explore various methods that best elicit knowledge from large models. Our human study indicates that, though hallucinations exist, large models Cites: Proton: Probing schema linking information from pre-trained