---
layout: post
title:  "ADAPTERS FOR RESOURCE-EFFICIENT DEPLOYMENT OF NLU"
date:   2023-04-27 01:18:20 -0400
categories: jekyll update
author: "J Nehring, N Feldhus, A Ahmed"
---
Modern Transformer-based language models such as BERT are huge and, therefore, expensive to deploy in practical applications. In environments such as commercial chatbot-as-a-service platforms that deploy many NLP models in parallel, less powerful models with a smaller number of parameters are an alternative to transformers to keep deployment costs down, at the cost of lower accuracy values. This paper compares different models for Intent Detection concerning their memory …
Cites: ‪AdapterFusion: Non-destructive task composition for transfer learning‬