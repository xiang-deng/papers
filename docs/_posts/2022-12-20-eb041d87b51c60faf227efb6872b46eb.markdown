---
layout: post
title:  "MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers"
date:   2022-12-20 02:26:19 -0400
categories: jekyll update
author: "K Zhou, X Liu, Y Gong, WX Zhao, D Jiang, N Duan… - arXiv preprint arXiv …, 2022"
---
Dense retrieval aims to map queries and passages into low-dimensional vector space for efficient similarity measuring, showing promising effectiveness in various large-scale retrieval tasks. Since most existing methods commonly adopt pre-trained Transformers (eg BERT) for parameter initialization, some work focuses on proposing new pre-training tasks for compressing the useful semantic information from passages into dense vectors, achieving remarkable performances. However, it …
Cites: ‪AmbigQA: Answering ambiguous open-domain questions‬