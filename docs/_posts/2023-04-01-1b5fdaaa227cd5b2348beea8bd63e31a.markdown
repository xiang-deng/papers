---
layout: post
title:  "Pre-training Transformers for Knowledge Graph Completion"
date:   2023-04-01 04:48:36 -0400
categories: jekyll update
author: "S Chen, H Cheng, X Liu, J Jiao, Y Ji, J Gao - arXiv preprint arXiv:2303.15682, 2023"
---
Learning transferable representation of knowledge graphs (KGs) is challenging due to the heterogeneous, multi-relational nature of graph structures. Inspired by Transformer-based pretrained language models  success on learning transferable representation for texts, we introduce a novel inductive KG representation model (iHT) for KG completion by large-scale pre-training. iHT consists of a entity encoder (eg, BERT) and a neighbor-aware relational scoring function both parameterized by …
Cites: ‪Embedding entities and relations for learning and inference in …‬