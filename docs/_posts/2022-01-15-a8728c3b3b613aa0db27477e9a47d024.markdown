---
layout: post
title:  "A ConvNet for the 2020s"
date:   2022-01-15 10:11:37 -0400
categories: jekyll update
author: "Z Liu, H Mao, CY Wu, C Feichtenhofer, T Darrell, S Xie - arXiv preprint arXiv , 2022"
---
The  Roaring 20s  of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (eg, Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a Cites: Electra: Pre-training text encoders as discriminators rather than