---
layout: post
title:  "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models"
date:   2023-04-08 04:35:01 -0400
categories: jekyll update
author: "Z Hu, Y Lan, L Wang, W Xu, EP Lim, RKW Lee, L Bing… - arXiv preprint arXiv …, 2023"
---
The success of large language models (LLMs), like GPT-3 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by fine-tuning open-access LLMs with task-specific data (eg, ChatDoctor) or instruction data (eg, Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire …
Cites: ‪Self-Instruct: Aligning Language Model with Self Generated …‬