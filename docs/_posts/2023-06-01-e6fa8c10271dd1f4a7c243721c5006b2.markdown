--- 
layout: post 
title: "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time" 
date: 2023-06-01 02:05:49 -0400 
categories: jekyll update 
author: "Z Liu, A Desai, F Liao, W Wang, V Xie, Z Xu, A Kyrillidis - arXiv preprint arXiv , 2023" 
--- 
Large language models (LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference Cites: Rethinking the Role of Scale for In-Context Learning: An