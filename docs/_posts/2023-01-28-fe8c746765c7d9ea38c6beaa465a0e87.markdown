---
layout: post
title:  "Efficient Inference of Transformers in Natural Language Processing: Early Exiting and Beyond"
date:   2023-01-28 04:04:00 -0400
categories: jekyll update
author: "J Xin - 2023"
---
Large-scale pre-trained transformer models such as BERT have become ubiquitous in Natural Language Processing (NLP) research and applications. They bring significant improvements to both academia benchmarking tasks and industry applications: the average score on the General Language Understanding Evaluation benchmark (GLUE) has increased from 74 to 90+; commercial search engines such as Google and Microsoft Bing are also applying BERT-like models to search. Despite …
Cites: ‪Mobilebert: a compact task-agnostic bert for resource-limited devices‬