--- 
layout: post 
title: "Systematic Rectification of Language Models via Dead-end Analysis" 
date: 2023-03-02 06:18:50 -0400 
categories: jekyll update 
author: "M Cao, M Fatemi, JCK Cheung, S Shabanian - arXiv preprint arXiv:2302.14003, 2023" 
--- 
With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses. One way to reduce the risk of LLMs generating undesired discourses is to alter the training of the LLM. This can be very restrictive due to demanding computation requirements. Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse. Here, we center  Cites: Language models as knowledge bases?