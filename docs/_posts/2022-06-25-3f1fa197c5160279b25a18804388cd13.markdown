---
layout: post
title:  "A Survey of Using Unsupervised Learning Techniques in Building Masked Language Models for Low Resource Languages"
date:   2022-06-25 08:25:58 -0400
categories: jekyll update
author: "L Kryeziu, V Shehu - 2022 11th Mediterranean Conference on Embedded …, 2022"
---
A very common approach nowadays towards learning how to represent text in machines is to use transformers. These models are based on neural networks, and they show promising results when applied to problems in which both the input and output of the model are sequences. In this paper we give an overview on what transformers are with a focus on Bidirectional Encoder Representations from Transformers (BERT). Furthermore, we analyze different approaches on how these …
Cites: ‪XTREME: A massively multilingual multi-task benchmark for …‬  