--- 
layout: post 
title: "A Theoretical View on Sparsely Activated Networks" 
date: 2022-07-25 21:58:40 -0400 
categories: jekyll update 
author: "C Baykal, N Dikkala, R Panigrahy, C Rashtchian" 
--- 
Deep and wide neural networks successfully fit very complex functions today, but dense models are starting to be prohibitively expensive. To mitigate this, one promising research direction is networks that activate a sparse subgraph of the network. The subgraph is chosen by a datadependent routing function, enforcing a fixed mapping of inputs to subnetworks (eg, the Mixture of Experts (MoE) paradigm). However, there is little theoretical grounding for these sparsely activated models. As  Cites: Glam: Efficient scaling of language models with mixture-of-experts