--- 
layout: post 
title: "SPOT: Knowledge-Enhanced Language Representations for Information Extraction" 
date: 2022-09-17 00:49:30 -0400 
categories: jekyll update 
author: "J Li, Y Katsis, T Baldwin, HC Kim, A Bartko, J McAuley - 2022" 
--- 
Knowledge-enhanced pre-trained models for language representation have been shown to be more effective in knowledge base construction tasks (ie, relation extraction) than language models such as BERT. These knowledge-enhanced language models incorporate knowledge into pre-training to generate representations of entities or relationships. However, existing methods typically represent each entity with a separate embedding. As a result, these methods Cites: K-adapter: Infusing knowledge into pre-trained models with adapters