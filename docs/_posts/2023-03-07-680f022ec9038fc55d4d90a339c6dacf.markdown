---
layout: post
title:  "Semiparametric Language Models Are Scalable Continual Learners"
date:   2023-03-07 06:19:37 -0400
categories: jekyll update
author: "G Peng, T Ge, SQ Chen, F Wei, H Wang - arXiv preprint arXiv:2303.01421, 2023"
---
Semiparametric language models (LMs) have shown promise in continuously learning from new text data by combining a parameterized neural LM with a growable non-parametric memory for memorizing new content. However, conventional semiparametric LMs will finally become prohibitive for computing and storing if they are applied to continual learning over streaming data, because the non-parametric memory grows linearly with the amount of data they learn from over time …
Cites: ‪ELLE: Efficient Lifelong Pre-training for Emerging Data‬