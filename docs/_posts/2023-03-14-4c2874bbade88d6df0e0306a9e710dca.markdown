--- 
layout: post 
title: "The Quarks of Attention: Structure and Capacity of Neural Attention Building Blocks" 
date: 2023-03-14 05:28:18 -0400 
categories: jekyll update 
author: "P Baldi, R Vershynin - Artificial Intelligence, 2023" 
--- 
Attention plays a fundamental role in both natural and artificial intelligence systems. In deep learning, attention-based neural architectures, such as transformer architectures, are widely used to tackle problems in natural language processing and beyond. Here we investigate the most fundamental building blocks of attention and their computational properties within the standard model of deep learning. We first derive a systematic taxonomy of all possible attention mechanisms within, or as Cites: So, and Quoc V Le. Pay attention to mlps