---
layout: post
title:  "Word Order Matters when you Increase Masking"
date:   2022-11-11 23:39:32 -0400
categories: jekyll update
author: "K Lasri, A Lenci, T Poibeau - arXiv preprint arXiv:2211.04427, 2022"
---
Word order, an essential property of natural languages, is injected in Transformer-based neural language models using position encoding. However, recent experiments have shown that explicit position encoding is not always useful, since some models without such feature managed to achieve state-of-the art performance on some tasks. To understand better this phenomenon, we examine the effect of removing position encodings on the pre-training objective itself (ie, masked …
Cites: ‪Train short, test long: Attention with linear biases enables input …‬