--- 
layout: post 
title: "PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation" 
date: 2022-05-30 22:20:45 -0400 
categories: jekyll update 
author: "A Liu, H Dong, N Okazaki, S Han, D Zhang - arXiv preprint arXiv:2205.12697, 2022" 
--- 
Logical table-to-text generation is a task that involves generating logically faithful sentences from tables, which requires models to derive logical level facts from table records via logical inference. It raises a new challenge on the logical-level content planning of table-to-text models. However, directly learning the logical inference knowledge from table-text pairs is very difficult for neural models because of the ambiguity of natural language and the scarcity of parallel data. Hence even large Cites: UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge