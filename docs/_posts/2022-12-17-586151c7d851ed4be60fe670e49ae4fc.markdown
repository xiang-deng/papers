--- 
layout: post 
title: "Do Text-to-Text Multi-Task Learners Suffer from Task Conflict?" 
date: 2022-12-17 01:50:56 -0400 
categories: jekyll update 
author: "D Mueller, N Andrews, M Dredze - arXiv preprint arXiv:2212.06645, 2022" 
--- 
Traditional multi-task learning architectures train a single model across multiple tasks through a shared encoder followed by task-specific decoders. Learning these models often requires specialized training algorithms that address task-conflict in the shared parameter updates, which otherwise can lead to negative transfer. A new type of multi-task learning within NLP homogenizes multi-task architectures as a shared encoder and language model decoder, which does surprisingly well across a Cites: Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter