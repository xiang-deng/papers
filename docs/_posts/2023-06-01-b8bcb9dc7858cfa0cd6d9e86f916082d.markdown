--- 
layout: post 
title: "MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies" 
date: 2023-06-01 02:05:49 -0400 
categories: jekyll update 
author: "S Zhang, S Wu, O Irsoy, S Lu, M Bansal, M Dredze - arXiv preprint arXiv , 2023" 
--- 
Autoregressive language models are trained by minimizing the cross-entropy of the model distribution Q relative to the data distribution P--that is, minimizing the forward cross-entropy, which is equivalent to maximum likelihood estimation (MLE). We have observed that models trained in this way may over-generalize , in the sense that they produce non-human-like text. Moreover, we believe that reverse cross-entropy, ie, the cross-entropy of P relative to Q, is a better reflection of how a human would  Cites: Truncation Sampling as Language Model Desmoothing