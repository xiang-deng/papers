---
layout: post
title:  "GPT-RE: In-context Learning for Relation Extraction using Large Language Models"
date:   2023-05-06 06:19:24 -0400
categories: jekyll update
author: "Z Wan, F Cheng, Z Mao, Q Liu, H Song, J Li… - arXiv preprint arXiv …, 2023"
---
In spite of the potential for ground-breaking achievements offered by large language models (LLMs)(eg, GPT-3), they still lag significantly behind fully-supervised baselines (eg, fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE:(1) low relevance regarding entity and relation in retrieved demonstrations for in-context learning; and (2) the strong inclination to wrongly classify NULL examples into other pre-defined labels. In this paper, we …
Cites: ‪Prompting Language Models for Linguistic Structure‬