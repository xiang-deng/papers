--- 
layout: post 
title: "A Slice and Dice Approach to Accelerate Compound Sparse Attention on GPU" 
date: 2022-11-10 01:14:02 -0400 
categories: jekyll update 
author: "H Li, J Choi, J Ahn - IEEE International Symposium on Workload , 2022" 
--- 
Transformer-based models are vital to various research domains, including NLP, computer vision, and recommendation systems. However, because the attention mechanism with quadratic complexity limits computation and memory footprint in long sequences, numerous sparse attention-based transformer models are proposed to alleviate these problems. To efficiently infer these models on GPUs, prior solutions such as Triton and Sputnik have accelerated sparse attention in the sparse Cites: HotpotQA: A dataset for diverse, explainable multi-hop question