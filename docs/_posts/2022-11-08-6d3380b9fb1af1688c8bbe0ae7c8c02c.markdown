--- 
layout: post 
title: "MPCFormer: fast, performant and private Transformer inference with MPC" 
date: 2022-11-08 00:47:36 -0400 
categories: jekyll update 
author: "D Li, R Shao, H Wang, H Guo, EP Xing, H Zhang - arXiv preprint arXiv:2211.01452, 2022" 
--- 
Enabling private inference is crucial for many cloud inference services that are based on Transformer models. However, existing private inference solutions for Transformers can increase the inference latency by more than 60x or significantly compromise the quality of inference results. In this paper, we design the framework MPCFORMER using secure multi-party computation (MPC) and Knowledge Distillation (KD). It can be used in tandem with many specifically designed MPC  Cites: Codebert: A pre-trained model for programming and natural