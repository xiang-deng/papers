--- 
layout: post 
title: "On the Domain Robustness with Prompt & Prefix Tuning" 
date: 2022-11-04 15:58:33 -0400 
categories: jekyll update 
author: "C Wang, L Wang, Y Luo" 
--- 
Prompt tuning and prefix tuning are two effective mechanisms to leverage frozen language models to perform downstream tasks. Robustness reflects models resilience of output under a change or noise in the input. In this paper, we analyze the robustness of natural language models using various tuning methods with respect to a domain shift (ie training on a domain but evaluating on out-of-domain data). We apply both prompt tuning and prefix tuning on T5 models for reading Cites: MRQA 2019 Shared Task: Evaluating Generalization in Reading