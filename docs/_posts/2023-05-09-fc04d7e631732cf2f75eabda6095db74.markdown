---
layout: post
title:  "Interpretable Sentence Representation with Variational Autoencoders and Attention"
date:   2023-05-09 11:33:00 -0400
categories: jekyll update
author: "G Felhi - arXiv preprint arXiv:2305.02810, 2023"
---
In this thesis, we develop methods to enhance the interpretability of recent representation learning techniques in natural language processing (NLP) while accounting for the unavailability of annotated data. We choose to leverage Variational Autoencoders (VAEs) due to their efficiency in relating observations to latent generative factors and their effectiveness in data-efficient learning and interpretable representation learning. As a first contribution, we identify and remove …
Cites: ‪Transformer feed-forward layers are key-value memories‬