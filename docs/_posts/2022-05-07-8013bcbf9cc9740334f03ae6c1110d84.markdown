---
layout: post
title:  "Paragraph-based Transformer Pre-training for Multi-Sentence Inference"
date:   2022-05-07 02:52:45 -0400
categories: jekyll update
author: "L Di Liello, S Garg, L Soldaini, A Moschitti - arXiv preprint arXiv:2205.01228, 2022"
---
Inference tasks such as answer sentence selection (AS2) or fact verification are typically solved by fine-tuning transformer-based models as individual sentence-pair classifiers. Recent studies show that these tasks benefit from modeling dependencies across multiple candidate sentences jointly. In this paper, we first show that popular pre-trained transformers perform poorly when used for fine-tuning on multi-candidate inference tasks. We then propose a new pre-training objective Cites: Reasoning over semantic-level graph for fact checking