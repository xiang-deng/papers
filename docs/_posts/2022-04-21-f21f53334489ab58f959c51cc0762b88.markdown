---
layout: post
title:  "Korean and Multilingual Language Models Study for Cross-Lingual Post-Training (XPT)"
date:   2022-04-21 03:59:43 -0400
categories: jekyll update
author: "S Son, C Park, J Lee, M Shim, C Lee, K Park, H Lim - Journal of the Korea , 2022"
---
It has been proven through many previous researches that the pretrained language model with a large corpus helps improve performance in various natural language processing tasks. However, there is a limit to building a large-capacity corpus for training in a language environment where resources are scarce. Using the Cross- lingual Post-Training (XPT) method, we analyze the method s efficiency in Korean, which is a low resource language. XPT selectively reuses the English pretrained Cites: Explicit alignment objectives for multilingual bidirectional encoders