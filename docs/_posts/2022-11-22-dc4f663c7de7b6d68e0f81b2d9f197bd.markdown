---
layout: post
title:  "Self-Training with Purpose Preserving Augmentation Improves Few-shot Generative Dialogue State Tracking"
date:   2022-11-22 02:23:19 -0400
categories: jekyll update
author: "J Lee, C Lee, Y Kim, GG Lee - arXiv preprint arXiv:2211.09379, 2022"
---
In dialogue state tracking (DST), labeling the dataset involves considerable human labor. We propose a new self-training framework for few-shot generative DST that utilize unlabeled data. Our self-training method iteratively improves the model by pseudo labeling and employs Purpose Preserving Augmentation (PPAug) to prevent overfitting. We increaese the few-shot 10% performance by approximately 4% on MultiWOZ 2.1 and enhances the slot-recall 8.34% for unseen values compared to …
Cites: ‪Strata: Self-training with task augmentation for better few-shot …‬