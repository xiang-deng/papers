---
layout: post
title:  "Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning"
date:   2022-06-25 08:25:58 -0400
categories: jekyll update
author: "X Xu, C Wu, S Rosenman, V Lal, N Duan - arXiv preprint arXiv:2206.08657, 2022"
---
Vision-Language (VL) models with the Two-Tower architecture have dominated visual-language representation learning in recent years. Current VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a cross-modal encoder, or feed the last-layer uni-modal features directly into the top cross-modal encoder, ignoring the semantic information at the different levels in the deep uni-modal encoders. Both approaches possibly …
Cites: ‪Vinvl: Revisiting visual representations in vision-language models‬  