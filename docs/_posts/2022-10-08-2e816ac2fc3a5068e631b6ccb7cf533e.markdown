--- 
layout: post 
title: "ThinkSum: Probabilistic reasoning over sets using large language models" 
date: 2022-10-08 00:45:41 -0400 
categories: jekyll update 
author: "B Ozturkler, N Malkin, Z Wang, N Jojic - arXiv preprint arXiv:2210.01293, 2022" 
--- 
Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the largest LLMs fail in scenarios that require reasoning over multiple objects or facts or making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, that reasons  Cites: Beyond the Imitation Game: Quantifying and extrapolating the