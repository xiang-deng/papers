---
layout: post
title:  "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"
date:   2023-05-30 03:09:06 -0400
categories: jekyll update
author: "S Anagnostidis, D Pavllo, L Biggio, L Noci, A Lucchi… - arXiv preprint arXiv …, 2023"
---
Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model s expressiveness, resulting in reduced memory and computational requirements …
Cites: ‪Llm. int8 (): 8-bit matrix multiplication for transformers at scale‬