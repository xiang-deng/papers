--- 
layout: post 
title: "Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models" 
date: 2022-08-29 19:44:55 -0400 
categories: jekyll update 
author: "M Bueno, C Gemmel, J Dalton, R Lotufo, R Nogueira - arXiv preprint arXiv , 2022" 
--- 
The ability to extrapolate, ie, to make predictions on sequences that are longer than those presented as training examples, is a challenging problem for current deep learning models. Recent work shows that this limitation persists in state-of-the-art Transformer-based models. Most solutions to this problem use specific architectures or training methods that do not generalize to other tasks. We demonstrate that large language models can succeed in extrapolation without modifying their architecture or Cites: Do NLP Models Know Numbers? Probing Numeracy in Embeddings