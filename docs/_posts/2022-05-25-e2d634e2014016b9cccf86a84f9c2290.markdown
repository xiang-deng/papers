--- 
layout: post 
title: "KRIT: Knowledge-Reasoning Intelligence in vision-language Transformer" 
date: 2022-05-25 22:16:33 -0400 
categories: jekyll update 
author: "K Chen, Q Huang, D McDuff, Y Bisk, J Gao - 2022" 
--- 
Transformer-based pretraining techniques have achieved impressive performance on learning cross-model representations for various multi-modality tasks. However, most off-the-shelf models do not take advantage of commonsense knowledge and logical reasoning that are crucial to many real-world tasks. To this end, we introduce a new variant of the Transformer model for representation learning, Knowledge Reasoning Intelligence in Vision-Language Transformer (KRIT). It utilizes a Cites: Detecting formal thought disorder by deep contextualized word