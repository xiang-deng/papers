--- 
layout: post 
title: "Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model" 
date: 2022-06-15 15:55:00 -0400 
categories: jekyll update 
author: "S Kumar, TR Sumers, T Yamakoshi, A Goldstein - bioRxiv, 2022" 
--- 
Piecing together the meaning of a narrative requires understanding not only the individual words but also the intricate relationships between them. How does the brain construct this kind of rich, contextual meaning from natural language? Recently, a new class of artificial neural networksbased on the Transformer architecturehas revolutionized the field of language modeling. Transformers integrate information across words via multiple layers of structured circuit Cites: Linguistic knowledge and transferability of contextual representations