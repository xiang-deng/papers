--- 
layout: post 
title: "Hard Gate Knowledge Distillation--Leverage Calibration for Robust and Reliable Language Model" 
date: 2022-10-26 13:20:27 -0400 
categories: jekyll update 
author: "D Lee, Z Tian, Y Zhao, KC Cheung, NL Zhang - arXiv preprint arXiv:2210.12427, 2022" 
--- 
In knowledge distillation, a student model is trained with supervisions from both knowledge from a teacher and observations drawn from a training data distribution. Knowledge of a teacher is considered a subject that holds inter-class relations which send a meaningful supervision to a student; hence, much effort has been put to find such knowledge to be distilled. In this paper, we explore a question that has been given little attention: when to distill such knowledge. The question is answered in  Cites: Noisy self-knowledge distillation for text summarization