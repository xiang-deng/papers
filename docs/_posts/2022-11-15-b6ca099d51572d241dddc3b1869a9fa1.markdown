--- 
layout: post 
title: "Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control" 
date: 2022-11-15 00:38:37 -0400 
categories: jekyll update 
author: "X Fan, Y Lyu, PP Liang, R Salakhutdinov, LP Morency - arXiv preprint arXiv , 2022" 
--- 
Pretrained language models have demonstrated extraordinary capabilities in language generation. However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. Existing techniques for controlling the distribution of generated text only work with quantified distributions, which require pre-defined categories, proportions of the distribution, or an existing corpus following the desired  Cites: Trick me if you can: Human-in-the-loop generation of adversarial