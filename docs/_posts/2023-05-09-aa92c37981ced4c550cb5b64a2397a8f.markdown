--- 
layout: post 
title: "Pay Attention to the Hidden Semanteme" 
date: 2023-05-09 11:33:00 -0400 
categories: jekyll update 
author: "H Tang, X Liu, Y Wang, Q Dou, M Lu - Information Sciences, 2023" 
--- 
With the capability of modeling lighter, MLP-based models like the pNLP-Mixer and the HyperMixer demonstrate the potential for diverse tasks in NLP. However, these linguistic models are not optimized for the regularity of textual hierarchical abstraction. Here, this paper proposes the hidden bias attention (HBA), a novel attention mechanism that is lighter than self-attention and focuses on extracting hidden (topic) semanteme. Additionally, this paper introduces a series of lightweight  Cites: Pay attention to mlps