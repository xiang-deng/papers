--- 
layout: post 
title: "Text Embeddings by Weakly-Supervised Contrastive Pre-training" 
date: 2022-12-10 20:24:02 -0400 
categories: jekyll update 
author: "L Wang, N Yang, X Huang, B Jiao, L Yang, D Jiang - arXiv preprint arXiv , 2022" 
--- 
This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned  Cites: Salient Phrase Aware Dense Retrieval: Can a Dense Retriever