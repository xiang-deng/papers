--- 
layout: post 
title: "General-Purpose In-Context Learning by Meta-Learning Transformers" 
date: 2022-12-14 16:04:21 -0400 
categories: jekyll update 
author: "L Kirsch, J Harrison, J Sohl-Dickstein, L Metz - arXiv preprint arXiv:2212.04458, 2022" 
--- 
Modern machine learning requires system designers to specify aspects of the learning pipeline, such as losses, architectures, and optimizers. Meta-learning, or learning-to-learn, instead aims to learn those aspects, and promises to unlock greater capabilities with less manual effort. One particularly ambitious goal of meta-learning is to train general-purpose in-context learning algorithms from scratch, using only black-box models with minimal inductive bias. Such a model takes in training  Cites: Palm: Scaling language modeling with pathways