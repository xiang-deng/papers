---
layout: post
title:  "Cross-Lingual Language Model Meta-Pretraining"
date:   2021-09-28 14:54:04 -0400
categories: jekyll update
author: "Z Chi, H Huang, L Liu, Y Bai, XL Mao - arXiv preprint arXiv:2109.11129, 2021"
---
The success of pretrained cross-lingual language models relies on two essential abilities, ie, generalization ability for learning downstream tasks in a source language, and cross-lingual transferability for transferring the task knowledge to other languages. However, current methods jointly learn the two abilities in a single- phase cross-lingual pretraining process, resulting in a trade-off between generalization and cross-lingual transfer. In this paper, we propose cross-lingual Cites: TyDi QA: A benchmark for information-seeking question answering