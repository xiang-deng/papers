--- 
layout: post 
title: "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers" 
date: 2023-05-18 07:22:22 -0400 
categories: jekyll update 
author: "L Yu, D Simig, C Flaherty, A Aghajanyan, L Zettlemoyer - arXiv preprint arXiv , 2023" 
--- 
Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger  Cites: Shortformer: Better language modeling using shorter inputs