--- 
layout: post 
title: "Pre-training for Information Retrieval: Are Hyperlinks Fully Explored?" 
date: 2022-09-20 01:42:47 -0400 
categories: jekyll update 
author: "J Wu, X Zhang, Y Zhu, Z Liu, Z Guo, Z Fei, R Lai, Y Wu - arXiv preprint arXiv , 2022" 
--- 
Recent years have witnessed great progress on applying pre-trained language models, eg, BERT, to information retrieval (IR) tasks. Hyperlinks, which are commonly used in Web pages, have been leveraged for designing pre-training objectives. For example, anchor texts of the hyperlinks have been used for simulating queries, thus constructing tremendous query-document pairs for pre-training. However, as a bridge across two web pages, the potential of hyperlinks has not been  Cites: MRQA 2019 Shared Task: Evaluating Generalization in Reading