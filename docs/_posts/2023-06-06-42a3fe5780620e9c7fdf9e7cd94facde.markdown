---
layout: post
title:  "Contextual Distortion Reveals Constituency: Masked Language Models are Implicit Parsers"
date:   2023-06-06 05:46:58 -0400
categories: jekyll update
author: "J Li, W Lu - arXiv preprint arXiv:2306.00645, 2023"
---
Recent advancements in pre-trained language models (PLMs) have demonstrated that these models possess some degree of syntactic awareness. To leverage this knowledge, we propose a novel chart-based method for extracting parse trees from masked language models (LMs) without the need to train separate parsers. Our method computes a score for each span based on the distortion of contextual representations resulting from linguistic perturbations. We design a set of …
Cites: ‪Unsupervised Latent Tree Induction with Deep Inside-Outside …‬