--- 
layout: post 
title: "Offsite-Tuning: Transfer Learning without Full Model" 
date: 2023-02-14 04:15:07 -0400 
categories: jekyll update 
author: "G Xiao, J Lin, S Han - arXiv preprint arXiv:2302.04870, 2023" 
--- 
Transfer learning is important for foundation models to adapt to downstream tasks. However, many foundation models are proprietary, so users must share their data with model owners to fine-tune the models, which is costly and raise privacy concerns. Moreover, fine-tuning large foundation models is computation-intensive and impractical for most downstream users. In this paper, we propose Offsite-Tuning, a privacy-preserving and efficient transfer learning framework that can adapt billion  Cites: HellaSwag: Can a machine really finish your sentence?