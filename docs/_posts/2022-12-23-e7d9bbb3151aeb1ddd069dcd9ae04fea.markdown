---
layout: post
title:  "Dataless Knowledge Fusion by Merging Weights of Language Models"
date:   2022-12-23 23:45:02 -0400
categories: jekyll update
author: "X Jin, X Ren, D Preotiuc-Pietro, P Cheng - arXiv preprint arXiv:2212.09849, 2022"
---
Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well …
Cites: ‪Branch-train-merge: Embarrassingly parallel training of expert …‬