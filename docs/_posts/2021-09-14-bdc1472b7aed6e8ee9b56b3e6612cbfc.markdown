--- 
layout: post 
title: "PPT: Pre-trained Prompt Tuning for Few-shot Learning" 
date: 2021-09-14 15:58:32 -0400 
categories: jekyll update 
author: "Y Gu, X Han, Z Liu, M Huang - arXiv preprint arXiv:2109.04332, 2021" 
--- 
Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large- scale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with Cites: Prefix-tuning: Optimizing continuous prompts for generation