---
layout: post
title:  "ORCA: Interpreting Prompted Language Models via Locating Supporting Data Evidence in the Ocean of Pretraining Data"
date:   2022-05-30 22:20:45 -0400
categories: jekyll update
author: "X Han, Y Tsvetkov - arXiv preprint arXiv:2205.12600, 2022"
---
Large pretrained language models have been performing increasingly well in a variety of downstream tasks via prompting. However, it remains unclear from where the model learns the task-specific knowledge, especially in a zero-shot setup. In this work, we want to find evidence of the model s task-specific competence from pretraining and are specifically interested in locating a very small subset of pretraining data that directly supports the model in the task. We call such a subset  Cites: Mauve: Measuring the gap between neural text and human text