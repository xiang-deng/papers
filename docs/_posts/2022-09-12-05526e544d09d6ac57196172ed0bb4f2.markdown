--- 
layout: post 
title: "Pre-Training a Graph Recurrent Network for Language Representation" 
date: 2022-09-12 23:50:28 -0400 
categories: jekyll update 
author: "Y Wang, L Yang, Z Teng, M Zhou, Y Zhang - arXiv preprint arXiv:2209.03834, 2022" 
--- 
Transformer-based pre-trained models have gained much advance in recent years, becoming one of the most important backbones in natural language processing. Recent work shows that the attention mechanism inside Transformer may not be necessary, both convolutional neural networks and multi-layer perceptron based models have also been investigated as Transformer alternatives. In this paper, we consider a graph recurrent network for language model pre-training, which builds a  Cites: Lamda: Language models for dialog applications