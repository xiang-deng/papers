---
layout: post
title:  "Injecting domain knowledge in language models for task-oriented dialogue systems"
date:   2022-11-01 03:49:43 -0400
categories: jekyll update
author: "D Emelin, D Bonadiman, S Alqahtani, Y Zhang… - 2022"
---
Pre-trained language models (PLM) have advanced the state-of-the-art across NLP applications, but lack domain-specific knowledge that does not naturally occur in pre-training data. Previous studies augmented PLMs with symbolic knowledge for different downstream NLP tasks. However, knowledge bases (KBs) utilized in these studies are usually large-scale and static, in contrast to small, domain-specific, and modifiable knowledge bases that are prominent in real-world task-oriented dialogue …
Cites: ‪K-adapter: Infusing knowledge into pre-trained models with adapters‬