---
layout: post
title:  "Heterformer: A Transformer Architecture for Node Representation Learning on Heterogeneous Text-Rich Networks"
date:   2022-05-28 02:05:27 -0400
categories: jekyll update
author: "B Jin, Y Zhang, Q Zhu, J Han - arXiv preprint arXiv:2205.10282, 2022"
---
We study node representation learning on heterogeneous text-rich networks, where nodes and edges are multi-typed and some types of nodes are associated with text information. Although recent studies on graph neural networks (GNNs) and pretrained language models (PLMs) have demonstrated their power in encoding network and text signals, respectively, less focus has been given to delicately coupling these two types of models on heterogeneous text-rich networks  Cites: Fine-grained fact verification with kernel graph attention network