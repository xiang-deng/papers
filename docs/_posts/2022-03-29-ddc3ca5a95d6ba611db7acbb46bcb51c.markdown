---
layout: post
title:  "A Theoretically Grounded Benchmark for Evaluating Machine Commonsense"
date:   2022-03-29 11:43:06 -0400
categories: jekyll update
author: "H Santos, K Shen, AM Mulvehill, Y Razeghi - arXiv preprint arXiv , 2022"
---
Programming machines with commonsense reasoning (CSR) abilities is a longstanding challenge in the Artificial Intelligence community. Current CSR benchmarks use multiple-choice (and in relatively fewer cases, generative) question- answering instances to evaluate machine commonsense. Recent progress in transformer-based language representation models suggest that considerable progress has been made on existing benchmarks. However, although tens of CSR Cites: Socialiqa: Commonsense reasoning about social interactions