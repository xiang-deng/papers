---
layout: post
title:  "Knowledge and Pre-trained Language Models Inside and Out: a deep-dive into datasets and external knowledge"
date:   2022-04-26 05:34:18 -0400
categories: jekyll update
author: "C Lyu - 2022"
---
Pre-trained language models such as BERT and XLNet have greatly improved the performance of many NLP tasks. These models can capture rich semantic patterns from large-scale text corpora and learn high-quality representations of texts. However, such models have shortcomingsthey underperform when faced with complicated noisy text or text that requires inference and/or external knowledge to be understood. Therefore the focus of this PhD project will be on the learning of Cites: Natural questions: a benchmark for question answering research