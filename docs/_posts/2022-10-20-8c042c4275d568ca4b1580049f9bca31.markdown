--- 
layout: post 
title: "Interpretability of BERT Latent Space through Knowledge Graphs" 
date: 2022-10-20 02:20:28 -0400 
categories: jekyll update 
author: "VW Anelli, GM Biancofiore, A De Bellis, T Di Noia - Proceedings of the 31st , 2022" 
--- 
The advent of pretrained language have renovated the ways of handling natural languages, improving the quality of systems that rely on them. BERT played a crucial role in revolutionizing the Natural Language Processing (NLP) area. However, the deep learning framework it implements lacks interpretability. Thus, recent research efforts aimed to explain what BERT learns from the text sources exploited to pre-train its linguistic model. In this paper, we analyze the latent vector space resulting from  Cites: Recent advances in natural language processing via large pre