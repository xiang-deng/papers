--- 
layout: post 
title: "GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment" 
date: 2023-04-01 04:48:36 -0400 
categories: jekyll update 
author: "Y Liu, D Iter, Y Xu, S Wang, R Xu, C Zhu - arXiv preprint arXiv:2303.16634, 2023" 
--- 
The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references Cites: BARTScore: Evaluating Generated Text as Text Generation