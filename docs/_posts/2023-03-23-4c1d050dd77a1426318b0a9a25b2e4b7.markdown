---
layout: post
title:  "Near-Duplicate Sequence Search at Scale for Large Language Model Memorization Evaluation"
date:   2023-03-23 03:27:25 -0400
categories: jekyll update
author: "Z Peng, Z Wang, D Deng - 2023"
---
Recent studies show that large language models (LLM) unintendedly memorize part of the training data, which brings serious privacy risks. For example, it has been shown that over 1% of tokens generated unprompted by an LLM are part of sequences in the training data. However, current studies mainly focus on the exact memorization behaviors. In this paper, we propose to evaluate how many generated texts have near-duplicates (eg, only differ by a couple of tokens out of 100) in the …
Cites: ‪Memorization without overfitting: Analyzing the training dynamics …‬