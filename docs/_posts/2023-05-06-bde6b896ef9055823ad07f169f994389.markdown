--- 
layout: post 
title: "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge" 
date: 2023-05-06 06:19:24 -0400 
categories: jekyll update 
author: "Y Onoe, MJQ Zhang, S Padmanabhan, G Durrett - arXiv preprint arXiv , 2023" 
--- 
Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts. We take a step forward and study LMs abilities to make inferences based on injected facts (or propagate those facts): for example, after learning that  Cites: Probing factually grounded content transfer with factual ablation