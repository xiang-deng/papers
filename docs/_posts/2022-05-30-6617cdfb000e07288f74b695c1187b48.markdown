--- 
layout: post 
title: "Your Transformer May Not be as Powerful as You Expect" 
date: 2022-05-30 22:20:45 -0400 
categories: jekyll update 
author: "S Luo, S Li, S Zheng, TY Liu, L Wang, D He - arXiv preprint arXiv:2205.13401, 2022" 
--- 
Relative Positional Encoding (RPE), which encodes the relative distance between any pair of tokens, is one of the most successful modifications to the original Transformer. As far as we know, theoretical understanding of the RPE-based Transformers is largely unexplored. In this work, we mathematically analyze the power of RPE-based Transformers regarding whether the model is capable of approximating any continuous sequence-to-sequence functions. One may naturally Cites: Train short, test long: Attention with linear biases enables input