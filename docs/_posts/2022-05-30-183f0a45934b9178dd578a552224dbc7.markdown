---
layout: post
title:  "Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models"
date:   2022-05-30 22:20:45 -0400
categories: jekyll update
author: "C Na, SV Mehta, E Strubell - arXiv preprint arXiv:2205.12694, 2022"
---
Model compression by way of parameter pruning, quantization, or distillation has recently gained popularity as an approach for reducing the computational requirements of modern deep neural network models for NLP. Pruning unnecessary parameters has emerged as a simple and effective method for compressing large models that is compatible with a wide variety of contemporary off-the-shelf hardware (unlike quantization), and that requires little additional training (unlike distillation) … Cites: ‪Structured Pruning Learns Compact and Accurate Models‬