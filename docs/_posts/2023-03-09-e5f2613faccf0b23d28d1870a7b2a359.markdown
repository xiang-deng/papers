--- 
layout: post 
title: "Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers" 
date: 2023-03-09 05:52:34 -0400 
categories: jekyll update 
author: "T Chen, Z Zhang, A Jaiswal, S Liu, Z Wang - arXiv preprint arXiv:2303.01610, 2023" 
--- 
Despite their remarkable achievement, gigantic transformers encounter significant drawbacks, including exorbitant computational and memory footprints during training, as well as severe collapse evidenced by a high degree of parameter redundancy. Sparsely-activated Mixture-of-Experts (SMoEs) have shown promise to mitigate the issue of training efficiency, yet they are prone to (1) redundant experts due to representational collapse; and (2) poor expert scalability for inference and Cites: Base layers: Simplifying training of large, sparse models