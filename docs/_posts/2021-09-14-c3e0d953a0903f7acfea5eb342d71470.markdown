--- 
layout: post 
title: "MATE: Multi-view Attention for Table Transformer Efficiency" 
date: 2021-09-14 15:58:32 -0400 
categories: jekyll update 
author: "JM Eisenschlos, M Gor, T Mller, WW Cohen - arXiv preprint arXiv:2109.04312, 2021" 
--- 
This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20% of relational tables on the web have 20 or more rows (Cafarella et al., 2008), and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose MATE, a novel Transformer architecture designed to model the structure of Cites: Uncovering the Relational Web.