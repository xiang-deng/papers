--- 
layout: post 
title: "Revisiting the Linear-Programming Framework for Offline RL with General Function Approximation" 
date: 2022-12-30 14:30:10 -0400 
categories: jekyll update 
author: "A Ozdaglar, S Pattathil, J Zhang, K Zhang - arXiv preprint arXiv:2212.13861, 2022" 
--- 
Offline reinforcement learning (RL) concerns pursuing an optimal policy for sequential decision-making from a pre-collected dataset, without further interaction with the environment. Recent theoretical progress has focused on developing sample-efficient offline RL algorithms with various relaxed assumptions on data coverage and function approximators, especially to handle the case with excessively large state-action spaces. Among them, the framework based on the linear Cites: Doubly robust bias reduction in infinite horizon off-policy estimation