--- 
layout: post 
title: "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks" 
date: 2023-05-19 23:52:25 -0400 
categories: jekyll update 
author: "A Himmi, E Irurozki, N Noiry, S Clemencon, P Colombo - arXiv preprint arXiv , 2023" 
--- 
The evaluation of natural language processing (NLP) systems is crucial for advancing the field, but current benchmarking approaches often assume that all systems have scores available for all tasks, which is not always practical. In reality, several factors such as the cost of running baseline, private systems, computational limitations, or incomplete data may prevent some systems from being evaluated on entire tasks. This paper formalize an existing problem in NLP research Cites: BoolQ: Exploring the Surprising Difficulty of Natural Yes/No