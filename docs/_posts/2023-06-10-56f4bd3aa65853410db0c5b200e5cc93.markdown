---
layout: post
title:  "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models"
date:   2023-06-10 05:24:39 -0400
categories: jekyll update
author: "Z Xie, T Lukasiewicz - arXiv preprint arXiv:2306.04067, 2023"
---
The increasingly large size of modern pretrained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and …
Cites: ‪AdapterFusion: Non-destructive task composition for transfer learning‬