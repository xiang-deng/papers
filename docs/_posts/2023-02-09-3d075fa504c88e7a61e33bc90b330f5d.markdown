---
layout: post
title:  "Computation vs. Communication Scaling for Future Transformers on Future Hardware"
date:   2023-02-09 01:30:47 -0400
categories: jekyll update
author: "S Pati, S Aga, M Islam, N Jayasena, MD Sinclair - arXiv preprint arXiv:2302.02825, 2023"
---
Scaling DNNs is shown to deliver dramatic quality gains across ML problems. This, however, has also led to a concomitant quadratic increase in computation cost. To tackle this, along with the failure of accelerator memory capacity to keep up, training these models increasingly relies on distributed training techniques. As such, an important question of interest is: how will compute and communication relatively scale as models scale and hardware evolves? A careful study which answers this …
Cites: ‪Palm: Scaling language modeling with pathways‬