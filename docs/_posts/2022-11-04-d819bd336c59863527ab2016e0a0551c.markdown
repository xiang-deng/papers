--- 
layout: post 
title: "Full Contextual Attention for Multi-resolution Transformers in Semantic Segmentation" 
date: 2022-11-04 15:58:33 -0400 
categories: jekyll update 
author: "L Themyr, C Rambour, N Thome, T Collins, A Hostettler" 
--- 
Transformers have proved to be very effective for visual recognition tasks. In particular, vision transformers construct compressed global representations through self-attention and learnable class tokens. Multi-resolution transformers have shown recent successes in semantic segmentation but can only capture local interactions in highresolution feature maps. This paper extends the notion of global tokens to build GLobal Attention Multi-resolution (GLAM) transformers. GLAM is a generic module  Cites: Random feature attention