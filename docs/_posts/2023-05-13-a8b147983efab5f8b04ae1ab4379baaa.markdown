--- 
layout: post 
title: "Multi-granularity knowledge distillation and prototype consistency regularization for class-incremental learning" 
date: 2023-05-13 06:32:20 -0400 
categories: jekyll update 
author: "Y Shi, D Shi, Z Qiao, Z Wang, Y Zhang, S Yang, C Qiu - Neural Networks, 2023" 
--- 
Deep neural networks (DNNs) are prone to the notorious catastrophic forgetting problem when learning new tasks incrementally. Class-incremental learning (CIL) is a promising solution to tackle the challenge and learn new classes while not forgetting old ones. Existing CIL approaches adopted stored representative exemplars or complex generative models to achieve good performance. However, storing data from previous tasks causes memory or privacy issues, and the training of  Cites: Better fine-tuning by reducing representational collapse