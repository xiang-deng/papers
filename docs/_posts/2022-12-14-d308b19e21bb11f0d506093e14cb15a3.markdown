--- 
layout: post 
title: "ResFed: Communication Efficient Federated Learning by Transmitting Deep Compressed Residuals" 
date: 2022-12-14 16:04:21 -0400 
categories: jekyll update 
author: "R Song, L Zhou, L Lyu, A Festag, A Knoll - arXiv preprint arXiv:2212.05602, 2022" 
--- 
Federated learning enables cooperative training among massively distributed clients by sharing their learned local model parameters. However, with increasing model size, deploying federated learning requires a large communication bandwidth, which limits its deployment in wireless networks. To address this bottleneck, we introduce a residual-based federated learning framework (ResFed), where residuals rather than model parameters are transmitted in communication networks for training. In Cites: Ctrl: A conditional transformer language model for controllable