--- 
layout: post 
title: "Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation" 
date: 2022-09-05 21:46:44 -0400 
categories: jekyll update 
author: "A Yazdanbakhsh, A Moradifirouzabadi, Z Li, M Kang - arXiv preprint arXiv , 2022" 
--- 
As its core computation, a self-attention mechanism gauges pairwise correlations across the entire input sequence. Despite favorable performance, calculating pairwise correlations is prohibitively costly. While recent work has shown the benefits of runtime pruning of elements with low attention scores, the quadratic complexity of self-attention mechanisms and their on-chip memory capacity demands are overlooked. This work addresses these constraints by architecting an accelerator Cites: Blockwise Self-Attention for Long Document Understanding