---
layout: post
title:  "DeepCuts: Single-Shot Interpretability based Pruning for BERT"
date:   2022-12-30 14:30:10 -0400
categories: jekyll update
author: "JS Grover, B Gawri, RR Manku - arXiv preprint arXiv:2212.13392, 2022"
---
As language models have grown in parameters and layers, it has become much harder to train and infer with them on single GPUs. This is severely restricting the availability of large language models such as GPT-3, BERT-Large, and many others. A common technique to solve this problem is pruning the network architecture by removing transformer heads, fully-connected weights, and other modules. The main challenge is to discern the important parameters from the less important ones. Our …
Cites: ‪Know what you don t need: Single-Shot Meta-Pruning for attention …‬