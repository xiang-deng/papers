--- 
layout: post 
title: "Why Do Neural Language Models Still Need Commonsense Knowledge to Handle Semantic Variations in Question Answering?" 
date: 2022-09-05 21:46:44 -0400 
categories: jekyll update 
author: "S Kwon, C Kang, J Han, J Choi - arXiv preprint arXiv:2209.00599, 2022" 
--- 
Many contextualized word representations are now learned by intricate neural network models, such as masked neural language models (MNLMs) which are made up of huge neural network structures and trained to restore the masked text. Such representations demonstrate superhuman performance in some reading comprehension (RC) tasks which extract a proper answer in the context given a question. However, identifying the detailed knowledge trained in MNLMs is  Cites: CommonsenseQA: A Question Answering Challenge Targeting