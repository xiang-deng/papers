--- 
layout: post 
title: "Teaching Small Language Models to Reason" 
date: 2022-12-22 13:00:23 -0400 
categories: jekyll update 
author: "LC Magister, J Mallinson, J Adamek, E Malmi - arXiv preprint arXiv , 2022" 
--- 
Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters. In this paper, we explore the transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation. Specifically, we finetune a student model on the chain of thought outputs  Cites: Beyond the Imitation Game: Quantifying and extrapolating the