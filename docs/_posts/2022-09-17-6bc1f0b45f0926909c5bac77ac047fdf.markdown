--- 
layout: post 
title: "Hard Negatives or False Negatives: Correcting Pooling Bias in Training Neural Ranking Models" 
date: 2022-09-17 00:49:30 -0400 
categories: jekyll update 
author: "Y Cai, J Guo, Y Fan, Q Ai, R Zhang, X Cheng - arXiv preprint arXiv:2209.05072, 2022" 
--- 
Neural ranking models (NRMs) have become one of the most important techniques in information retrieval (IR). Due to the limitation of relevance labels, the training of NRMs heavily relies on negative sampling over unlabeled data. In general machine learning scenarios, it has shown that training with hard negatives (ie, samples that are close to positives) could lead to better performance. Surprisingly, we find opposite results from our empirical studies in IR. When sampling top-ranked results  Cites: Dense Passage Retrieval for Open-Domain Question Answering