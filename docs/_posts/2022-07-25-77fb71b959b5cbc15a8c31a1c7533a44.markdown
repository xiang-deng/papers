--- 
layout: post 
title: "Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification" 
date: 2022-07-25 21:58:40 -0400 
categories: jekyll update 
author: "GCN Aletras" 
--- 
Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations. Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions. Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a  Cites: Learning to deceive with attention-based explanations