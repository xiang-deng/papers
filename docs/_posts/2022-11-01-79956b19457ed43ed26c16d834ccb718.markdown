---
layout: post
title:  "A COMPREHENSIVE ANALYSIS OF SUBWORD TOKENIZERS FOR MORPHOLOGICALLY RICH LANGUAGES"
date:   2022-11-01 03:49:43 -0400
categories: jekyll update
author: "E Erkaya - 2022"
---
Transformer language models have paved the way for outstanding achievements on a wide variety of natural language processing tasks. The first step in transformer models is dividing the input into tokens. Over the years, various tokenization approaches have emerged. These approaches have further evolved from character and word-level representations to subword-level representations. However, the impact of tokenization on models performance has not been thoroughly discussed …
Cites: ‪Transformer feed-forward layers are key-value memories‬