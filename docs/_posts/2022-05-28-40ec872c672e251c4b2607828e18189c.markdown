---
layout: post
title:  "A Fine-grained Interpretability Evaluation Benchmark for Neural NLP"
date:   2022-05-28 02:05:27 -0400
categories: jekyll update
author: "L Wang, Y Shen, S Peng, S Zhang, X Xiao, H Liu… - arXiv preprint arXiv …, 2022"
---
While there is increasing concern about the interpretability of neural models, the evaluation of interpretability remains an open problem, due to the lack of proper evaluation datasets and metrics. In this paper, we present a novel benchmark to evaluate the interpretability of both neural models and saliency methods. This benchmark covers three representative NLP tasks: sentiment analysis, textual similarity and reading comprehension, each provided with both English and Chinese … Cites: ‪Fact or fiction: Verifying scientific claims‬