--- 
layout: post 
title: "A Close Look at Spatial Modeling: From Attention to Convolution" 
date: 2022-12-28 14:07:11 -0400 
categories: jekyll update 
author: "X Ma, H Wang, C Qin, K Li, X Zhao, J Fu, Y Fu - arXiv preprint arXiv:2212.12552, 2022" 
--- 
Vision Transformers have shown great promise recently for many vision tasks due to the insightful architecture design and attention mechanism. By revisiting the self-attention responses in Transformers, we empirically observe two interesting issues. First, Vision Transformers present a queryirrelevant behavior at deep layers, where the attention maps exhibit nearly consistent contexts in global scope, regardless of the query patch position (also head-irrelevant). Second, the attention maps are Cites: Coatnet: Marrying convolution and attention for all data sizes