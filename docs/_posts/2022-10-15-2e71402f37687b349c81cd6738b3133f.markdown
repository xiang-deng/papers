--- 
layout: post 
title: "Transferring Knowledge from Structure-aware Self-attention Language Model to Sequence-to-Sequence Semantic Parsing" 
date: 2022-10-15 02:59:22 -0400 
categories: jekyll update 
author: "R Ji, J Ji - Proceedings of the 29th International Conference on , 2022" 
--- 
Semantic parsing considers the task of mapping a natural language sentence into a target formal representation, where various sophisticated sequence-to-sequence (seq2seq) models have been applied with promising results. Generally, these target representations follow a syntax formalism that limits permitted forms. However, it is neither easy nor flexible to explicitly integrate this syntax formalism into a neural seq2seq model. In this paper, we present a structure-aware self-attention language  Cites: Incorporating external knowledge through pre-training for natural