---
layout: post
title:  "Recyclable Tuning for Continual Pre-training"
date:   2023-05-18 07:22:22 -0400
categories: jekyll update
author: "Y Qin, C Qian, X Han, Y Lin, H Wang, R Xie, Z Liu… - arXiv preprint arXiv …, 2023"
---
Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources. We bring this issue to the forefront and …
Cites: ‪On the opportunities and risks of foundation models‬