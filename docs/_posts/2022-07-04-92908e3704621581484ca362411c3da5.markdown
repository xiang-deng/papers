---
layout: post
title:  "Nepali Encoder Transformers: An Analysis of Auto Encoding Transformer Language Models for Nepali Text Classification"
date:   2022-07-04 12:22:48 -0400
categories: jekyll update
author: "U Maskey, M Bhatta, SR Bhatta, S Dhungel, BK Bal"
---
Abstract Language model pre-training has significantly impacted NLP and resulted in performance gains on many NLP-related tasks, but comparative study of different approaches on many low-resource languages seems to be missing. This paper attempts to investigate appropriate methods for pretraining a Transformer-based model for the Nepali language. We focus on the languagespecific aspects that need to be considered for modeling. Although some language models have been trained 
Cites: Fast wordpiece tokenization