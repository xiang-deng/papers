--- 
layout: post 
title: "RECKONING: Reasoning through Dynamic Knowledge Encoding" 
date: 2023-05-13 06:32:20 -0400 
categories: jekyll update 
author: "Z Chen, G Weiss, E Mitchell, A Celikyilmaz, A Bosselut - arXiv preprint arXiv , 2023" 
--- 
Recent studies on transformer-based language models show that they can answer questions by reasoning over knowledge provided as part of the context (ie, in-context reasoning). However, since the available knowledge is often not filtered for a particular question, in-context reasoning can be sensitive to distractor facts, additional content that is irrelevant to a question but that may be relevant for a different question (ie, not necessarily random noise). In these situations, the model  Cites: Natural questions: a benchmark for question answering research