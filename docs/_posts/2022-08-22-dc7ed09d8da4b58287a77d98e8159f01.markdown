--- 
layout: post 
title: "DP-RBAdaBound: A differentially private randomized block-coordinate adaptive gradient algorithm for training deep neural networks" 
date: 2022-08-22 23:37:16 -0400 
categories: jekyll update 
author: "Q Wu, M Li, J Zhu, R Zheng, L Xing, M Zhang - Expert Systems with Applications, 2022" 
--- 
In order to rapidly train deep learning models, many adaptive gradient methods have been proposed in recent years, such as Adam and AMSGrad. However, the computation of the full gradient vectors in the above methods becomes expensive prohibitively at each iteration for handling high dimensional data. Moreover, the private information may be leaked in the process of training. For these reasons, we propose a differentially private randomized block-coordinate adaptive gradient Cites: Improving generalization performance by switching from adam to sgd