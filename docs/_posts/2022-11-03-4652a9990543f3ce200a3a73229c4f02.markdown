---
layout: post
title:  "A Solvable Model of Neural Scaling Laws"
date:   2022-11-03 01:42:13 -0400
categories: jekyll update
author: "A Maloney, DA Roberts, J Sully - arXiv preprint arXiv:2210.16859, 2022"
---
Large language models with a huge number of parameters, when trained on near internet-sized number of tokens, have been empirically shown to obey neural scaling laws: specifically, their performance behaves predictably as a power law in either parameters or dataset size until bottlenecked by the other resource. To understand this better, we first identify the necessary properties allowing such scaling laws to arise and then propose a statistical model--a joint generative data model and …
Cites: ‪Palm: Scaling language modeling with pathways‬