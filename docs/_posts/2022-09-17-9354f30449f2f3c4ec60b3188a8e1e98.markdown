--- 
layout: post 
title: "Fast Neural Kernel Embeddings for General Activations" 
date: 2022-09-17 00:49:30 -0400 
categories: jekyll update 
author: "I Han, A Zandieh, J Lee, R Novak, L Xiao, A Karbasi - arXiv preprint arXiv:2209.04121, 2022" 
--- 
Infinite width limit has shed light on generalization and optimization aspects of deep learning by establishing connections between neural networks and kernel methods. Despite their importance, the utility of these kernel methods was limited in large-scale learning settings due to their (super-) quadratic runtime and memory complexities. Moreover, most prior works on neural kernels have focused on the ReLU activation, mainly due to its popularity but also due to the difficulty of Cites: Smooth adversarial training