---
layout: post
title:  "Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?"
date:   2022-10-15 02:59:22 -0400
categories: jekyll update
author: "J Lovón-Melgarejo, JG Moreno, R Besançon, O Ferret… - Proceedings of the 29th …, 2022"
---
Despite the success of state-of-the-art pre-trained language models (PLMs) on a series of multi-hop reasoning tasks, they still suffer from their limited abilities to transfer learning from simple to complex tasks and vice-versa. We argue that one step forward to overcome this limitation is to better understand the behavioral trend of PLMs at each hop over the inference chain. Our critical underlying idea is to mimic human-style reasoning: we envision the multi-hop reasoning process as a sequence …
Cites: ‪Leap-Of-Thought: Teaching Pre-Trained Models to Systematically …‬