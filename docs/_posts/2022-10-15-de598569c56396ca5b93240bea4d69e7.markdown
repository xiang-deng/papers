--- 
layout: post 
title: "Enabling Transformers to Understand Low-Level Programs" 
date: 2022-10-15 02:59:22 -0400 
categories: jekyll update 
author: "ZC Guo, WS Moses - ret" 
--- 
Unlike prior approaches to machine learning, Transformer models can first be trained on a large corpus of unlabeled data with a generic objective and then on a smaller task-specific dataset. This versatility has led to both larger models and datasets. Consequently, Transformers have led to breakthroughs in the field of natural language processing. Generic program optimization presently operates on low-level programs such as LLVM. Unlike the high-level languages (eg C, Python, Java) Cites: Learning to Superoptimize Real-world Programs