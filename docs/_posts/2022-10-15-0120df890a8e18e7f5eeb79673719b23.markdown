--- 
layout: post 
title: "Speeding up Transformer Decoding via an Attention Refinement Network" 
date: 2022-10-15 02:59:22 -0400 
categories: jekyll update 
author: "K Wu, Y Zhang, B Hu, T Zhang - Proceedings of the 29th International Conference on , 2022" 
--- 
Despite the revolutionary advances made by Transformer in Neural Machine Translation (NMT), inference efficiency remains an obstacle due to the heavy use of attention operations in auto-regressive decoding. We thereby propose a lightweight attention structure called Attention Refinement Network (ARN) for speeding up Transformer. Specifically, we design a weighted residual network, which reconstructs the attention by reusing the features across layers. To further improve the  Cites: Deep encoder, shallow decoder: Reevaluating non-autoregressive