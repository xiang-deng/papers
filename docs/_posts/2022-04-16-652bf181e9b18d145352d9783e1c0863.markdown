---
layout: post
title:  "BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model"
date:   2022-04-16 01:25:48 -0400
categories: jekyll update
author: "H Yuan, Z Yuan, R Gan, J Zhang, Y Xie, S Yu - arXiv preprint arXiv:2204.03905, 2022"
---
Pretrained language models have served as important backbones for natural language processing. Recently, in-domain pretraining has been shown to benefit various domain-specific downstream tasks. In the biomedical domain, natural language generation (NLG) tasks are of critical importance, while understudied. Approaching natural language understanding (NLU) tasks as NLG achieves satisfying performance in the general domain through constrained language Cites: Don t stop pretraining: adapt language models to domains and tasks