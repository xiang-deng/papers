---
layout: post
title:  "Dialogue-adaptive Language Model Pre-training From Quality Estimation"
date:   2022-10-24 23:22:19 -0400
categories: jekyll update
author: "J Li, Z Zhang, H Zhao - Neurocomputing, 2022"
---
Pre-trained language models (PrLMs) have achieved great success on a wide range of natural language processing tasks by virtue of the universal language representation ability obtained by self-supervised learning on a large corpus. These models are pre-trained on standard plain texts with general language model (LM) training objectives, which would be insufficient to model dialogue-exclusive attributes like specificity and informativeness reflected in these tasks that are not explicitly …
Cites: ‪TOD-BERT: Pre-trained natural language understanding for task …‬