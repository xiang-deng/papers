--- 
layout: post 
title: "Cramming: Training a Language Model on a Single GPU in One Day" 
date: 2022-12-30 14:30:10 -0400 
categories: jekyll update 
author: "J Geiping, T Goldstein - arXiv preprint arXiv:2212.14034, 2022" 
--- 
Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single GPU in just one day? We investigate the downstream performance achievable with a transformer-based  Cites: Mobilebert: a compact task-agnostic bert for resource-limited devices