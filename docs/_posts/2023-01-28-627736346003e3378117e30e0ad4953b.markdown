--- 
layout: post 
title: "SuperScaler: Supporting Flexible DNN Parallelization via a Unified Abstraction" 
date: 2023-01-28 04:04:00 -0400 
categories: jekyll update 
author: "Z Lin, Y Miao, G Liu, X Shi, Q Zhang, F Yang, S Maleki - arXiv preprint arXiv , 2023" 
--- 
With the growing model size, deep neural networks (DNN) are increasingly trained over massive GPU accelerators, which demands a proper parallelization plan that transforms a DNN model into fine-grained tasks and then schedules them to GPUs for execution. Due to the large search space, the contemporary parallelization plan generators often rely on empirical rules that couple transformation and scheduling, and fall short in exploring more flexible schedules that yield better memory usage Cites: Palm: Scaling language modeling with pathways