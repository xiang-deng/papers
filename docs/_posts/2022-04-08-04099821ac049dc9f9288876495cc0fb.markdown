---
layout: post
title:  "Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension"
date:   2022-04-08 14:57:15 -0400
categories: jekyll update
author: "S Wu, X Zhang, D Xiong, S Chen, Z Zhuang, Z Feng - arXiv preprint arXiv , 2022"
---
Multilingual pre-trained models are able to zero-shot transfer knowledge from rich- resource to low-resource languages in machine reading comprehension (MRC). However, inherent linguistic discrepancies in different languages could make answer spans predicted by zero-shot transfer violate syntactic constraints of the target language. In this paper, we propose a novel multilingual MRC framework equipped with a Siamese Semantic Disentanglement Model (SSDM) to disassociate semantics Cites: MLQA: Evaluating cross-lingual extractive question answering