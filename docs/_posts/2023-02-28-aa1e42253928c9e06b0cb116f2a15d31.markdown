--- 
layout: post 
title: "On the Generalization Ability of Retrieval-Enhanced Transformers" 
date: 2023-02-28 01:22:42 -0400 
categories: jekyll update 
author: "T Norlund, E Doostmohammadi, R Johansson - arXiv preprint arXiv , 2023" 
--- 
Recent work on the Retrieval-Enhanced Transformer (RETRO) model has shown that off-loading memory from trainable weights to a retrieval database can significantly improve language modeling and match the performance of non-retrieval models that are an order of magnitude larger in size. It has been suggested that at least some of this performance gain is due to non-trivial generalization based on both model weights and retrieval. In this paper, we try to better understand the  Cites: REALM: Retrieval-Augmented Language Model Pre-Training