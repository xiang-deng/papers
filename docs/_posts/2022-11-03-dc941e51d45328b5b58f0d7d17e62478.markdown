---
layout: post
title:  "Stop Measuring Calibration When Humans Disagree"
date:   2022-11-03 01:42:13 -0400
categories: jekyll update
author: "J Baan, W Aziz, B Plank, R Fernandez - arXiv preprint arXiv:2210.16133, 2022"
---
Calibration is a popular framework to evaluate whether a classifier knows when it does not know-ie, its predictive probabilities are a good indication of how likely a prediction is to be correct. Correctness is commonly estimated against the human majority class. Recently, calibration to human majority has been measured on tasks where humans inherently disagree about which class applies. We show that measuring calibration to human majority given inherent disagreements is …
Cites: ‪On the Effects of Transformer Size on In-and Out-of-Domain …‬