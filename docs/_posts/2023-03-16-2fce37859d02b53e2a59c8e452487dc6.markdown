--- 
layout: post 
title: "Improving Transformers capabilities in Commonsense Reasoning" 
date: 2023-03-16 06:48:33 -0400 
categories: jekyll update 
author: "Y Zhou, Y Han, H Zhou, GY Wu" 
--- 
Masked language models, such as BERT (Devlin et al., 2019) and its further variations (He et al., 2021b; Liu et al., 2019; Lewis et al., 2020), are models pretrained by masking some tokens in a sentence and trying to predict the masked tokens. They provide solid foundations for finetuning on common natural language processing (NLP) tasks. Endowing computers with human-like commonsense knowledge has remained a challenge of NLP for decades (Sap et al., 2020)  Cites: Commonsense reasoning for natural language processing