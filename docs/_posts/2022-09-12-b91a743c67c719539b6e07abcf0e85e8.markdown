--- 
layout: post 
title: "The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-Based Sentiment Analysis and Emotion Detection" 
date: 2022-09-12 23:50:28 -0400 
categories: jekyll update 
author: "R Mao, Q Liu, K He, W Li, E Cambria - IEEE Transactions on Affective Computing, 2022" 
--- 
Thanks to the breakthrough of large-scale pre-trained language model (PLM) technology, prompt-based classification tasks, eg, sentiment analysis and emotion detection, have raised increasing attention. Such tasks are formalized as masked language prediction tasks which are in line with the pre-training objects of most language models. Thus, one can use a PLM to infer the masked words in a downstream task, then obtaining label predictions with manually defined label-word Cites: Deep contextualized word representations