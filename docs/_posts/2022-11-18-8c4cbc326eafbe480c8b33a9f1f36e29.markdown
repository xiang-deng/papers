--- 
layout: post 
title: "REPAIR: REnormalizing Permuted Activations for Interpolation Repair" 
date: 2022-11-18 16:55:42 -0400 
categories: jekyll update 
author: "K Jordan, H Sedghi, O Saukh, R Entezari, B Neyshabur - arXiv preprint arXiv , 2022" 
--- 
In this paper we look into the conjecture of Entezari et al.(2021) which states that if the permutation invariance of neural networks is taken into account, then there is likely no loss barrier to the linear interpolation between SGD solutions. First, we observe that neuron alignment methods alone are insufficient to establish low-barrier linear connectivity between SGD solutions due to a phenomenon we call variance collapse: interpolated deep networks suffer a collapse in the variance of their  Cites: Patching open-vocabulary models by interpolating weights