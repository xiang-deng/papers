--- 
layout: post 
title: "Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation" 
date: 2022-10-26 13:20:27 -0400 
categories: jekyll update 
author: "Z Wang, Y Wu, F Liu, D Liu, L Hou, H Yu, J Li, H Ji - arXiv preprint arXiv:2210.11768, 2022" 
--- 
Knowledge distillation is one of the primary methods of transferring knowledge from large to small models. However, it requires massive task-specific data, which may not be plausible in many real-world applications. Data augmentation methods such as representation interpolation, token replacement, or augmentation with models are applied to tackle this problem. However, these data augmentation methods either potentially cause shifts in decision boundaries (representation interpolation), are not Cites: Mobilebert: a compact task-agnostic bert for resource-limited devices