---
layout: post
title:  "Hyena Hierarchy: Towards Larger Convolutional Language Models"
date:   2023-02-25 03:28:56 -0400
categories: jekyll update
author: "M Poli, S Massaroli, E Nguyen, DY Fu, T Dao, S Baccus… - arXiv preprint arXiv …, 2023"
---
Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose …
Cites: ‪Holistic evaluation of language models‬