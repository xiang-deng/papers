---
layout: post
title:  "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights"
date:   2022-10-15 02:59:22 -0400
categories: jekyll update
author: "HH Mao - arXiv preprint arXiv:2210.04243, 2022"
---
Autoregressive Transformers are strong language models but incur O (T) complexity during per-token generation due to the self-attention mechanism. Recent work proposes kernel-based methods to approximate causal self-attention by replacing it with recurrent formulations with various update rules and feature maps to achieve O (1) time and memory complexity. We explore these approaches and find that they are unnecessarily complex, and propose a simple alternative-decaying fast weights-that …
Cites: ‪Simple Local Attentions Remain Competitive for Long-Context Tasks‬