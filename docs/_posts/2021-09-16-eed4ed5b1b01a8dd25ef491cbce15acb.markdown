---
layout: post
title:  "Block Pruning For Faster Transformers"
date:   2021-09-16 19:19:30 -0400
categories: jekyll update
author: "F Lagunas, E Charlaix, V Sanh, AM Rush - arXiv preprint arXiv:2109.04838, 2021"
---
Pre-training has improved model accuracy for both classification and generation tasks at the cost of introducing much larger and slower models. Pruning methods have proven to be an effective way of reducing model size, whereas distillation methods are proven for speeding up inference. We introduce a block pruning approach targeting both small and fast models. Our approach extends structured methods by considering blocks of any size and integrates this structure into the Cites: Know what you don t know: Unanswerable questions for SQuAD