--- 
layout: post 
title: "DiMS: Distilling Multiple Steps of Iterative Non-Autoregressive Transformers for Machine Translation" 
date: 2023-06-08 03:52:18 -0400 
categories: jekyll update 
author: "S Norouzi, AI Layer, R Hosseinzadeh, F Prez" 
--- 
The computational benefits of iterative nonautoregressive transformers decrease as the number of decoding steps increases. As a remedy, we introduce Distill Multiple Steps (DiMS), a simple yet effective distillation technique to decrease the number of required steps to reach a certain translation quality. The distilled model enjoys the computational benefits of early iterations while preserving the enhancements from several iterative steps. DiMS relies on two models namely student and teacher. The  Cites: Iterative refinement in the continuous space for non-autoregressive