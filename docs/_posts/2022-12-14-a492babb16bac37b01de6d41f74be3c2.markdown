---
layout: post
title:  "Predictor networks and stop-grads provide implicit variance regularization in BYOL/SimSiam"
date:   2022-12-14 16:04:21 -0400
categories: jekyll update
author: "MS Halvagal, A Laborieux, F Zenke - arXiv preprint arXiv:2212.04858, 2022"
---
Self-supervised learning (SSL) learns useful representations from unlabelled data by training networks to be invariant to pairs of augmented versions of the same input. Non-contrastive methods avoid collapse either by directly regularizing the covariance matrix of network outputs or through asymmetric loss architectures, two seemingly unrelated approaches. Here, by building on DirectPred, we lay out a theoretical framework that reconciles these two views. We derive analytical …
Cites: ‪A framework for contrastive self-supervised learning and designing …‬