---
layout: post
title:  "Energy-efficient Online Scheduling of Transformer Inference Services on GPU Servers"
date:   2022-05-12 04:06:18 -0400
categories: jekyll update
author: "Y Wang, Q Wang, X Chu - IEEE Transactions on Green Communications and , 2022"
---
Cloud service providers are deploying Transformer-based deep learning models on GPU servers to support many online inference-as-a-service (IAAS) applications, given the predominant performance of Transformers in natural language processing (NLP) tasks. However, Transformers  inherent high complexity and large model size (eg, billions to hundreds of billions of parameters) tax the resource-constrained GPU servers. Improving the energy efficiency and payload capability of IAAS without Cites: BERT: Pre-training of Deep Bidirectional Transformers for