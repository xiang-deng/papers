---
layout: post
title:  "Depth-Wise Attention (DWAtt): A Layer Fusion Method for Data-Efficient Classification"
date:   2022-10-06 01:25:19 -0400
categories: jekyll update
author: "M ElNokrashy, B AlKhamissi, M Diab - arXiv preprint arXiv:2209.15168, 2022"
---
Language Models pretrained on large textual data have been shown to encode different types of knowledge simultaneously. Traditionally, only the features from the last layer are used when adapting to new tasks or data. We put forward that, when using or finetuning deep pretrained models, intermediate layer features that may be relevant to the downstream task are buried too deep to be used efficiently in terms of needed samples or steps. To test this, we propose a new layer fusion method: Depth …
Cites: ‪RoBERTa: A Robustly Optimized BERT Pretraining Approach‬