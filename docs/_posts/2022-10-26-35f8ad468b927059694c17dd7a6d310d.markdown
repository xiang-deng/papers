--- 
layout: post 
title: "Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning" 
date: 2022-10-26 13:20:27 -0400 
categories: jekyll update 
author: "X Peng, C Xing, PK Choubey, CS Wu, C Xiong - arXiv preprint arXiv:2210.12587, 2022" 
--- 
Prompt tuning approaches, which learn task-specific soft prompts for a downstream task conditioning on frozen pre-trained models, have attracted growing interest due to its parameter efficiency. With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning. In this work, we focus on improving the few-shot performance Cites: Avoiding the hypothesis-only bias in natural language inference