--- 
layout: post 
title: "Can Language Representation Models Think in Bets?" 
date: 2022-10-20 02:20:28 -0400 
categories: jekyll update 
author: "Z Tang, M Kejriwal - arXiv preprint arXiv:2210.07519, 2022" 
--- 
In recent years, transformer-based language representation models (LRMs) have achieved state-of-the-art results on difficult natural language understanding problems, such as question answering and text summarization. As these models are integrated into real-world applications, evaluating their ability to make rational decisions is an important research agenda, with practical ramifications. This article investigates LRMs rational decision-making ability through a carefully designed set  Cites: Palm: Scaling language modeling with pathways