--- 
layout: post 
title: "Dynamic Masking Rate Schedules for MLM Pretraining" 
date: 2023-05-27 10:00:59 -0400 
categories: jekyll update 
author: "Z Ankner, N Saphra, D Blalock, J Frankle, ML Leavitt - arXiv preprint arXiv , 2023" 
--- 
Most works on transformers trained with the Masked Language Modeling (MLM) objective use the original BERT model s fixed masking rate of 15%. Our work instead dynamically schedules the masking ratio throughout training. We found that linearly decreasing the masking rate from 30% to 15% over the course of pretraining improves average GLUE accuracy by 0.46% in BERT-base, compared to a standard 15% fixed rate. Further analyses demonstrate that the gains from scheduling come  Cites: Should You Mask 15% in Masked Language Modeling?