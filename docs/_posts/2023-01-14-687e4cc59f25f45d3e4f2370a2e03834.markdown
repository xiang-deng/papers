--- 
layout: post 
title: "Data Distillation: A Survey" 
date: 2023-01-14 01:50:54 -0400 
categories: jekyll update 
author: "N Sachdeva, J McAuley - arXiv preprint arXiv:2301.04272, 2023" 
--- 
The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time;(b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements Cites: Lamda: Language models for dialog applications