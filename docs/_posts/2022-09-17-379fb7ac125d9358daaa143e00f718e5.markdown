--- 
layout: post 
title: "On The Computational Complexity of Self-Attention" 
date: 2022-09-17 00:49:30 -0400 
categories: jekyll update 
author: "FD Keles, PM Wijewardena, C Hegde - arXiv preprint arXiv:2209.04881, 2022" 
--- 
Transformer architectures have led to remarkable progress in many state-of-art applications. However, despite their successes, modern transformers rely on the self-attention mechanism, whose time-and space-complexity is quadratic in the length of the input. Several approaches have been proposed to speed up self-attention mechanisms to achieve sub-quadratic running time; however, the large majority of these works are not accompanied by rigorous error guarantees. In this work, we  Cites: Random feature attention