--- 
layout: post 
title: "Semantic projection recovers rich human knowledge of multiple object features from word embeddings" 
date: 2022-04-19 07:59:02 -0400 
categories: jekyll update 
author: "G Grand, IA Blank, F Pereira, E Fedorenko - Nature Human Behaviour, 2022" 
--- 
How is knowledge about word meaning represented in the mental lexicon? Current computational models infer word meanings from lexical co-occurrence patterns. They learn to represent words as vectors in a multidimensional space, wherein words that are used in more similar linguistic contextsthat is, are more semantically related are located closer together. However, whereas inter-word proximity captures only overall relatedness, human judgements are highly context dependent. For example Cites: BERT: Pre-training of Deep Bidirectional Transformers for