---
layout: post
title:  "Pre-training language model incorporating domain-specific heterogeneous knowledge into a unified representation"
date:   2022-12-03 01:42:11 -0400
categories: jekyll update
author: "H Zhu, H Peng, Z Lyu, L Hou, J Li, J Xiao - Expert Systems with Applications, 2022"
---
Existing technologies expand BERT from different perspectives, eg designing different pre-training tasks, different semantic granularities, and different model architectures. Few models consider expanding BERT from different text formats. In this paper, we propose a heterogeneous knowledge language model (HKLM), a unified pre-trained language model (PLM) for all forms of text, including unstructured text, semi-structured text, and well-structured text. To capture the corresponding …
Cites: ‪TaBERT: Pretraining for Joint Understanding of Textual and …‬