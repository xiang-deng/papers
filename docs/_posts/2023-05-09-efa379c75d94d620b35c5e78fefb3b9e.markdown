---
layout: post
title:  "BranchNorm: Robustly Scaling Extremely Deep Transformers"
date:   2023-05-09 11:33:00 -0400
categories: jekyll update
author: "Y Liu, X Zeng, F Meng, J Zhou - arXiv preprint arXiv:2305.02790, 2023"
---
Recently, DeepNorm scales Transformers into extremely deep (ie, 1000 layers) and reveals the promising potential of deep scaling. To stabilize the training of deep models, DeepNorm (Wang et al., 2022) attempts to constrain the model update to a constant value. Although applying such a constraint can benefit the early stage of model training, it may lead to undertrained models during the whole training procedure. In this paper, we propose BranchNorm, which dynamically rescales the …
Cites: ‪Improved Zero-shot Neural Machine Translation via Ignoring …‬