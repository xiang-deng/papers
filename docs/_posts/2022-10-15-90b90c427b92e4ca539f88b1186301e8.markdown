--- 
layout: post 
title: "Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation Approach" 
date: 2022-10-15 02:59:22 -0400 
categories: jekyll update 
author: "P Mi, L Shen, T Ren, Y Zhou, X Sun, R Ji, D Tao - arXiv preprint arXiv:2210.05177, 2022" 
--- 
Deep neural networks often suffer from poor generalization caused by complex and non-convex loss landscapes. One of the popular solutions is Sharpness-Aware Minimization (SAM), which smooths the loss landscape via minimizing the maximized change of training loss when adding a perturbation to the weight. However, we find the indiscriminate perturbation of SAM on all parameters is suboptimal, which also results in excessive computation, ie, double the overhead of Cites: The break-even point on optimization trajectories of deep neural