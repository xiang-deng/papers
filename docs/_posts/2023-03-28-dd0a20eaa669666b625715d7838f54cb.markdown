--- 
layout: post 
title: "Visual-Language Prompt Tuning with Knowledge-guided Context Optimization" 
date: 2023-03-28 04:46:22 -0400 
categories: jekyll update 
author: "H Yao, R Zhang, C Xu - arXiv preprint arXiv:2303.13283, 2023" 
--- 
Prompt tuning is an effective way to adapt the pre-trained visual-language model (VLM) to the downstream task using task-related textual tokens. Representative CoOp-based work combines the learnable textual tokens with the class tokens to obtain specific textual knowledge. However, the specific textual knowledge is the worse generalization to the unseen classes because it forgets the essential general textual knowledge having a strong generalization ability. To tackle this issue, we Cites: Unifying Vision-and-Language Tasks via Text Generation