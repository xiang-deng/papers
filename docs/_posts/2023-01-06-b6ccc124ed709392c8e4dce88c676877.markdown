---
layout: post
title:  "Backdoor Attacks Against Dataset Distillation"
date:   2023-01-06 12:31:49 -0400
categories: jekyll update
author: "Y Liu, Z Li, M Backes, Y Shen, Y Zhang - arXiv preprint arXiv:2301.01197, 2023"
---
Dataset distillation has emerged as a prominent technique to improve data efficiency when training machine learning models. It encapsulates the knowledge from a large dataset into a smaller synthetic dataset. A model trained on this smaller distilled dataset can attain comparable performance to a model trained on the original training dataset. However, the existing dataset distillation techniques mainly aim at achieving the best trade-off between resource usage efficiency and model utility. The …
Cites: ‪Green AI‬