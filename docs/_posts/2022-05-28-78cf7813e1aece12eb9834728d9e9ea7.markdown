---
layout: post
title:  "Utilizing Language-Image Pretraining for Efficient and Robust Bilingual Word Alignment"
date:   2022-05-28 02:05:27 -0400
categories: jekyll update
author: "T Dinh, J Sohn, S Rajput, T Ossowski, Y Ming, J Hu… - arXiv preprint arXiv …, 2022"
---
Word translation without parallel corpora has become feasible, rivaling the performance of supervised methods. Recent findings have shown that the accuracy and robustness of unsupervised word translation (UWT) can be improved by making use of visual observations, which are universal representations across languages. In this work, we investigate the potential of using not only visual observations but also pretrained language-image models for enabling a more efficient and robust UWT … Cites: ‪XTREME: A massively multilingual multi-task benchmark for …‬