---
layout: post
title:  "Diverse Weight Averaging for Out-of-Distribution Generalization"
date:   2022-05-24 00:00:36 -0400
categories: jekyll update
author: "A Rame, M Kirchmeyer, T Rahier, A Rakotomamonjy - arXiv preprint arXiv , 2022"
---
Standard neural networks struggle to generalize under distribution shifts. For out-of-distribution generalization in computer vision, the best current approach averages the weights along a training run. In this paper, we propose Diverse Weight Averaging (DiWA) that makes a simple change to this strategy: DiWA averages the weights obtained from several independent training runs rather than from a single run. Perhaps surprisingly, averaging these weights performs well under soft constraints  Cites: Fine-tuning can distort pretrained features and underperform out-of