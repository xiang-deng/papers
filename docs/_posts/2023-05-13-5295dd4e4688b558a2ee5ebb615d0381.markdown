--- 
layout: post 
title: "What should be encoded by position embedding for neural network language models?" 
date: 2023-05-13 06:32:20 -0400 
categories: jekyll update 
author: "S Yu, Z Zhang, H Liu - Natural Language Engineering, 2023" 
--- 
Word order is one of the most important grammatical devices and the basis for language understanding. However, as one of the most popular NLP architectures, Transformer does not explicitly encode word order. A solution to this problem is to incorporate position information by means of position encoding/embedding (PE). Although a variety of methods of incorporating position information have been proposed, the NLP community is still in want of detailed statistical researches on  Cites: Deep contextualized word representations