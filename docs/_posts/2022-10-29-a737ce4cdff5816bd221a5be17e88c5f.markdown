--- 
layout: post 
title: "On Fine-Tuned Deep Features for Unsupervised Domain Adaptation" 
date: 2022-10-29 01:49:44 -0400 
categories: jekyll update 
author: "Q Wang, TP Breckon - arXiv preprint arXiv:2210.14083, 2022" 
--- 
Prior feature transformation based approaches to Unsupervised Domain Adaptation (UDA) employ the deep features extracted by pre-trained deep models without fine-tuning them on the specific source or target domain data for a particular domain adaptation task. In contrast, end-to-end learning based approaches optimise the pre-trained backbones and the customised adaptation modules simultaneously to learn domain-invariant features for UDA. In this work, we explore the potential of  Cites: Fine-tuning can distort pretrained features and underperform out-of