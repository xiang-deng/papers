---
layout: post
title:  "Residual Learning of Neural Text Generation with $ n $-gram Language Model"
date:   2022-10-29 01:49:44 -0400
categories: jekyll update
author: "H Li, D Cai, J Xu, T Watanabe - arXiv preprint arXiv:2210.14431, 2022"
---
$ N $-gram language models (LM) have been largely superseded by neural LMs as the latter exhibits better performance. However, we find that $ n $-gram models can achieve satisfactory performance on a large proportion of testing cases, indicating they have already captured abundant knowledge of the language with relatively low computational cost. With this observation, we propose to learn a neural LM that fits the residual between an $ n $-gram LM and the real-data distribution. The …
Cites: ‪Text summarization with pretrained encoders‬