---
layout: post
title:  "Complexity of symbolic representation in working memory of transformer correlates with the complexity of a task"
date:   2022-06-06 21:51:57 -0400
categories: jekyll update
author: "A Sagirova, M Burtsev - Cognitive Systems Research, 2022"
---
Even though Transformers are extensively used for Natural Language Processing tasks, especially for machine translation, they lack an explicit memory to store key concepts of processed texts. This paper explores the properties of the content of symbolic working memory added to the Transformer model decoder. Such working memory enhances the quality of model predictions in machine translation task and works as a neural-symbolic representation of information that is important for the 
Cites: Dynamic neural turing machine with continuous and discrete