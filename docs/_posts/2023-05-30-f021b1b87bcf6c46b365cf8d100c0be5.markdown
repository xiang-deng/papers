--- 
layout: post 
title: "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models" 
date: 2023-05-30 03:09:06 -0400 
categories: jekyll update 
author: "H Duan, A Dziedzic, N Papernot, F Boenisch - arXiv preprint arXiv:2305.15594, 2023" 
--- 
Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the Cites: Memorization without overfitting: Analyzing the training dynamics