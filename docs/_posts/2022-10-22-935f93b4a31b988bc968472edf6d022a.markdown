---
layout: post
title:  "Self-supervised Graph Masking Pre-training for Graph-to-Text Generation"
date:   2022-10-22 02:20:44 -0400
categories: jekyll update
author: "J Han, E Shareghi - arXiv preprint arXiv:2210.10599, 2022"
---
Large-scale pre-trained language models (PLMs) have advanced Graph-to-Text (G2T) generation by processing the linearised version of a graph. However, the linearisation is known to ignore the structural information. Additionally, PLMs are typically pre-trained on free text which introduces domain mismatch between pre-training and downstream G2T generation tasks. To address these shortcomings, we propose graph masking pre-training strategies that neither require supervision …
Cites: ‪Dart: Open-domain structured data record to text generation‬