---
layout: post
title:  "Mimicking Infants  Bilingual Language Acquisition for Domain Specialized Neural Machine Translation"
date:   2022-04-23 07:54:44 -0400
categories: jekyll update
author: "C Park, WY Go, S Eo, H Moon, S Lee, H Lim - IEEE Access, 2022"
---
Existing methods of training domain-specialized neural machine translation (DS- NMT) models are based on the pretrain-finetuning approach (PFA). In this study, we reinterpret existing methods based on the perspective of cognitive science related to cross language speech perception. We propose the cross communication method (CCM), a new DS-NMT training approach. Inspired by the learning method of infants, we perform DS-NMT training by configuring and training DC and GC concurrently in Cites: Fine-tuning pretrained language models: Weight initializations