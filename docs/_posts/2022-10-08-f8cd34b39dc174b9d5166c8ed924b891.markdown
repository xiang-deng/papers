--- 
layout: post 
title: "Calibrating Sequence likelihood Improves Conditional Language Generation" 
date: 2022-10-08 00:45:41 -0400 
categories: jekyll update 
author: "Y Zhao, M Khalman, R Joshi, S Narayan, M Saleh - arXiv preprint arXiv , 2022" 
--- 
Conditional language models are predominantly trained with maximum likelihood estimation (MLE), giving probability mass to sparsely observed target sequences. While MLE trained models assign high probability to plausible sequences given the context, the model probabilities often do not accurately rank-order generated sequences by quality. This has been empirically observed in beam search decoding as output quality degrading with large beam sizes, and decoding strategies  Cites: Towards a human-like open-domain chatbot