--- 
layout: post 
title: "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning" 
date: 2023-05-19 23:52:25 -0400 
categories: jekyll update 
author: "H Chen, Y Zhang, Q Zhang, H Yang, X Hu, X Ma - arXiv preprint arXiv , 2023" 
--- 
Instruction tuning for large language models (LLMs) has gained attention from researchers due to its ability to unlock the potential of LLMs in following instructions. While instruction tuning offers advantages for facilitating the adaptation of large language models (LLMs) to downstream tasks as a fine-tuning approach, training models with tens of millions or even billions of parameters on large amounts of data results in unaffordable computational costs. To address this, we focus on reducing Cites: HINT: Hypernetwork Instruction Tuning for Efficient Zero-Shot