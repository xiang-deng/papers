--- 
layout: post 
title: "LexMAE: Lexicon-Bottlenecked Pretraining for Large-Scale Retrieval" 
date: 2022-09-05 21:46:44 -0400 
categories: jekyll update 
author: "T Shen, X Geng, C Tao, C Xu, X Huang, B Jiao, L Yang - arXiv preprint arXiv , 2022" 
--- 
In large-scale retrieval, the lexicon-weighting paradigm, learning weighted sparse representations in vocabulary space, has shown promising results with high quality and low latency. Despite it deeply exploiting the lexicon-representing capability of pre-trained language models, a crucial gap remains between language modeling and lexicon-weighting retrieval--the former preferring certain or low-entropy words whereas the latter favoring pivot or high-entropy words--becoming the main barrier to Cites: Latent retrieval for weakly supervised open domain question