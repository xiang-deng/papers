--- 
layout: post 
title: "Learn To Remember: Transformer with Recurrent Memory for Document-Level Machine Translation" 
date: 2022-05-07 02:52:45 -0400 
categories: jekyll update 
author: "Y Feng, F Li, Z Song, B Zheng, P Koehn - arXiv preprint arXiv:2205.01546, 2022" 
--- 
The Transformer architecture has led to significant gains in machine translation. However, most studies focus on only sentence-level translation without considering the context dependency within documents, leading to the inadequacy of document- level coherence. Some recent research tried to mitigate this issue by introducing an additional context encoder or translating with multiple sentences or even the entire document. Such methods may lose the information on the target side or have an Cites: Blockwise Self-Attention for Long Document Understanding