--- 
layout: post 
title: "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned" 
date: 2022-09-08 01:52:02 -0400 
categories: jekyll update 
author: "D Ganguli, L Lovitt, J Kernion, A Askell, Y Bai" 
--- 
We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7 B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using  Cites: On the opportunities and risks of foundation models