---
layout: post
title:  "Big Little Transformer Decoder"
date:   2023-02-18 05:28:11 -0400
categories: jekyll update
author: "S Kim, K Mangalam, J Malik, MW Mahoney, A Gholami… - arXiv preprint arXiv …, 2023"
---
The recent emergence of Large Language Models based on the Transformer architecture has enabled dramatic advancements in the field of Natural Language Processing. However, these models have long inference latency, which limits their deployment, and which makes them prohibitively expensive for various real-time applications. The inference latency is further exacerbated by autoregressive generative tasks, as models need to run iteratively to generate tokens sequentially …
Cites: ‪Deep encoder, shallow decoder: Reevaluating non-autoregressive …‬