--- 
layout: post 
title: "In defense of dual-encoders for neural ranking" 
date: 2022-07-14 01:37:31 -0400 
categories: jekyll update 
author: "A Menon, S Jayasumana, AS Rawat, S Kim, S Reddi - International Conference on , 2022" 
--- 
Transformer-based models such as BERT have proven successful in information retrieval problem, which seek to identify relevant documents for a given query. There are two broad flavours of such models: cross-attention (CA) models, which learn a joint embedding for the query and document, and dual-encoder (DE) models, which learn separate embeddings for the query and document. Empirically, CA models are often found to be more accurate, which has motivated a series of works seeking to  Cites: Dense Passage Retrieval for Open-Domain Question Answering