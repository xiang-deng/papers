---
layout: post
title:  "XTab: Cross-table Pretraining for Tabular Transformers"
date:   2023-05-13 06:32:20 -0400
categories: jekyll update
author: "B Zhu, X Shi, N Erickson, M Li, G Karypis, M Shoaran - arXiv preprint arXiv …, 2023"
---
The success of self-supervised learning in computer vision and natural language processing has motivated pretraining methods on tabular data. However, most existing tabular self-supervised learning models fail to leverage information across multiple data tables and cannot generalize to new tables. In this work, we introduce XTab, a framework for cross-table pretraining of tabular transformers on datasets from various domains. We address the challenge of inconsistent column types and …
Cites: ‪TaBERT: Pretraining for Joint Understanding of Textual and …‬