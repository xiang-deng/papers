--- 
layout: post 
title: "Differentially Private Bias-Term only Fine-tuning of Foundation Models" 
date: 2022-10-08 00:45:41 -0400 
categories: jekyll update 
author: "Z Bu, YX Wang, S Zha, G Karypis - arXiv preprint arXiv:2210.00036, 2022" 
--- 
We study the problem of differentially private (DP) fine-tuning of large pre-trained models--a recent privacy-preserving approach suitable for solving downstream tasks with sensitive data. Existing work has demonstrated that high accuracy is possible under strong privacy constraint, yet requires significant computational overhead or modifications to the network architecture. We propose differentially private bias-term fine-tuning (DP-BiTFiT), which matches the state-of-the-art accuracy for DP Cites: Towards a unified view of parameter-efficient transfer learning