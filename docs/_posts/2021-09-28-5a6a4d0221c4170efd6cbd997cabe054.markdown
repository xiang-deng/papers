---
layout: post
title:  "K-AID: Enhancing Pre-trained Language Models with Domain Knowledge for Question Answering"
date:   2021-09-28 14:54:04 -0400
categories: jekyll update
author: "F Sun, FL Li, R Wang, Q Chen, X Cheng, J Zhang - arXiv preprint arXiv:2109.10547, 2021"
---
Knowledge enhanced pre-trained language models (K-PLMs) are shown to be effective for many public tasks in the literature but few of them have been successfully applied in practice. To address this problem, we propose K-AID, a systematic approach that includes a low-cost knowledge acquisition process for acquiring domain knowledge, an effective knowledge infusion module for improving model performance, and a knowledge distillation component for reducing the model Cites: Entities as experts: Sparse memory access with entity supervision