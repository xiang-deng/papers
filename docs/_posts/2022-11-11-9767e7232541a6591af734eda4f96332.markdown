--- 
layout: post 
title: "COPEN: Probing Conceptual Knowledge in Pre-trained Language Models" 
date: 2022-11-11 23:39:32 -0400 
categories: jekyll update 
author: "H Peng, X Wang, S Hu, H Jin, L Hou, J Li, Z Liu, Q Liu - arXiv preprint arXiv , 2022" 
--- 
Conceptual knowledge is fundamental to human cognition and knowledge bases. However, existing knowledge probing works only focus on evaluating factual knowledge of pre-trained language models (PLMs) and ignore conceptual knowledge. Since conceptual knowledge often appears as implicit commonsense behind texts, designing probes for conceptual knowledge is hard. Inspired by knowledge representation schemata, we comprehensively evaluate conceptual Cites: Language models as knowledge bases?