---
layout: post
title:  "Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers"
date:   2022-11-24 01:38:18 -0400
categories: jekyll update
author: "Z Yao, X Wu, C Li, C Holmes, M Zhang, C Li, Y He - arXiv preprint arXiv:2211.11586, 2022"
---
Large-scale transformer models have become the de-facto architectures for various machine learning applications, eg, CV and NLP. However, those large models also introduce prohibitive training costs. To mitigate this issue, we propose a novel random and layerwise token dropping method (random-LTD), which skips the computation of a subset of the input tokens at all middle layers. Particularly, random-LTD achieves considerable speedups and comparable accuracy as the standard …
Cites: ‪Train short, test long: Attention with linear biases enables input …‬