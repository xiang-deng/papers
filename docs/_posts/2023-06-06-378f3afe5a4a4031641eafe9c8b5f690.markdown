--- 
layout: post 
title: "Exposing Attention Glitches with Flip-Flop Language Modeling" 
date: 2023-06-06 05:46:58 -0400 
categories: jekyll update 
author: "B Liu, JT Ash, S Goel, A Krishnamurthy, C Zhang - arXiv preprint arXiv:2306.00946, 2023" 
--- 
Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the  Cites: What can transformers learn in-context? a case study of simple