--- 
layout: post 
title: "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining" 
date: 2023-05-19 23:52:25 -0400 
categories: jekyll update 
author: "SM Xie, H Pham, X Dong, N Du, H Liu, Y Lu, P Liang - arXiv preprint arXiv , 2023" 
--- 
The mixture proportions of pretraining data domains (eg, Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full  Cites: Semantic parsing on freebase from question-answer pairs