---
layout: post
title:  "Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding"
date:   2023-05-27 10:00:59 -0400
categories: jekyll update
author: "Y Zhang, H Cheng, Z Shen, X Liu, YY Wang, J Gao - arXiv preprint arXiv:2305.14232, 2023"
---
Scientific literature understanding tasks have gained significant attention due to their potential to accelerate scientific discovery. Pre-trained language models (LMs) have shown effectiveness in these tasks, especially when tuned via contrastive learning. However, jointly utilizing pre-training data across multiple heterogeneous tasks (eg, extreme classification, citation prediction, and literature search) remains largely unexplored. To bridge this gap, we propose a multi-task contrastive learning …
Cites: ‪Task-aware retrieval with instructions‬