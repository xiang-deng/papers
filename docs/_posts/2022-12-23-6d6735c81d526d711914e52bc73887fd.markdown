--- 
layout: post 
title: "Large Language Models Are Reasoning Teachers" 
date: 2022-12-23 23:45:02 -0400 
categories: jekyll update 
author: "N Ho, L Schmid, SY Yun - arXiv preprint arXiv:2212.10071, 2022" 
--- 
Language models (LMs) have demonstrated remarkable performance on downstream tasks, using in-context exemplars or human instructions. Recent works have shown that chain-of-thought (CoT) prompting can elicit models to solve complex reasoning tasks, step-by-step. However, the efficacy of prompt-based CoT methods is restricted to very large LMs such as GPT-3 (175B), thus limiting deployability. In this paper, we revisit the fine-tuning approach to enable complex Cites: CommonsenseQA: A Question Answering Challenge Targeting