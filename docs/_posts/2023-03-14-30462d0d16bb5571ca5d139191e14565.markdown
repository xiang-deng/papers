--- 
layout: post 
title: "Dynamic Stashing Quantization for Efficient Transformer Training" 
date: 2023-03-14 05:28:18 -0400 
categories: jekyll update 
author: "G Yang, D Lo, R Mullins, Y Zhao - arXiv preprint arXiv:2303.05295, 2023" 
--- 
Large Language Models (LLMs) have demonstrated impressive performance on a range of Natural Language Processing (NLP) tasks. Unfortunately, the immense amount of computations and memory accesses required for LLM training makes them prohibitively expensive in terms of hardware cost, and thus challenging to deploy in use cases such as on-device learning. In this paper, motivated by the observation that LLM training is memory-bound, we propose a novel dynamic Cites: Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis