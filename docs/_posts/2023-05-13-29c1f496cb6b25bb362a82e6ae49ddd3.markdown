--- 
layout: post 
title: "MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts" 
date: 2023-05-13 06:32:20 -0400 
categories: jekyll update 
author: "X Li, X Qiu - arXiv preprint arXiv:2305.05181, 2023" 
--- 
Large Language Models have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, human can easily improve themselves by thinking and memory, without external resources. In this paper, we propose a framework, MoT, to let the LLM self-improve through Memory of Thoughts, without annotated datasets and parameter updates. Specifically, the framework is divided  Cites: BoolQ: Exploring the surprising difficulty of natural yes/no questions