--- 
layout: post 
title: "Friend-training: Learning from Models of Different but Related Tasks" 
date: 2023-02-03 14:16:33 -0400 
categories: jekyll update 
author: "M Zhang, L Jin, L Song, H Mi, X Zhou, D Yu - arXiv preprint arXiv:2301.13683, 2023" 
--- 
Current self-training methods such as standard self-training, co-training, tri-training, and others often focus on improving model performance on a single task, utilizing differences in input features, model architectures, and training processes. However, many tasks in natural language processing are about different but related aspects of language, and models trained for one task can be great teachers for other related tasks. In this work, we propose friend-training, a cross-task self-training framework  Cites: Can you unpack that? learning to rewrite questions-in-context