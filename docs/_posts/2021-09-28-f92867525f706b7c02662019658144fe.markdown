--- 
layout: post 
title: "Hierarchical Attention Transformer Networks for Long Document Classification" 
date: 2021-09-28 14:54:04 -0400 
categories: jekyll update 
author: "Y Hu, P Chen, T Liu, J Gao, Y Sun, B Yin - 2021 International Joint Conference on , 2021" 
--- 
Profiting from the pre-trained language representation models like BERT, the recently proposed document classification methods have obtained considerable improvement. However, most of these methods usually model the document as a sequence of text and omit the structure information, which appears obviously in long document composed of several sections with assigned relations. For this purpose, we propose a novel Hierarchical Attention Transformer Network (HATN) for long Cites: Blockwise Self-Attention for Long Document Understanding