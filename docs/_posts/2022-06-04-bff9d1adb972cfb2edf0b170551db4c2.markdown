---
layout: post
title:  "Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning"
date:   2022-06-04 01:43:25 -0400
categories: jekyll update
author: "X Chen, L Li, N Zhang, X Liang, S Deng, C Tan - arXiv preprint arXiv , 2022"
---
Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop  Cites: Knowledgeable Prompt-tuning: Incorporating Knowledge into