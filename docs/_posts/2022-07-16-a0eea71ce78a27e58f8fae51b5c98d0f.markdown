---
layout: post
title:  "Distillation or loss of information?: The effects of distillation on model redundancy"
date:   2022-07-16 11:01:18 -0400
categories: jekyll update
author: "EE Sventickaite - 2022"
---
The necessity for billions of parameters in large language models has lately been questioned as there are still unanswered questions regarding how information is captured in the networks. It could be argued that without this knowledge, there may be a tendency to overparametarize the models. In turn, the investigation of model redundancy and the methods which minimize it is important both to the academic and commercial entities. As such, the two main goals of this project were to, firstly …
Cites: ‪A Systematic Evaluation of Large Language Models of Code‬  