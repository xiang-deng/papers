--- 
layout: post 
title: "Real-World Compositional Generalization with Disentangled Sequence-to-Sequence Learning" 
date: 2022-12-14 16:04:21 -0400 
categories: jekyll update 
author: "H Zheng, M Lapata - arXiv preprint arXiv:2212.05982, 2022" 
--- 
Compositional generalization is a basic mechanism in human language learning, which current neural networks struggle with. A recently proposed Disentangled sequence-to-sequence model (Dangle) shows promising generalization capability by learning specialized encodings for each decoding step. We introduce two key modifications to this model which encourage more disentangled representations and improve its compute and memory efficiency, allowing us to tackle compositional Cites: Compositional generalization for neural semantic parsing via span