--- 
layout: post 
title: "An End-to-End Contrastive Self-Supervised Learning Framework for Language Understanding" 
date: 2022-12-10 20:24:02 -0400 
categories: jekyll update 
author: "H Fang, P Xie - Transactions of the Association for Computational , 2022" 
--- 
Self-supervised learning (SSL) methods such as Word2vec, BERT, and GPT have shown great effectiveness in language understanding. Contrastive learning, as a recent SSL approach, has attracted increasing attention in NLP. Contrastive learning learns data representations by predicting whether two augmented data instances are generated from the same original data example. Previous contrastive learning methods perform data augmentation and contrastive learning separately. As a result  Cites: SimCSE: Simple Contrastive Learning of Sentence Embeddings