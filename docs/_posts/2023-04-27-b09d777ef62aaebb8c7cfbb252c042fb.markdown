--- 
layout: post 
title: "Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models" 
date: 2023-04-27 01:18:20 -0400 
categories: jekyll update 
author: "J Sun, Y Luo, Y Gong, C Lin, Y Shen, J Guo, N Duan - arXiv preprint arXiv , 2023" 
--- 
Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT  Cites: Mathqa: Towards interpretable math word problem solving with