---
layout: post
title:  "BMCook: A Task-agnostic Compression Toolkit for Big Models"
date:   2023-02-11 02:41:58 -0400
categories: jekyll update
author: "Z Zhang, B Gong, Y Chen, X Han, G Zeng, W Zhao… - Proceedings of the The …, 2022"
---
Recently, pre-trained language models (PLMs) have achieved great success on various NLP tasks and have shown a trend of exponential growth in model size. To alleviate the unaffordable computational costs brought by the size growth, model compression has been widely explored. Existing efforts have achieved promising results in compressing medium-sized models for specific tasks, while task-agnostic compression for big models with over billions of parameters is rarely studied. Task …
Cites: ‪Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt …‬