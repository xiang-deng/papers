--- 
layout: post 
title: "Prototype-guided Cross-task Knowledge Distillation for Large-scale Models" 
date: 2022-12-28 14:07:11 -0400 
categories: jekyll update 
author: "D Li, A Wu, Y Han, Q Tian - arXiv preprint arXiv:2212.13180, 2022" 
--- 
Recently, large-scale pre-trained models have shown their advantages in many tasks. However, due to the huge computational complexity and storage requirements, it is challenging to apply the large-scale model to real scenes. A common solution is knowledge distillation which regards the large-scale model as a teacher model and helps to train a small student model to obtain a competitive performance. Cross-task Knowledge distillation expands the application scenarios of  Cites: LXMERT: Learning Cross-Modality Encoder Representations from