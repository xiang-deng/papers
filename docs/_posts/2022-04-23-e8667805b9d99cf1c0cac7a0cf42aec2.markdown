---
layout: post
title:  "mGPT: Few-Shot Learners Go Multilingual"
date:   2022-04-23 07:54:44 -0400
categories: jekyll update
author: "O Shliazhko, A Fenogenova, M Tikhonova, V Mikhailov - arXiv preprint arXiv , 2022"
---
Recent studies report that autoregressive language models can successfully solve many NLP tasks via zero-and few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using GPT-2 sources and the Cites: True few-shot learning with language models