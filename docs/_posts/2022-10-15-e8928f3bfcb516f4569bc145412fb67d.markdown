---
layout: post
title:  "AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models"
date:   2022-10-15 02:59:22 -0400
categories: jekyll update
author: "SJ Kwon, J Kim, J Bae, KM Yoo, JH Kim, B Park, B Kim… - arXiv preprint arXiv …, 2022"
---
There are growing interests in adapting large-scale language models using parameter-efficient fine-tuning methods. However, accelerating the model itself and achieving better inference efficiency through model compression has not been thoroughly explored yet. Model compression could provide the benefits of reducing memory footprints, enabling low-precision computations, and ultimately achieving cost-effective inference. To combine parameter-efficient adaptation and model …
Cites: ‪Palm: Scaling language modeling with pathways‬