--- 
layout: post 
title: "Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer" 
date: 2023-06-01 02:05:49 -0400 
categories: jekyll update 
author: "Y Tian, Y Wang, B Chen, S Du - arXiv preprint arXiv:2305.16380, 2023" 
--- 
Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient\emph {training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next Cites: What learning algorithm is in-context learning? investigations with