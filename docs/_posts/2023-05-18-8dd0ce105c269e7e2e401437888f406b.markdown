---
layout: post
title:  "Knowledge Rumination for Pre-trained Language Models"
date:   2023-05-18 07:22:22 -0400
categories: jekyll update
author: "Y Yao, P Wang, S Mao, C Tan, F Huang, H Chen… - arXiv preprint arXiv …, 2023"
---
Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fails to fully utilize them when applying to knowledge-intensive tasks. In this paper, we propose a new …
Cites: ‪Abductive commonsense reasoning‬