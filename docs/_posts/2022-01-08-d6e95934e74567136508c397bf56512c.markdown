---
layout: post
title:  "ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation"
date:   2022-01-08 08:13:01 -0400
categories: jekyll update
author: "H Zhang, W Yin, Y Fang, L Li, B Duan, Z Wu, Y Sun - arXiv preprint arXiv , 2021"
---
Conventional methods for the image-text generation tasks mainly tackle the naturally bidirectional generation tasks separately, focusing on designing task-specific frameworks to improve the quality and fidelity of the generated samples. Recently, Vision-Language Pre-training models have greatly improved the performance of the image-to-text generation tasks, but large-scale pre-training models for text-to-image synthesis task are still under-developed. In this paper, we propose ERNIE-ViLG, a Cites: X-lxmert: Paint, caption and answer questions with multi-modal