---
layout: post
title:  "Federated Split BERT for Heterogeneous Text Classification"
date:   2022-05-30 22:20:45 -0400
categories: jekyll update
author: "Z Li, S Si, J Wang, J Xiao - arXiv preprint arXiv:2205.13299, 2022"
---
Pre-trained BERT models have achieved impressive performance in many natural language processing (NLP) tasks. However, in many real-world situations, textual data are usually decentralized over many clients and unable to be uploaded to a central server due to privacy protection and regulations. Federated learning (FL) enables multiple clients collaboratively to train a global model while keeping the local data privacy. A few researches have investigated BERT in federated learning … Cites: ‪Texthide: Tackling Data Privacy in Language Understanding Tasks‬