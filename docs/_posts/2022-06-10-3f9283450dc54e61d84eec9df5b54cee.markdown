---
layout: post
title:  "Good, Better, Best: Textual Distractors Generation for Multiple-Choice Visual Question Answering via Reinforcement Learning"
date:   2022-06-10 22:27:43 -0400
categories: jekyll update
author: "J Lu, X Ye, Y Ren, Y Yang - Proceedings of the IEEE/CVF Conference on Computer …, 2022"
---
Multiple-choice VQA has drawn increasing attention from researchers and end-users recently. As the demand for automatically constructing large-scale multiple-choice VQA data grows, we introduce a novel task called textual Distractors Generation for VQA (DG-VQA) focusing on generating challenging yet meaningful distractors given the context image, question, and correct answer. The DG-VQA task aims at generating distractors without ground-truth training samples since such resources …
Cites: ‪Language models as knowledge bases?‬  