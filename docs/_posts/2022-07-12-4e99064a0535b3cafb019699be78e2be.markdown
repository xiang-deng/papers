---
layout: post
title:  "When does SGD favor flat minima? A quantitative characterization via linear stability"
date:   2022-07-12 02:15:42 -0400
categories: jekyll update
author: "L Wu, M Wang, W Su - arXiv preprint arXiv:2207.02628, 2022"
---
The observation that stochastic gradient descent (SGD) favors flat minima has played a fundamental role in understanding implicit regularization of SGD and guiding the tuning of hyperparameters. In this paper, we provide a quantitative explanation of this striking phenomenon by relating the particular noise structure of SGD to its\emph {linear stability}(Wu et al., 2018). Specifically, we consider training over-parameterized models with square loss. We prove that if a global minimum $\theta …
Cites: ‪The break-even point on optimization trajectories of deep neural …‬  