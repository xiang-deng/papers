--- 
layout: post 
title: "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning" 
date: 2022-10-29 01:49:44 -0400 
categories: jekyll update 
author: "Y Wang, S Agarwal, S Mukherjee, X Liu, J Gao" 
--- 
Standard fine-tuning of large pre-trained language models (PLMs) for downstream tasks requires updating hundreds of millions to billions of parameters, and storing a large copy of the PLM weights for every task resulting in increased cost for storing, sharing and serving the models. To address this, parameter-efficient fine-tuning (PEFT) techniques were introduced where small trainable components are injected in the PLM and updated during fine-tuning. We propose AdaMix as a general PEFT  Cites: Unipelt: A unified framework for parameter-efficient language