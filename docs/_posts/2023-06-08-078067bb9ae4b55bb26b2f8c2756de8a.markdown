--- 
layout: post 
title: "BaSFormer: A Balanced Sparsity Regularized Attention Network for Transformer" 
date: 2023-06-08 03:52:18 -0400 
categories: jekyll update 
author: "S Jiang, Q Chen, Y Xiang, Y Pan - 2023" 
--- 
Attention networks often make decisions relying solely on a few pieces of tokens, even if those reliances are not truly indicative of the underlying meaning or intention of the full context. That can lead to over-fitting in Transformers and hinder their ability to generalize. Attention regularization and sparsity-based methods have been used to overcome this issue. However, these methods cannot guarantee that all tokens have sufficient receptive fields for global information inference. Thus, the impact of Cites: Adaptive testing and debugging of nlp models