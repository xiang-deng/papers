--- 
layout: post 
title: "Large Language Models Struggle to Learn Long-Tail Knowledge" 
date: 2022-11-18 16:55:42 -0400 
categories: jekyll update 
author: "N Kandpal, H Deng, A Roberts, E Wallace, C Raffel - arXiv preprint arXiv:2211.08411, 2022" 
--- 
The internet contains a wealth of knowledge--from the birthdays of historical figures to tutorials on how to code--all of which may be learned by language models. However, there is a huge variability in the number of times a given piece of information appears on the web. In this paper, we study the relationship between the knowledge memorized by large language models and the information in their pre-training datasets. In particular, we show that a language model s ability to answer a Cites: Latent retrieval for weakly supervised open domain question