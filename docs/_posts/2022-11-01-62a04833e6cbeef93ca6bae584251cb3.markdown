---
layout: post
title:  "An Empirical Study of Pre-trained Language Models in Simple Knowledge Graph Question Answering"
date:   2022-11-01 03:49:43 -0400
categories: jekyll update
author: "N Hu, Y Wu, G Qi, D Min, J Chen, JZ Pan, Z Ali - 2022"
---
Large-scale pre-trained language models (PLMs) such as BERT have recently achieved great success and become a milestone in natural language processing (NLP). It is now the consensus of the NLP community to adopt PLMs as the backbone for downstream tasks. In recent works on knowledge graph question answering (KGQA), BERT or its variants have become necessary in their KGQA models. However, there is still a lack of comprehensive research and comparison of the …
Cites: ‪Human language understanding & reasoning‬