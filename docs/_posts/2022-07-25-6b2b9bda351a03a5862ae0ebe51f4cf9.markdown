--- 
layout: post 
title: "Beyond neural scaling laws: beating power law scaling via data pruning" 
date: 2022-07-25 21:58:40 -0400 
categories: jekyll update 
author: "B Sorscher, R Geirhos, S Shekhar, S Ganguli - arXiv preprint arXiv , 2022" 
--- 
Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how both in theory and practice we can break beyond power law scaling and reduce it to exponential scaling instead if we have access to a high  Cites: On the opportunities and risks of foundation models