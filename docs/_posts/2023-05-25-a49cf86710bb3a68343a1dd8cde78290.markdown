--- 
layout: post 
title: "Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline" 
date: 2023-05-25 03:51:47 -0400 
categories: jekyll update 
author: "Z Zheng, X Ren, F Xue, Y Luo, X Jiang, Y You - arXiv preprint arXiv:2305.13144, 2023" 
--- 
Large language models (LLMs) have revolutionized the field of AI, demonstrating unprecedented capacity across various tasks. However, the inference process for LLMs comes with significant computational costs. In this paper, we propose an efficient LLM inference pipeline that harnesses the power of LLMs. Our approach begins by tapping into the potential of LLMs to accurately perceive and predict the response length with minimal overhead. By leveraging this information, we introduce  Cites: High-throughput generative inference of large language models