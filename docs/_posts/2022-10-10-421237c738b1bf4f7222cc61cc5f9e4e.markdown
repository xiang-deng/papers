--- 
layout: post 
title: "Guess the Instruction! Making Language Models Stronger Zero-Shot Learners" 
date: 2022-10-10 14:05:52 -0400 
categories: jekyll update 
author: "S Ye, D Kim, J Jang, J Shin, M Seo - arXiv preprint arXiv:2210.02969, 2022" 
--- 
Meta-training, which fine-tunes the language model (LM) on various downstream tasks by maximizing the likelihood of the target label given the task instruction and input instance, has improved the zero-shot task generalization performance. However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training. In this paper, we propose Flipped Learning, an alternative method of meta-training which trains the LM to Cites: Beyond the Imitation Game: Quantifying and extrapolating the