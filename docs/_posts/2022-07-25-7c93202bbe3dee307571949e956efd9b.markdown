--- 
layout: post 
title: "Low Resource Pipeline for Spoken Language Understanding via Weak Supervision" 
date: 2022-07-25 21:58:40 -0400 
categories: jekyll update 
author: "A Kumar, RK Tripathi, J Vepa - arXiv preprint arXiv:2206.10559, 2022" 
--- 
In Weak Supervised Learning (WSL), a model is trained over noisy labels obtained from semantic rules and task-specific pre-trained models. Rules offer limited generalization over tasks and require significant manual efforts while pre-trained models are available only for limited tasks. In this work, we propose to utilize prompt-based methods as weak sources to obtain the noisy labels on unannotated data. We show that task-agnostic prompts are generalizable and can be used to obtain noisy Cites: Making Pre-trained Language Models Better Few-shot Learners