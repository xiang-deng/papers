---
layout: post
title:  "AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models"
date:   2023-01-10 01:37:31 -0400
categories: jekyll update
author: "JA Hernández López, M Weyssow, JS Cuadrado… - 37th IEEE/ACM International …, 2022"
---
The objective of pre-trained language models is to learn contextual representations of textual data. Pre-trained language models have become mainstream in natural language processing and code modeling. Using probes, a technique to study the linguistic properties of hidden vector spaces, previous works have shown that these pre-trained language models encode simple linguistic properties in their hidden representations. However, none of the previous work assessed whether these …
Cites: ‪Finding Universal Grammatical Relations in Multilingual BERT‬