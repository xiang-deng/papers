---
layout: post
title:  "RetroMAE: Pre-training Retrieval-oriented Transformers via Masked Auto-Encoder"
date:   2022-05-28 02:05:27 -0400
categories: jekyll update
author: "Z Liu, Y Shao - arXiv preprint arXiv:2205.12035, 2022"
---
Pre-trained models have demonstrated superior power on many important tasks. However, it is still an open problem of designing effective pre-training strategies so as to promote the models  usability on dense retrieval. In this paper, we propose a novel pre-training framework for dense retrieval based on the Masked Auto-Encoder, known as RetroMAE. Our proposed framework is highlighted for the following critical designs: 1) a MAE based pre-training workflow, where the input sentence is polluted  Cites: Towards Unsupervised Dense Information Retrieval with 