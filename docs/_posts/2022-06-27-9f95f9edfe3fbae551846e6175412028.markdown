--- 
layout: post 
title: "reStructured Pre-training" 
date: 2022-06-27 23:23:24 -0400 
categories: jekyll update 
author: "W Yuan, P Liu - arXiv preprint arXiv:2206.11147, 2022" 
--- 
In this work, we try to decipher the internal connection of NLP technology development in the past decades, searching for essence, which rewards us with a (potential) new learning paradigm for NLP tasks, dubbed as reStructured Pre-training (RST). In such a paradigm, the role of data will be re-emphasized, and model pre-training and fine-tuning of downstream tasks are viewed as a process of data storing and accessing. Based on that, we operationalize the simple principle that a good Cites: DataLab: A Platform for Data Analysis and Intervention