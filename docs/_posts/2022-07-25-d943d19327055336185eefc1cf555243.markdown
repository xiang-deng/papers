--- 
layout: post 
title: "Mitigating Algorithmic Bias with Limited Annotations" 
date: 2022-07-25 21:58:40 -0400 
categories: jekyll update 
author: "G Wang, M Du, N Liu, N Zou, X Hu - arXiv preprint arXiv:2207.10018, 2022" 
--- 
Existing work on fairness modeling commonly assumes that sensitive attributes for all instances are fully available, which may not be true in many real-world applications due to the high cost of acquiring sensitive information. When sensitive attributes are not disclosed or available, it is needed to manually annotate a small part of the training data to mitigate bias. However, the skewed distribution across different sensitive groups preserves the skewness of the original dataset in the annotated Cites: Just train twice: Improving group robustness without training group