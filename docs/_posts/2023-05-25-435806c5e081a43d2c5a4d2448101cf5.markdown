--- 
layout: post 
title: "FIT: Far-reaching Interleaved Transformers" 
date: 2023-05-25 03:51:47 -0400 
categories: jekyll update 
author: "T Chen, L Li - arXiv preprint arXiv:2305.12689, 2023" 
--- 
We present FIT: a transformer-based architecture with efficient self-attention and adaptive computation. Unlike original transformers, which operate on a single sequence of data tokens, we divide the data tokens into groups, with each group being a shorter sequence of tokens. We employ two types of transformer layers: local layers operate on data tokens within each group, while global layers operate on a smaller set of introduced latent tokens. These layers, comprising the same set of self  Cites: MEGABYTE: Predicting Million-byte Sequences with Multiscale