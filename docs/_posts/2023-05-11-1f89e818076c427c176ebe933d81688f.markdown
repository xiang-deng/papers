--- 
layout: post 
title: "PreCog: Exploring the Relation between Memorization and Performance in Pre-trained Language Models" 
date: 2023-05-11 03:26:59 -0400 
categories: jekyll update 
author: "L Ranaldi, ES Ruzzetti, FM Zanzotto - arXiv preprint arXiv:2305.04673, 2023" 
--- 
Pre-trained Language Models such as BERT are impressive machines with the ability to memorize, possibly generalized learning examples. We present here a small, focused contribution to the analysis of the interplay between memorization and performance of BERT in downstream tasks. We propose PreCog, a measure for evaluating memorization from pre-training, and we analyze its correlation with the BERT s performance. Our experiments show that highly memorized examples are  Cites: Language models as knowledge bases?