---
layout: post
title:  "Billions of Parameters Are Worth More Than In-domain Training Data: A case study in the Legal Case Entailment Task"
date:   2022-06-04 01:43:25 -0400
categories: jekyll update
author: "GM Rosa, L Bonifacio, V Jeronymo, H Abonizio - arXiv preprint arXiv , 2022"
---
Recent work has shown that language models scaled to billions of parameters, such as GPT-3, perform remarkably well in zero-shot and few-shot scenarios. In this work, we experiment with zero-shot models in the legal case entailment task of the COLIEE 2022 competition. Our experiments show that scaling the number of parameters in a language model improves the F1 score of our previous zero-shot result by more than 6 points, suggesting that stronger zero-shot capability may be a characteristic of  Cites: Rapidly deploying a neural search engine for the covid-19 open 