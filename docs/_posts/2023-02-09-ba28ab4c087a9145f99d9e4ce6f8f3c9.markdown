---
layout: post
title:  "BERTIN: Preentrenamiento eficiente de un modelo de lenguaje en español usando muestreo de perplejidad"
date:   2023-02-09 01:30:47 -0400
categories: jekyll update
author: "M Grandury, M Romero, J de la Rosa, EG Ponferrada… - Procesamiento del lenguaje …, 2022"
---
El preentrenamiento de grandes modelos de lenguaje generalmente requiere cantidades masivas de recursos, tanto en términos de computación como de datos. Las fuentes web comúnmente usadas, como Common Crawl, pueden contener el suficiente ruido para que el preentrenamiento no sea óptimo. En este trabajo experimentamos con diferentes métodos de muestreo de la versión en español de mC4 y presentamos una técnica novedosa centrada en datos que llamamos …
Cites: ‪Pre-Trained Models: Past, Present and Future‬