--- 
layout: post 
title: "BERTIN: Preentrenamiento eficiente de un modelo de lenguaje en espaol usando muestreo de perplejidad" 
date: 2023-02-09 01:30:47 -0400 
categories: jekyll update 
author: "M Grandury, M Romero, J de la Rosa, EG Ponferrada - Procesamiento del lenguaje , 2022" 
--- 
El preentrenamiento de grandes modelos de lenguaje generalmente requiere cantidades masivas de recursos, tanto en trminos de computacin como de datos. Las fuentes web comnmente usadas, como Common Crawl, pueden contener el suficiente ruido para que el preentrenamiento no sea ptimo. En este trabajo experimentamos con diferentes mtodos de muestreo de la versin en espaol de mC4 y presentamos una tcnica novedosa centrada en datos que llamamos Cites: Pre-Trained Models: Past, Present and Future