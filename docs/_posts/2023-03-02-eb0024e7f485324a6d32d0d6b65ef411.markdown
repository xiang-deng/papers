--- 
layout: post 
title: "Make Every Example Count: On Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets" 
date: 2023-03-02 06:18:50 -0400 
categories: jekyll update 
author: "I Bejan, A Sokolov, K Filippova - arXiv preprint arXiv:2302.13959, 2023" 
--- 
Increasingly larger datasets have become a standard ingredient to advancing the state of the art in NLP. However, data quality might have already become the bottleneck to unlock further gains. Given the diversity and the sizes of modern datasets, standard data filtering is not straight-forward to apply, because of the multifacetedness of the harmful data and elusiveness of filtering rules that would generalize across multiple tasks. We study the fitness of task-agnostic self-influence  Cites: QA Dataset Explosion: A Taxonomy of NLP Resources for