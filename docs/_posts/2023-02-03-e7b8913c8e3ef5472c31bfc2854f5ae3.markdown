--- 
layout: post 
title: "Contrastive Code-Comment Pre-training" 
date: 2023-02-03 14:16:33 -0400 
categories: jekyll update 
author: "X Pei, D Liu, L Qian, C Xu - 2022 IEEE International Conference on Data Mining , 2022" 
--- 
Pre-trained models for Natural Languages (NL) have been recently shown to transfer well to Programming Languages (PL) and largely benefit different intelligence code-related tasks, such as code search, clone detection, programming translation and code document generation. However, existing pre-trained methods for programming languages are mainly conducted by masked language modeling and next sentence prediction at token or graph levels. This restricted form limits their performance and  Cites: Graphcodebert: Pre-training code representations with data flow