---
layout: post
title:  "A novel domain adaptation theory with Jensen–Shannon divergence"
date:   2022-09-29 01:10:17 -0400
categories: jekyll update
author: "C Shui, Q Chen, J Wen, F Zhou, C Gagné, B Wang - Knowledge-Based Systems, 2022"
---
In this paper, we reveal the incoherence between the empirical domain adversarial training and its generally assumed theoretical counterpart based on H-divergence. Concretely, we find that H-divergence is not equivalent to Jensen–Shannon divergence, the optimization objective in domain adversarial training. To this end, we establish a new theoretical framework by directly proving the upper and lower target risk bounds based on the joint distributional Jensen–Shannon divergence. We …
Cites: ‪DIME: An Information-Theoretic Difficulty Measure for AI Datasets‬