--- 
layout: post 
title: "A novel domain adaptation theory with JensenShannon divergence" 
date: 2022-09-29 01:10:17 -0400 
categories: jekyll update 
author: "C Shui, Q Chen, J Wen, F Zhou, C Gagn, B Wang - Knowledge-Based Systems, 2022" 
--- 
In this paper, we reveal the incoherence between the empirical domain adversarial training and its generally assumed theoretical counterpart based on H-divergence. Concretely, we find that H-divergence is not equivalent to JensenShannon divergence, the optimization objective in domain adversarial training. To this end, we establish a new theoretical framework by directly proving the upper and lower target risk bounds based on the joint distributional JensenShannon divergence. We  Cites: DIME: An Information-Theoretic Difficulty Measure for AI Datasets