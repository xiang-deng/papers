---
layout: post
title:  "Extreme Multi-Domain, Multi-Task Learning With Unified Text-to-Text Transfer Transformers"
date:   2022-09-27 02:04:52 -0400
categories: jekyll update
author: "A Oshingbesan, C Ekoh, G Atakpa, Y Byaruagaba - arXiv preprint arXiv:2209.10106, 2022"
---
Text-to-text transformers have shown remarkable success in the task of multi-task transfer learning, especially in natural language processing (NLP). However, while there have been several attempts to train transformers on different domains, there is usually a clear relationship between these domains, eg,, code summarization, where the natural language summary describes the code. There have been very few attempts to study how multi-task transfer learning works on tasks in significantly …
Cites: ‪Exploring and predicting transferability across nlp tasks‬