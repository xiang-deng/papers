---
layout: post
title:  "Explanation Graph Generation via Pre-trained Language Models: An Empirical Study with Contrastive Learning"
date:   2022-04-16 01:25:48 -0400
categories: jekyll update
author: "S Saha, P Yadav, M Bansal - arXiv preprint arXiv:2204.04813, 2022"
---
Pre-trained sequence-to-sequence language models have led to widespread success in many natural language generation tasks. However, there has been relatively less work on analyzing their ability to generate structured outputs such as graphs. Unlike natural language, graphs have distinct structural and semantic properties in the context of a downstream NLP task, eg, generating a graph that is connected and acyclic can be attributed to its structural constraints, while the Cites: CommonsenseQA 2.0: Exposing the limits of AI through gamification