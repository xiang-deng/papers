--- 
layout: post 
title: "Vision Lanauge Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation" 
date: 2023-05-11 03:26:59 -0400 
categories: jekyll update 
author: "C Jiang, W Ye, H Xu, S Zhang, J Zhang, F Huang - arXiv preprint arXiv:2305.04474, 2023" 
--- 
Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound  Cites: How Much Can CLIP Benefit Vision-and-Language Tasks?