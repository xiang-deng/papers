--- 
layout: post 
title: "Large pre-trained language models contain human-like biases of what is right and wrong to do" 
date: 2022-03-29 11:43:06 -0400 
categories: jekyll update 
author: "P Schramowski, C Turan, N Andersen, CA Rothkopf - Nature Machine Intelligence, 2022" 
--- 
Artificial writing is permeating our lives due to recent advances in large-scale, transformer-based language models (LMs) such as BERT, GPT-2 and GPT-3. Using them as pre-trained models and fine-tuning them for specific tasks, researchers have extended the state of the art for many natural language processing tasks and shown that they capture not only linguistic knowledge but also retain general knowledge implicitly present in the data. Unfortunately, LMs trained on unfiltered text corpora Cites: Social chemistry 101: Learning to reason about social and moral