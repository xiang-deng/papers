---
layout: post
title:  "Towards Understanding Self-Supervised Representation Learning"
date:   2022-10-10 14:05:52 -0400
categories: jekyll update
author: "N Saunshi - 2022"
---
While supervised learning sparked the deep learning boom, it has some critical shortcomings:(1) it requires an abundance of expensive labeled data, and (2) it solves tasks from scratch rather than the human-like approach of leveraging knowledge and skills acquired from prior experiences. Pre-training has emerged as an alternative and effective paradigm, to overcome these shortcomings, whereby a model is first trained using easily acquirable data, and later used to solve …
Cites: ‪oLMpics-on what language model pre-training captures‬