---
layout: post
title:  "On the SDEs and Scaling Rules for Adaptive Gradient Algorithms"
date:   2022-05-28 02:05:27 -0400
categories: jekyll update
author: "S Malladi, K Lyu, A Panigrahi, S Arora - arXiv preprint arXiv:2205.10287, 2022"
---
Approximating Stochastic Gradient Descent (SGD) as a Stochastic Differential Equation (SDE) has allowed researchers to enjoy the benefits of studying a continuous optimization trajectory while carefully preserving the stochasticity of SGD. Analogous study of adaptive gradient methods, such as RMSprop and Adam, has been challenging because there were no rigorously proven SDE approximations for these methods. This paper derives the SDE approximations for RMSprop and Adam  Cites: Should You Mask 15% in Masked Language Modeling?