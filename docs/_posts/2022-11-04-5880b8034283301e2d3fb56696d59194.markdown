---
layout: post
title:  "The future is different: Large pre-trained language models fail in prediction tasks"
date:   2022-11-04 15:58:33 -0400
categories: jekyll update
author: "K Cvejoski, RJ Sánchez, C Ojeda - arXiv preprint arXiv:2211.00384, 2022"
---
Large pre-trained language models (LPLM) have shown spectacular success when fine-tuned on downstream supervised tasks. Yet, it is known that their performance can drastically drop when there is a distribution shift between the data used during training and that used at inference time. In this paper we focus on data distributions that naturally change over time and introduce four new REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and POLITICS sub-reddits. First …
Cites: ‪Polylingual tree-based topic models for translation domain …‬