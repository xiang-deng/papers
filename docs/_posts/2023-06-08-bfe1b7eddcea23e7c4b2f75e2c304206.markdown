---
layout: post
title:  "Are the BERT Family Zero-Shot Learners? A Study on Their Potential and Limitations"
date:   2023-06-08 03:52:18 -0400
categories: jekyll update
author: "Y Wang, L Wu, J Li, X Liang, M Zhang - Artificial Intelligence, 2023"
---
Starting from the resurgence of deep learning, language models (LMs) have never been so popular. Through simply increasing model scale and data size, large LMs pre-trained with self-supervision objectives demonstrate awe-inspiring results on both task performance and generalization. At the early stage, supervised fine-tuning is indispensable in adapting pre-trained language models (PLMs) to downstream tasks. Later on, the sustained growth of model capacity and data size, as well as …
Cites: ‪Pre-train, prompt, and predict: A systematic survey of prompting …‬