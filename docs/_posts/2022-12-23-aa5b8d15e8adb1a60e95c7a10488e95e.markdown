--- 
layout: post 
title: "Pretraining Without Attention" 
date: 2022-12-23 23:45:02 -0400 
categories: jekyll update 
author: "J Wang, JN Yan, A Gu, AM Rush - arXiv preprint arXiv:2212.10544, 2022" 
--- 
Transformers have been essential to pretraining success in NLP. Other architectures have been used, but require attention layers to match benchmark accuracy. This work explores pretraining without attention. We test recently developed routing layers based on state-space models (SSM) and model architectures based on multiplicative gating. Used together these modeling choices have a large impact on pretraining accuracy. Empirically the proposed Bidirectional Gated SSM (BiGS) replicates BERT  Cites: Transformer quality in linear time