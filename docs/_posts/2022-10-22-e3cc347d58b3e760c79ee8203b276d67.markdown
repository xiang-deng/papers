--- 
layout: post 
title: "Revision Transformers: Getting RiT of No-Nos" 
date: 2022-10-22 02:20:44 -0400 
categories: jekyll update 
author: "F Friedrich, W Stammer, P Schramowski, K Kersting - arXiv preprint arXiv:2210.10332, 2022" 
--- 
Current transformer language models (LM) are large-scale models with billions of parameters. They have been shown to provide high performances on a variety of tasks but are also prone to shortcut learning and bias. Addressing such incorrect model behavior via parameter adjustments is very costly. This is particularly problematic for updating dynamic concepts, such as moral values, which vary culturally or interpersonally. In this work, we question the current common practice of  Cites: How context affects language models factual predictions