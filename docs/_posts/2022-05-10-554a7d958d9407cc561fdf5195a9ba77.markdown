---
layout: post
title:  "Visual Commonsense in Pretrained Unimodal and Multimodal Models"
date:   2022-05-10 03:22:04 -0400
categories: jekyll update
author: "C Zhang, B Van Durme, Z Li, E Stengel-Eskin - arXiv preprint arXiv:2205.01850, 2022"
---
Our commonsense knowledge about objects includes their typical visual attributes; we know that bananas are typically yellow or green, and not purple. Text and image corpora, being subject to reporting bias, represent this world-knowledge to varying degrees of faithfulness. In this paper, we investigate to what degree unimodal (language-only) and multimodal (image and language) models capture a broad range of visually salient attributes. To that end, we create the Visual Commonsense Cites: Experience grounds language