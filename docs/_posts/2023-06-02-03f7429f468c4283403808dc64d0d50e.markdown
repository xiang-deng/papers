---
layout: post
title:  "SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics"
date:   2023-06-02 15:36:55 -0400
categories: jekyll update
author: "A Ardakani, A Haan, S Tan, DT Popovici, A Cheung… - arXiv preprint arXiv …, 2023"
---
Transformer-based models, such as BERT and ViT, have achieved state-of-the-art results across different natural language processing (NLP) and computer vision (CV) tasks. However, these models are extremely memory intensive during their fine-tuning process, making them difficult to deploy on GPUs with limited memory resources. To address this issue, we introduce a new tool called SlimFit that reduces the memory requirements of these models by dynamically analyzing their training …
Cites: ‪GACT: Activation compressed training for generic network …‬