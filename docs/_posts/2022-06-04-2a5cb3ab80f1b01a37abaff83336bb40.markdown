--- 
layout: post 
title: "Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models" 
date: 2022-06-04 01:43:25 -0400 
categories: jekyll update 
author: "M Xia, M Artetxe, J Du, D Chen, V Stoyanov - arXiv preprint arXiv:2205.15223, 2022" 
--- 
Pre-trained masked language models successfully perform few-shot learning by formulating downstream tasks as text infilling. However, as a strong alternative in full-shot settings, discriminative pre-trained models like ELECTRA do not fit into the paradigm. In this work, we adapt prompt-based few-shot learning to ELECTRA and show that it outperforms masked language models in a wide range of tasks. ELECTRA is pre-trained to distinguish if a token is generated or original. We Cites: Prompt Tuning for Discriminative Pre-trained Language Models