--- 
layout: post 
title: "The Sensitivity of Annotator Bias to Task Definitions in Argument Mining" 
date: 2022-10-04 00:49:37 -0400 
categories: jekyll update 
author: "TST Jakobsen, M Barrett, A Sgaard, D Lassen - Proceedings of the 16th Lingusitic , 2022" 
--- 
NLP models are dependent on the data they are trained on, including how this data is annotated. NLP research increasingly examines the social biases of models, but often in the light of their training data and specific social biases that can be identified in the text itself. In this paper, we present an annotation experiment that is the first to examine the extent to which social bias is sensitive to how data is annotated. We do so by collecting annotations of arguments in the same documents following four  Cites: Are we modeling the task or the annotator? an investigation of