---
layout: post
title:  "BertNet: Harvesting Knowledge Graphs from Pretrained Language Models"
date:   2022-06-10 22:27:43 -0400
categories: jekyll update
author: "S Hao, B Tan, K Tang, H Zhang, EP Xing, Z Hu"
---
Symbolic knowledge graphs (KGs) have been constructed either by expensive human crowdsourcing or with domain-specific complex information extraction pipelines. The emerging large pretrained language models (LMs), such as BERT, have shown to implicitly encode massive knowledge which can be queried with properly designed prompts. However, compared to the explicit KGs, the implict knowledge in the black-box LMs is often difficult to access or edit and lacks …
Cites: ‪Do Language Models Have Beliefs? Methods for Detecting …‬  