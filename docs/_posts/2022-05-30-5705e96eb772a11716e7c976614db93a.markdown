---
layout: post
title:  "Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning"
date:   2022-05-30 22:20:45 -0400
categories: jekyll update
author: "P Gupta, C Jiao, YT Yeh, S Mehri, M Eskenazi - arXiv preprint arXiv , 2022"
---
Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Instructions have been shown to enable good performance on unseen tasks and datasets in both large and small language models. Dialogue is an especially interesting area to explore instruction tuning because dialogue systems perform multiple kinds of tasks related to language (eg, natural language  Cites: A simple language model for task-oriented dialogue