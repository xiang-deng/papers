--- 
layout: post 
title: "On Feature Learning in Neural Networks with Global Convergence Guarantees" 
date: 2022-04-30 03:01:01 -0400 
categories: jekyll update 
author: "Z Chen, E Vanden-Eijnden, J Bruna - arXiv preprint arXiv:2204.10782, 2022" 
--- 
We study the optimization of wide neural networks (NNs) via gradient flow (GF) in setups that allow feature learning while admitting non-asymptotic global convergence guarantees. First, for wide shallow NNs under the mean-field scaling and with a general class of activation functions, we prove that when the input dimension is no less than the size of the training set, the training loss converges to zero at a linear rate under GF. Building upon this analysis, we study a model of wide Cites: Towards understanding hierarchical learning: Benefits of neural