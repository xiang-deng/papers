---
layout: post
title:  "Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding"
date:   2022-10-20 02:20:28 -0400
categories: jekyll update
author: "J Wang, W Huang, Q Shi, H Wang, M Qiu, X Li, M Gao - arXiv preprint arXiv …, 2022"
---
Knowledge-enhanced Pre-trained Language Model (PLM) has recently received significant attention, which aims to incorporate factual knowledge into PLMs. However, most existing methods modify the internal structures of fixed types of PLMs by stacking complicated modules, and introduce redundant and irrelevant factual knowledge from knowledge bases (KBs). In this paper, to address these problems, we introduce a seminal knowledge prompting paradigm and further propose a …
Cites: ‪K-adapter: Infusing knowledge into pre-trained models with adapters‬