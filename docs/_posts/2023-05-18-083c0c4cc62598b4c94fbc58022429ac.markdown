--- 
layout: post 
title: "CodeT5+: Open Code Large Language Models for Code Understanding and Generation" 
date: 2023-05-18 07:22:22 -0400 
categories: jekyll update 
author: "Y Wang, H Le, AD Gotmare, NDQ Bui, J Li, SCH Hoi - arXiv preprint arXiv:2305.07922, 2023" 
--- 
Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system  Cites: Gedi: Generative discriminator guided sequence generation