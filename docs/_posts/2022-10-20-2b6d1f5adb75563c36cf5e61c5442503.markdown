---
layout: post
title:  "Probing the Robustness of Pre-trained Language Models for Entity Matching"
date:   2022-10-20 02:20:28 -0400
categories: jekyll update
author: "M Akbarian Rastaghi, E Kamalloo, D Rafiei - Proceedings of the 31st ACM …, 2022"
---
The paradigm of fine-tuning Pre-trained Language Models (PLMs) has been successful in Entity Matching (EM). Despite their remarkable performance, PLMs exhibit tendency to learn spurious correlations from training data. In this work, we aim at investigating whether PLM-based entity matching models can be trusted in real-world applications where data distribution is different from that of training. To this end, we design an evaluation benchmark to assess the robustness of EM models to …
Cites: ‪BERT: Pre-training of deep bidirectional transformers for language …‬