---
layout: post
title:  "When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario"
date:   2023-05-19 23:52:25 -0400
categories: jekyll update
author: "C Han, L Cui, R Zhu, J Wang, N Chen, Q Sun, X Li… - arXiv preprint arXiv …, 2023"
---
Large pre-trained language models (PLMs) have garnered significant attention for their versatility and potential for solving a wide spectrum of natural language processing (NLP) tasks. However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to commercial considerations and potential risks of misuse, such as GPT-3. The parameters and gradients of PLMs are unavailable in this scenario. To solve the issue, black-box tuning has been …
Cites: ‪Delta Tuning: A Comprehensive Study of Parameter Efficient …‬