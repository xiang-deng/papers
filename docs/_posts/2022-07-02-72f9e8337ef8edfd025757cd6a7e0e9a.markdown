--- 
layout: post 
title: "Simple Flow-Based Contrastive Learning for BERT Sentence Representations" 
date: 2022-07-02 02:42:16 -0400 
categories: jekyll update 
author: "Z Tian, Q Liu, M Liu, W Deng - Advances in Swarm Intelligence: 13th International , 2022" 
--- 
Natural language processing is a significant branch of machine learning, and pre-trained models such as BERT have been widely used in it. Previous research has shown that sentence embeddings from pre-trained language models without fine-tune have difficulty in capturing their exact semantics. The ambiguous semantics leads to poor performance on semantic text similarity (STS) tasks. However, fine-tune tends to skew the model toward high-frequency distributions due to the Cites: SimCSE: Simple Contrastive Learning of Sentence Embeddings