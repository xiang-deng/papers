--- 
layout: post 
title: "Gradient-based Intra-attention Pruning on Pre-trained Language Models" 
date: 2022-12-20 02:26:19 -0400 
categories: jekyll update 
author: "Z Yang, Y Cui, X Yao, S Wang - arXiv preprint arXiv:2212.07634, 2022" 
--- 
Pre-trained language models achieve superior performance, but they are computationally expensive due to their large size. Techniques such as pruning and knowledge distillation (KD) have been developed to reduce their size and latency. In most structural pruning methods, the pruning units, such as attention heads and feed-forward hidden dimensions, only span a small model structure space and limit the structures that the pruning algorithm can explore. In this work, we propose Gradient Cites: Mobilebert: a compact task-agnostic bert for resource-limited devices