--- 
layout: post 
title: "Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation" 
date: 2023-06-06 05:46:58 -0400 
categories: jekyll update 
author: "R Zhai, B Liu, A Risteski, Z Kolter, P Ravikumar - arXiv preprint arXiv:2306.00788, 2023" 
--- 
Good data augmentation is one of the key factors that lead to the empirical success of self-supervised representation learning such as contrastive learning and masked language modeling, yet theoretical understanding of its role in learning good representations remains limited. Recent work has built the connection between self-supervised learning and approximating the top eigenspace of a graph Laplacian operator. Learning a linear probe on top of such features can naturally be connected Cites: Should You Mask 15% in Masked Language Modeling?