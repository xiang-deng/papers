--- 
layout: post 
title: "Transcending Scaling Laws with 0.1% Extra Compute" 
date: 2022-10-24 23:22:19 -0400 
categories: jekyll update 
author: "Y Tay, J Wei, HW Chung, VQ Tran, DR So, S Shakeri - arXiv preprint arXiv , 2022" 
--- 
Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (eg, PaLM) on a few more steps with UL2 s mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new Cites: Muppet: Massive multi-task representations with pre-finetuning