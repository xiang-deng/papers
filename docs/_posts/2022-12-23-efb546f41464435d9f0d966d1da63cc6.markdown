---
layout: post
title:  "GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator"
date:   2022-12-23 23:45:02 -0400
categories: jekyll update
author: "J Yang, S Ma, L Dong, S Huang, H Huang, Y Yin… - arXiv preprint arXiv …, 2022"
---
Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for encoder-decoder pre-training by introducing an auxiliary discriminator, unifying the ability of language understanding and generation in a single model. Our model, named as GanLM, is …
Cites: ‪SpanBERT: Improving Pre-training by Representing and Predicting …‬