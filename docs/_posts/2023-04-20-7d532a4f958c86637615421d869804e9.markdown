---
layout: post
title:  "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
date:   2023-04-20 07:45:04 -0400
categories: jekyll update
author: "H Dong, W Xiong, D Goyal, R Pan, S Diao, J Zhang… - arXiv preprint arXiv …, 2023"
---
Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning …
Cites: ‪Training Language Models with Language Feedback at Scale‬