--- 
layout: post 
title: "Improving Log-Based Anomaly Detection by Pre-Training Hierarchical Transformers" 
date: 2023-03-18 01:48:35 -0400 
categories: jekyll update 
author: "S Huang, Y Liu, C Fung, H Wang, H Yang, Z Luan - IEEE Transactions on Computers, 2023" 
--- 
Pre-trained models, such as BERT, have resulted in significant pre-trained models, such as BERT, have resulted in significant improvements in many natural language processing (NLP) applications. However, due to differences in word distribution and domain data distribution, applying NLP advancements to log analysis directly faces some performance challenges. This paper studies how to adapt the recently introduced pre-trained language model BERT for log analysis. In this work, we  Cites: Well-read students learn better: The impact of student initialization