---
layout: post
title:  "Towards Automated Distillation: A Systematic Study of Knowledge Distillation in Natural Language Processing"
date:   2022-07-04 12:22:48 -0400
categories: jekyll update
author: "H He, XSJMS Zha, MLG Karypis"
---
Key factors underpinning the optimal Knowledge Distillation (KD) performance remain elusive as the effects of these factors are often confounded in sophisticated distillation algorithms. This poses a challenge for choosing the best distillation algorithm from the large design space for existing and new tasks alike and hinders automated distillation. In this work, we aim to identify how the distillation performance across different tasks is affected by the components in the KD pipeline, such as the  Cites: Well-Read Students Learn Better: On the Importance of Pre