---
layout: post
title:  "Fine-grained image captioning with clip reward"
date:   2022-05-30 22:20:45 -0400
categories: jekyll update
author: "J Cho, S Yoon, A Kale, F Dernoncourt, T Bui, M Bansal - arXiv preprint arXiv …, 2022"
---
Modern image captioning models are usually trained with text similarity objectives. However, since reference captions in public datasets often describe the most salient common objects, models trained with text similarity objectives tend to ignore specific and detailed aspects of an image that distinguish it from others. Toward more descriptive and distinctive caption generation, we propose using CLIP, a multimodal encoder trained on huge image-text pairs from web, to calculate multimodal similarity … Cites: ‪Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis …‬