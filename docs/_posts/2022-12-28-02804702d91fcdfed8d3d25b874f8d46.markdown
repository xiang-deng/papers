--- 
layout: post 
title: "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization" 
date: 2022-12-28 14:07:11 -0400 
categories: jekyll update 
author: "S Iyer, XV Lin, R Pasunuru, T Mihaylov, D Simig, P Yu - arXiv preprint arXiv , 2022" 
--- 
Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, aka instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and  Cites: Scaling instruction-finetuned language models