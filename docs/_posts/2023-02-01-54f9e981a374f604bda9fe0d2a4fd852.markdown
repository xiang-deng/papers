--- 
layout: post 
title: "Representation biases in sentence transformers" 
date: 2023-02-01 14:37:22 -0400 
categories: jekyll update 
author: "D Nikolaev, S Pad - arXiv preprint arXiv:2301.13039, 2023" 
--- 
Variants of the BERT architecture specialised for producing full-sentence representations often achieve better performance on downstream tasks than sentence embeddings extracted from vanilla BERT. However, there is still little understanding of what properties of inputs determine the properties of such representations. In this study, we construct several sets of sentences with pre-defined lexical and syntactic structures and show that SOTA sentence transformers have a  Cites: Modeling local coherence: An entity-based approach