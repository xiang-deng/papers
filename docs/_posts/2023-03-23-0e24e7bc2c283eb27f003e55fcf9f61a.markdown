---
layout: post
title:  "Linear Complexity Self-Attention with $3^{\text {rd}} $ Order Polynomials"
date:   2023-03-23 03:27:25 -0400
categories: jekyll update
author: "F Babiloni, I Marras, J Deng, F Kokkinos, M Maggioni… - IEEE Transactions on …, 2023"
---
Self-attention mechanisms and non-local blocks have become crucial building blocks for state-of-the-art neural architectures thanks to their unparalleled ability in capturing long-range dependencies in the input. However their cost is quadratic with the number of spatial positions hence making their use impractical in many real case applications. In this work, we analyze these methods through a polynomial lens, and we show that self-attention can be seen as a special case of a 3 rd order polynomial …
Cites: ‪BERT: Pre-training of Deep Bidirectional Transformers for …‬