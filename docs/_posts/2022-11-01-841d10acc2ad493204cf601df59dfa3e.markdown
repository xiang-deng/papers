---
layout: post
title:  "Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost"
date:   2022-11-01 03:49:43 -0400
categories: jekyll update
author: "S Cho, S Min, J Kim, M Lee, H Lee, S Hong - arXiv preprint arXiv:2210.15541, 2022"
---
To overcome the quadratic cost of self-attention, recent works have proposed various sparse attention modules, most of which fall under one of two groups: 1) sparse attention under a hand-crafted patterns and 2) full attention followed by a sparse variant of softmax such as $\alpha $-entmax. Unfortunately, the first group lacks adaptability to data while the second still requires quadratic cost in training. In this work, we propose SBM-Transformer, a model that resolves both problems by …
Cites: ‪What does BERT look at? An analysis of BERT s attention‬