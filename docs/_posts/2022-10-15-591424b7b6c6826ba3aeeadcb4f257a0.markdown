---
layout: post
title:  "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis"
date:   2022-10-15 02:59:22 -0400
categories: jekyll update
author: "Y Xiao, PP Liang, U Bhatt, W Neiswanger… - arXiv preprint arXiv …, 2022"
---
Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline:(1) the …
Cites: ‪On the Effects of Transformer Size on In-and Out-of-Domain …‬