---
layout: post
title:  "One Reference Is Not Enough: Diverse Distillation with Reference Selection for Non-Autoregressive Translation"
date:   2022-06-04 01:43:25 -0400
categories: jekyll update
author: "C Shao, X Wu, Y Feng - arXiv preprint arXiv:2205.14333, 2022"
---
Non-autoregressive neural machine translation (NAT) suffers from the multi-modality problem: the source sentence may have multiple correct translations, but the loss function is calculated only according to the reference sentence. Sequence-level knowledge distillation makes the target more deterministic by replacing the target with the output from an autoregressive model. However, the multi-modality problem in the distilled dataset is still nonnegligible. Furthermore, learning from a specific  Cites: Latent-variable non-autoregressive neural machine translation 