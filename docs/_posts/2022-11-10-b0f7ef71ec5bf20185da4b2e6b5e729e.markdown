--- 
layout: post 
title: "On the Domain Adaptation and Generalization of Pretrained Language Models: A Survey" 
date: 2022-11-10 01:14:02 -0400 
categories: jekyll update 
author: "X Guo, H Yu - arXiv preprint arXiv:2211.03154, 2022" 
--- 
Recent advances in NLP are brought by a range of large-scale pretrained language models (PLMs). These PLMs have brought significant performance gains for a range of NLP tasks, circumventing the need to customize complex designs for specific tasks. However, most current work focus on finetuning PLMs on a domain-specific datasets, ignoring the fact that the domain gap can lead to overfitting and even performance drop. Therefore, it is practically important to find an appropriate method Cites: Fast model editing at scale