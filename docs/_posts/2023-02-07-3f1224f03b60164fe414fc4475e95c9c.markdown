---
layout: post
title:  "CheckHARD: Checking Hard Labels for Adversarial Text Detection, Prediction Correction, and Perturbed Word Suggestion"
date:   2023-02-07 01:43:12 -0400
categories: jekyll update
author: "HQ Nguyen-Son, HQ Ung, S Hidano, K Fukushima… - Findings of the Association …, 2022"
---
An adversarial attack generates harmful text that fools a target model. More dangerously, this text is unrecognizable by humans. Existing work detects adversarial text and corrects a target s prediction by identifying perturbed words and changing them into their synonyms, but many benign words are also changed. In this paper, we directly detect adversarial text, correct the prediction, and suggest perturbed words by checking the change in the hard labels from the target s …
Cites: ‪Beyond Accuracy: Behavioral Testing of NLP Models with CheckList‬