--- 
layout: post 
title: "Don t Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner" 
date: 2023-05-06 06:19:24 -0400 
categories: jekyll update 
author: "Z Shi, A Lipani - arXiv preprint arXiv:2305.01711, 2023" 
--- 
Language models (LMs) trained on vast quantities of unlabelled data have greatly advanced the field of natural language processing (NLP). In this study, we re-visit the widely accepted notion in NLP that continued pre-training LMs on task-related texts improves the performance of fine-tuning (FT) in downstream tasks. Through experiments on eight single-sentence tasks and eight sentence-pair tasks in both semi-supervised and fully-supervised settings, we find that conventional continued Cites: Semi-supervised semantic role labeling with cross-view training