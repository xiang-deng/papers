--- 
layout: post 
title: "RRHF: Rank Responses to Align Language Models with Human Feedback without tears" 
date: 2023-04-14 18:18:10 -0400 
categories: jekyll update 
author: "Z Yuan, H Yuan, C Tan, W Wang, S Huang, F Huang - arXiv preprint arXiv , 2023" 
--- 
Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and these models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). PPO, however, is sensitive to hyperparameters and requires a minimum of four models in its standard Cites: Super-naturalinstructions: Generalization via declarative