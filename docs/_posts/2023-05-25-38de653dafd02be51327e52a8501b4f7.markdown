---
layout: post
title:  "LIMA: Less Is More for Alignment"
date:   2023-05-25 03:51:47 -0400
categories: jekyll update
author: "C Zhou, P Liu, P Xu, S Iyer, J Sun, Y Mao, X Ma, A Efrat… - arXiv preprint arXiv …, 2023"
---
Large language models are trained in two stages:(1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences …
