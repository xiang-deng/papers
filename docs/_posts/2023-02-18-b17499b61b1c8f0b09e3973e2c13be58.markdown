---
layout: post
title:  "Bag of Tricks for In-Distribution Calibration of Pretrained Transformers"
date:   2023-02-18 05:28:11 -0400
categories: jekyll update
author: "J Kim, D Na, S Choi, S Lim - arXiv preprint arXiv:2302.06690, 2023"
---
While pre-trained language models (PLMs) have become a de-facto standard promoting the accuracy of text classification tasks, recent studies find that PLMs often predict over-confidently. Although various calibration methods have been proposed, such as ensemble learning and data augmentation, most of the methods have been verified in computer vision benchmarks rather than in PLM-based text classification tasks. In this paper, we present an empirical study on confidence calibration for …
Cites: ‪On the effects of transformer size on in-and out-of-domain calibration‬