--- 
layout: post 
title: "An Efficient Transformer with Distance-aware Attention" 
date: 2023-05-30 03:09:06 -0400 
categories: jekyll update 
author: "G Duan, X Zheng, Y Zhu, T Ren, Y Yan - 2023 IEEE 9th Intl Conference on Big Data , 2023" 
--- 
In recent years, the transformer model has become one of the main highlights of advances in natural language processing (NLP). The attention mechanism of the transformer model makes it possible to track the relations between words across very long text sequences in both forward and reverse directions. However, the complexity of the attention mechanism is quadratic and introduces a performance bottleneck in the transformer. We propose a distance-aware attention mechanism which integrates  Cites: Blockwise Self-Attention for Long Document Understanding