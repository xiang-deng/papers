--- 
layout: post 
title: "Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark" 
date: 2022-09-27 02:04:52 -0400 
categories: jekyll update 
author: "N Dziri, H Rashkin, T Linzen, D Reitter - Transactions of the Association for , 2022" 
--- 
Abstract Knowledge-grounded dialogue systems powered by large language models often generate responses that, while fluent, are not attributable to a relevant source of information. Progress towards models that do not exhibit this issue requires evaluation metrics that can quantify its prevalence. To this end, we introduce the Benchmark for Evaluation of Grounded INteraction (Begin), comprising 12k dialogue turns generated by neural dialogue systems trained on three knowledge-grounded Cites: BARTScore: Evaluating Generated Text as Text Generation